{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef95541d",
   "metadata": {},
   "source": [
    "# 畳み込みニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524eceb1",
   "metadata": {},
   "source": [
    "### Convolution/Poolingレイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f2d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b68311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(10,1,28,28)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e13d8e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.7060744 , 0.91926254, 0.6726526 ],\n",
       "        [0.06161747, 0.64811467, 0.74639766]],\n",
       "\n",
       "       [[0.22792606, 0.75864532, 0.21596095],\n",
       "        [0.28511664, 0.45102482, 0.75461466]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.random.rand(2,2,2,3)\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1281c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ca7d24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4.20941670e-01, 1.11516217e-01, 2.51869407e-01, 6.11205881e-01,\n",
       "         1.25727377e-01, 1.42771752e-01, 3.60279290e-01, 9.15966021e-01,\n",
       "         9.94927269e-01, 1.91022795e-01, 6.68080067e-01, 4.28422139e-01,\n",
       "         3.70627183e-01, 4.09742072e-01, 7.16127122e-01, 1.55298496e-01,\n",
       "         3.67282189e-02, 8.90569090e-01, 5.25854473e-01, 8.49884103e-01,\n",
       "         9.70199584e-01, 6.79570226e-01, 8.78270630e-01, 4.95851225e-01,\n",
       "         8.95929668e-01, 3.18780976e-01, 8.81174977e-01, 2.19279143e-01],\n",
       "        [9.19617965e-02, 4.27190487e-01, 5.16067578e-01, 7.47736497e-01,\n",
       "         1.69674927e-02, 6.52084125e-01, 9.45248208e-01, 6.50876896e-01,\n",
       "         8.70127078e-01, 6.12521605e-01, 3.48992634e-01, 3.08095590e-01,\n",
       "         9.27988608e-01, 2.09074395e-01, 1.02554848e-01, 5.68100502e-01,\n",
       "         3.25551174e-01, 8.70108697e-02, 3.07844623e-01, 8.23383124e-01,\n",
       "         2.88234285e-01, 5.73702443e-02, 9.39779052e-01, 7.36109220e-02,\n",
       "         3.10389373e-01, 8.65461313e-01, 4.47196521e-01, 1.30961955e-01],\n",
       "        [2.31600095e-02, 2.77348701e-02, 9.74350270e-01, 8.26376765e-01,\n",
       "         8.04626434e-02, 9.10363969e-01, 2.21468607e-01, 4.80711532e-02,\n",
       "         4.95370920e-01, 9.60415263e-01, 3.34941468e-01, 1.43735883e-01,\n",
       "         5.09561252e-01, 2.87204574e-01, 4.73272558e-01, 1.22850812e-01,\n",
       "         5.63254211e-01, 3.01690651e-01, 5.91057148e-01, 3.06734331e-01,\n",
       "         4.02782705e-01, 7.69152296e-01, 7.59663917e-01, 1.64468436e-01,\n",
       "         8.89508540e-01, 9.10988395e-01, 9.53635909e-01, 9.20901086e-01],\n",
       "        [5.35871030e-01, 7.72396434e-01, 3.67374524e-02, 6.70556688e-01,\n",
       "         6.44598973e-01, 8.86200631e-01, 5.98968967e-01, 3.23603123e-01,\n",
       "         5.52911049e-01, 6.53949037e-01, 4.18323644e-01, 8.90304503e-01,\n",
       "         9.41200666e-01, 4.33390093e-02, 7.34389485e-01, 8.86542850e-01,\n",
       "         9.76346370e-01, 9.60097059e-01, 6.07289295e-01, 7.32509687e-01,\n",
       "         1.65925262e-01, 7.25845388e-01, 8.82093566e-01, 1.02716591e-01,\n",
       "         7.91101920e-01, 7.61616868e-01, 2.18886992e-01, 1.72206054e-01],\n",
       "        [9.91183277e-01, 2.99565650e-01, 5.26286200e-01, 4.93788592e-02,\n",
       "         9.80123130e-01, 2.68349413e-01, 9.78694729e-01, 6.68147453e-01,\n",
       "         3.65917879e-01, 5.04915341e-01, 8.45582976e-01, 4.85649400e-01,\n",
       "         1.29426908e-01, 9.48103840e-01, 6.97338218e-01, 8.84667330e-01,\n",
       "         7.21389472e-01, 5.84530056e-01, 7.62048091e-01, 1.13907446e-01,\n",
       "         4.24324618e-01, 6.41195767e-01, 5.75158865e-01, 9.55286570e-01,\n",
       "         8.59603437e-01, 8.58282144e-02, 2.57081988e-01, 3.93579707e-01],\n",
       "        [8.98373228e-01, 8.10178004e-01, 6.42700531e-01, 3.84655006e-01,\n",
       "         9.52281195e-01, 3.67355310e-01, 1.10048482e-01, 5.84920482e-01,\n",
       "         3.19170892e-01, 5.50141984e-01, 5.89124215e-01, 9.97307611e-01,\n",
       "         1.17573414e-01, 3.76813610e-01, 4.56090854e-01, 9.97655676e-01,\n",
       "         9.31773682e-01, 2.68301716e-01, 2.54094937e-01, 1.33227188e-01,\n",
       "         6.33970708e-02, 5.31762461e-02, 8.02253335e-01, 2.10119284e-01,\n",
       "         9.48356136e-01, 4.84037188e-01, 3.47245350e-01, 2.99921113e-01],\n",
       "        [6.68679730e-01, 6.54440090e-01, 4.19136947e-01, 1.44307871e-01,\n",
       "         7.35538834e-01, 2.67405600e-01, 7.11853616e-01, 3.39929025e-01,\n",
       "         2.37173116e-01, 2.31490481e-02, 2.99333527e-01, 8.94734643e-01,\n",
       "         8.66824890e-01, 3.48941091e-01, 1.86170592e-01, 8.97522213e-01,\n",
       "         7.67854906e-02, 7.17929140e-02, 2.15884760e-01, 4.08049265e-01,\n",
       "         7.24382078e-01, 6.04951549e-01, 4.67337329e-01, 8.42083940e-01,\n",
       "         2.99811872e-01, 7.39649702e-01, 4.74773373e-01, 4.75563442e-03],\n",
       "        [1.24938547e-01, 1.64911275e-01, 8.99388638e-01, 8.18480294e-01,\n",
       "         9.26656137e-01, 6.35190210e-01, 8.45528526e-01, 7.36149114e-02,\n",
       "         6.51620123e-02, 8.61725419e-01, 6.69259712e-01, 5.99166649e-01,\n",
       "         5.04573462e-02, 8.82871691e-02, 1.95738340e-01, 3.92022891e-02,\n",
       "         6.79872014e-01, 9.71393925e-01, 7.77563668e-01, 3.16227979e-01,\n",
       "         7.96038654e-01, 8.72195045e-01, 7.70547044e-01, 6.55186690e-01,\n",
       "         1.44033363e-01, 5.82318352e-01, 8.87563243e-01, 6.16884570e-01],\n",
       "        [7.30531553e-01, 4.62802002e-01, 9.94410904e-01, 3.90251638e-01,\n",
       "         5.64977060e-01, 1.67160327e-01, 2.67825445e-01, 9.95866146e-01,\n",
       "         8.08811404e-01, 2.56823512e-01, 8.34068466e-01, 2.93932846e-01,\n",
       "         8.89915682e-01, 8.56638558e-02, 5.99260036e-02, 1.10285673e-01,\n",
       "         8.48286336e-01, 5.35055500e-02, 1.02488116e-01, 8.30266790e-01,\n",
       "         2.79489003e-01, 9.82492879e-01, 8.98581552e-01, 1.04131590e-01,\n",
       "         3.38495007e-01, 8.94206053e-01, 3.54508369e-01, 7.66872676e-01],\n",
       "        [9.74124118e-01, 3.26154104e-01, 2.93158372e-01, 1.08263517e-02,\n",
       "         2.73257905e-01, 9.34407630e-01, 8.60200901e-01, 9.70068430e-01,\n",
       "         4.36240185e-01, 2.51798374e-02, 1.52361842e-01, 9.25329560e-01,\n",
       "         2.85235008e-01, 4.18967957e-01, 5.37122433e-01, 1.10963147e-01,\n",
       "         2.25015494e-01, 2.25655911e-01, 8.25823664e-01, 9.47541493e-01,\n",
       "         6.31456556e-01, 7.83629283e-01, 7.52799556e-01, 4.11103667e-01,\n",
       "         8.42876581e-02, 5.12621600e-01, 6.94373272e-01, 9.76172778e-01],\n",
       "        [6.26721244e-01, 9.86661435e-01, 7.23616061e-01, 9.86535925e-01,\n",
       "         6.36988021e-01, 9.01876588e-01, 2.63703337e-01, 9.16215881e-01,\n",
       "         9.24640995e-01, 2.54395223e-01, 5.45686911e-01, 3.47766575e-01,\n",
       "         3.84770957e-01, 3.31182522e-01, 9.80796261e-01, 3.37144183e-01,\n",
       "         4.99442512e-01, 2.11354662e-01, 5.14167900e-02, 3.52337708e-01,\n",
       "         4.90218645e-01, 6.62608067e-02, 9.91509370e-01, 4.73926767e-02,\n",
       "         1.60713872e-01, 1.63014923e-01, 9.29049059e-01, 1.10936984e-02],\n",
       "        [6.52153833e-01, 4.70431555e-01, 9.05789077e-01, 9.19024415e-02,\n",
       "         5.67460169e-01, 3.13142963e-01, 4.59224707e-02, 2.84648384e-02,\n",
       "         6.15996687e-01, 2.67650060e-01, 3.18316919e-01, 3.70369282e-01,\n",
       "         6.95856418e-01, 8.27504740e-01, 5.24797914e-01, 5.31107685e-02,\n",
       "         4.22035663e-01, 4.02907157e-01, 7.36142203e-01, 5.21769960e-01,\n",
       "         7.59734676e-01, 3.66871430e-01, 6.29779461e-01, 4.41416895e-01,\n",
       "         9.75567169e-01, 7.87917904e-01, 1.18531271e-01, 7.09003326e-01],\n",
       "        [3.93456320e-03, 8.96670771e-01, 4.14019661e-02, 8.72507818e-01,\n",
       "         9.62532104e-01, 4.25604273e-01, 1.75301714e-01, 7.54167516e-01,\n",
       "         6.48633192e-01, 3.80139654e-01, 8.83266120e-01, 3.30259180e-03,\n",
       "         2.99401467e-01, 6.83130540e-01, 2.32585905e-01, 3.23234221e-01,\n",
       "         8.13452693e-01, 7.05219063e-02, 5.34222428e-01, 2.01264241e-01,\n",
       "         9.94612134e-01, 4.13084999e-01, 5.73570032e-01, 1.97571868e-01,\n",
       "         1.70254610e-01, 3.36316120e-02, 7.69588421e-01, 5.09291886e-01],\n",
       "        [3.49352698e-02, 3.05238666e-01, 6.86837255e-01, 3.81428576e-01,\n",
       "         4.77864176e-01, 5.91192471e-02, 9.02024864e-01, 2.04654264e-01,\n",
       "         6.21395358e-01, 8.87071061e-01, 1.39249278e-01, 2.93589113e-01,\n",
       "         1.16455538e-01, 6.70833134e-02, 4.32736422e-01, 4.70394018e-01,\n",
       "         5.32555587e-01, 5.98650557e-01, 5.81327142e-01, 4.26489591e-01,\n",
       "         4.40997623e-02, 9.55765040e-01, 7.79774426e-01, 9.67108171e-01,\n",
       "         7.29872141e-01, 3.67275971e-01, 8.30598825e-01, 7.94943956e-01],\n",
       "        [2.25075100e-01, 3.02555337e-01, 3.97273188e-01, 7.06103367e-01,\n",
       "         4.34655134e-01, 9.24443511e-01, 5.17193733e-01, 1.71542019e-01,\n",
       "         4.44815949e-01, 2.78883042e-01, 1.52584465e-01, 4.06146094e-01,\n",
       "         8.79576908e-01, 4.00945907e-02, 3.01309737e-01, 9.34882185e-01,\n",
       "         5.57135156e-01, 6.95467183e-01, 7.63823150e-01, 1.13552313e-01,\n",
       "         8.99063125e-01, 4.21814558e-02, 4.49986544e-01, 9.77386658e-01,\n",
       "         9.41450370e-01, 8.95603660e-01, 9.59213779e-01, 8.39220264e-01],\n",
       "        [9.95027364e-01, 9.92942256e-01, 5.84524901e-01, 1.95098914e-01,\n",
       "         5.50631637e-01, 9.68402407e-01, 1.18772361e-01, 7.67563032e-01,\n",
       "         5.46876617e-01, 1.89881180e-01, 9.26195907e-01, 9.73823217e-01,\n",
       "         1.60619840e-01, 9.57370798e-01, 1.31921927e-01, 6.07011205e-01,\n",
       "         7.69566381e-01, 4.57919840e-01, 7.33909847e-01, 5.48117550e-01,\n",
       "         3.19269934e-01, 9.12366185e-02, 3.83055841e-01, 9.30141153e-01,\n",
       "         8.14312909e-01, 4.04478371e-01, 9.26543733e-01, 5.44036110e-01],\n",
       "        [5.10643968e-01, 4.23510434e-01, 4.56364280e-01, 4.34505166e-01,\n",
       "         9.10749060e-01, 2.16893764e-01, 5.06325869e-01, 6.02613753e-01,\n",
       "         2.10519639e-01, 1.85444684e-01, 1.82153603e-01, 1.24733025e-01,\n",
       "         8.55134093e-01, 6.39692534e-01, 8.47678580e-01, 4.83319461e-01,\n",
       "         8.01395361e-01, 7.47749269e-01, 6.79459274e-01, 1.51254038e-01,\n",
       "         1.26974978e-01, 6.43272007e-01, 1.42733136e-02, 5.15238730e-01,\n",
       "         9.04661758e-01, 7.67411262e-01, 5.99970798e-01, 8.82958758e-01],\n",
       "        [7.10961456e-01, 8.16316179e-01, 6.78992158e-02, 2.99224545e-01,\n",
       "         8.83658631e-01, 8.41968716e-01, 2.87901813e-01, 5.09095988e-01,\n",
       "         2.92209056e-01, 9.17131731e-02, 9.39018923e-01, 2.38189622e-01,\n",
       "         6.13963410e-01, 6.94804996e-01, 4.51271322e-01, 2.73689458e-01,\n",
       "         6.77301448e-01, 8.27044289e-01, 2.89596655e-01, 2.36691620e-01,\n",
       "         3.18999345e-01, 9.57058947e-01, 6.91832568e-01, 6.85378228e-01,\n",
       "         1.27202713e-01, 2.22396932e-01, 3.48753364e-01, 8.53845952e-01],\n",
       "        [3.14109109e-01, 9.44757093e-01, 1.25449607e-01, 2.38825852e-01,\n",
       "         6.39061682e-01, 8.66674353e-01, 5.24151629e-01, 9.23000749e-01,\n",
       "         7.15555233e-01, 5.60505874e-01, 2.28821361e-01, 4.99951572e-01,\n",
       "         4.59477380e-01, 9.29535569e-01, 7.60878360e-01, 5.13058285e-01,\n",
       "         6.62604756e-01, 8.83906341e-01, 8.05526907e-01, 8.06149775e-02,\n",
       "         4.79829025e-01, 7.55077324e-01, 3.70613990e-01, 5.67992104e-01,\n",
       "         7.18973738e-01, 4.11721922e-01, 7.36558975e-01, 8.73127882e-01],\n",
       "        [6.69270452e-01, 1.99534479e-01, 5.68209585e-01, 5.28925107e-01,\n",
       "         9.81503671e-01, 6.54795381e-01, 3.28058988e-01, 2.33607192e-01,\n",
       "         2.59145135e-01, 2.60915942e-01, 7.89961094e-04, 8.63444169e-01,\n",
       "         8.57948418e-01, 4.02970282e-01, 7.30255227e-01, 7.49101222e-01,\n",
       "         3.61811576e-01, 9.78078651e-01, 5.90592103e-01, 1.18587523e-01,\n",
       "         4.14723289e-01, 7.63117837e-01, 8.38906328e-01, 7.46479042e-01,\n",
       "         6.66983889e-01, 8.99403418e-01, 4.40716109e-01, 1.76086795e-01],\n",
       "        [2.47821004e-01, 3.96820655e-01, 1.69292662e-01, 2.98796241e-02,\n",
       "         7.98725116e-01, 4.70518193e-01, 7.22087879e-01, 8.44047608e-01,\n",
       "         7.30446959e-02, 9.30154199e-01, 8.55678507e-01, 3.38097714e-01,\n",
       "         7.49867011e-01, 3.26613811e-01, 9.20366098e-01, 7.27564909e-01,\n",
       "         4.64064407e-01, 9.92916751e-01, 9.68663648e-01, 3.48472729e-01,\n",
       "         8.21696664e-01, 2.64715834e-01, 8.72673491e-01, 9.37826911e-03,\n",
       "         4.13383843e-01, 8.51204870e-01, 2.57853511e-01, 6.67650934e-02],\n",
       "        [6.87607603e-01, 4.30612602e-01, 7.64326515e-01, 7.16018323e-01,\n",
       "         8.77836147e-01, 4.85874783e-01, 4.94466473e-01, 9.93378445e-01,\n",
       "         3.51539333e-01, 8.68921544e-01, 3.78839367e-01, 3.09928150e-02,\n",
       "         5.08680788e-01, 4.25594342e-01, 4.56158629e-02, 3.61307389e-02,\n",
       "         8.34521464e-01, 6.89400928e-01, 3.54471265e-01, 3.70876710e-01,\n",
       "         2.70491236e-01, 7.56643723e-01, 2.77347288e-01, 3.32264341e-01,\n",
       "         6.35897587e-02, 9.73149091e-01, 9.23684764e-01, 6.67110150e-01],\n",
       "        [1.30081408e-01, 8.66962585e-01, 7.44222355e-01, 2.04691575e-02,\n",
       "         8.30229173e-01, 2.71929152e-01, 2.59770140e-01, 8.73478214e-01,\n",
       "         3.22942180e-02, 6.31556391e-01, 5.49967204e-01, 4.91517955e-01,\n",
       "         6.56383105e-01, 8.03531557e-01, 7.19656630e-02, 4.04062838e-01,\n",
       "         2.77559260e-01, 5.76218884e-01, 3.86220804e-01, 9.74676778e-01,\n",
       "         7.92176738e-01, 8.36897651e-01, 2.04380734e-02, 5.25620763e-01,\n",
       "         2.61821273e-01, 9.72659647e-01, 9.21193147e-01, 8.38349535e-01],\n",
       "        [1.21759877e-01, 5.73712527e-01, 5.17301586e-01, 8.16756301e-01,\n",
       "         9.56303888e-02, 2.22705330e-01, 8.24460148e-01, 7.74013995e-01,\n",
       "         1.44667271e-02, 3.60542717e-01, 1.13153796e-02, 8.42272504e-01,\n",
       "         3.21917521e-01, 2.93176769e-01, 4.39596675e-01, 5.90079327e-01,\n",
       "         6.95190606e-01, 8.57764309e-01, 4.39779361e-02, 2.34475046e-01,\n",
       "         9.23774325e-01, 5.62962500e-01, 8.83878708e-02, 7.19880174e-01,\n",
       "         3.66386539e-01, 4.90704628e-01, 6.54887237e-01, 2.41924833e-01],\n",
       "        [4.30607316e-01, 8.50645178e-01, 3.83752218e-01, 8.07193188e-01,\n",
       "         2.29036054e-01, 3.50575400e-01, 4.29780320e-01, 6.49158155e-02,\n",
       "         4.83386384e-01, 1.12069059e-03, 1.40539110e-01, 2.83779274e-02,\n",
       "         1.58863019e-01, 2.72489296e-01, 4.93321667e-01, 8.91317800e-01,\n",
       "         6.06094626e-01, 6.85104491e-01, 4.27485808e-01, 8.23814007e-02,\n",
       "         5.33927677e-01, 9.14632718e-01, 1.31645760e-01, 2.48447007e-01,\n",
       "         1.24919444e-01, 2.98597112e-01, 3.49603064e-01, 4.18238782e-01],\n",
       "        [3.44328556e-01, 5.49074463e-01, 1.78456168e-01, 3.43263532e-02,\n",
       "         1.61046069e-01, 8.69154034e-01, 9.89366323e-01, 3.09630021e-01,\n",
       "         1.56562865e-01, 8.40967237e-01, 3.72818502e-01, 5.47296474e-01,\n",
       "         1.72672423e-01, 3.70483265e-01, 8.73434638e-01, 2.91186187e-01,\n",
       "         9.64862695e-01, 5.25278876e-01, 6.63976623e-01, 9.48297193e-01,\n",
       "         4.76113973e-01, 8.16159441e-01, 8.55971918e-01, 6.14893071e-01,\n",
       "         4.65115190e-01, 7.17584972e-01, 9.89765478e-01, 4.17787252e-01],\n",
       "        [7.67645603e-01, 2.03649349e-01, 6.72814456e-01, 5.72401147e-01,\n",
       "         9.83094886e-01, 1.45650698e-01, 9.31951936e-01, 7.41581434e-01,\n",
       "         4.14844030e-01, 5.73314333e-01, 8.24590086e-01, 9.50060176e-01,\n",
       "         5.54007358e-01, 2.31935860e-01, 8.00961977e-01, 2.35744438e-01,\n",
       "         7.39587680e-01, 8.91745915e-01, 7.28994376e-01, 2.07608052e-01,\n",
       "         8.46257546e-01, 1.79368450e-01, 6.60730583e-01, 4.89870362e-01,\n",
       "         1.77606753e-02, 1.17271072e-01, 1.43177773e-01, 9.41097759e-01],\n",
       "        [4.10122902e-01, 2.08494351e-01, 5.12786758e-01, 5.61935107e-01,\n",
       "         3.42553274e-01, 8.51748587e-01, 7.30034006e-01, 7.28945382e-01,\n",
       "         7.54624473e-01, 7.75052674e-01, 9.54294785e-01, 8.13985716e-01,\n",
       "         3.38120185e-01, 5.14094615e-01, 3.33435427e-01, 1.62329884e-01,\n",
       "         2.40644042e-01, 1.74293621e-01, 5.61518602e-01, 5.35084859e-01,\n",
       "         3.40062210e-01, 5.43232404e-01, 2.57921963e-02, 1.76547893e-01,\n",
       "         1.92642493e-01, 6.25077423e-01, 9.50990134e-01, 2.77014264e-01]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79c2635d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.20941670e-01, 1.11516217e-01, 2.51869407e-01, 6.11205881e-01,\n",
       "        1.25727377e-01, 1.42771752e-01, 3.60279290e-01, 9.15966021e-01,\n",
       "        9.94927269e-01, 1.91022795e-01, 6.68080067e-01, 4.28422139e-01,\n",
       "        3.70627183e-01, 4.09742072e-01, 7.16127122e-01, 1.55298496e-01,\n",
       "        3.67282189e-02, 8.90569090e-01, 5.25854473e-01, 8.49884103e-01,\n",
       "        9.70199584e-01, 6.79570226e-01, 8.78270630e-01, 4.95851225e-01,\n",
       "        8.95929668e-01, 3.18780976e-01, 8.81174977e-01, 2.19279143e-01],\n",
       "       [9.19617965e-02, 4.27190487e-01, 5.16067578e-01, 7.47736497e-01,\n",
       "        1.69674927e-02, 6.52084125e-01, 9.45248208e-01, 6.50876896e-01,\n",
       "        8.70127078e-01, 6.12521605e-01, 3.48992634e-01, 3.08095590e-01,\n",
       "        9.27988608e-01, 2.09074395e-01, 1.02554848e-01, 5.68100502e-01,\n",
       "        3.25551174e-01, 8.70108697e-02, 3.07844623e-01, 8.23383124e-01,\n",
       "        2.88234285e-01, 5.73702443e-02, 9.39779052e-01, 7.36109220e-02,\n",
       "        3.10389373e-01, 8.65461313e-01, 4.47196521e-01, 1.30961955e-01],\n",
       "       [2.31600095e-02, 2.77348701e-02, 9.74350270e-01, 8.26376765e-01,\n",
       "        8.04626434e-02, 9.10363969e-01, 2.21468607e-01, 4.80711532e-02,\n",
       "        4.95370920e-01, 9.60415263e-01, 3.34941468e-01, 1.43735883e-01,\n",
       "        5.09561252e-01, 2.87204574e-01, 4.73272558e-01, 1.22850812e-01,\n",
       "        5.63254211e-01, 3.01690651e-01, 5.91057148e-01, 3.06734331e-01,\n",
       "        4.02782705e-01, 7.69152296e-01, 7.59663917e-01, 1.64468436e-01,\n",
       "        8.89508540e-01, 9.10988395e-01, 9.53635909e-01, 9.20901086e-01],\n",
       "       [5.35871030e-01, 7.72396434e-01, 3.67374524e-02, 6.70556688e-01,\n",
       "        6.44598973e-01, 8.86200631e-01, 5.98968967e-01, 3.23603123e-01,\n",
       "        5.52911049e-01, 6.53949037e-01, 4.18323644e-01, 8.90304503e-01,\n",
       "        9.41200666e-01, 4.33390093e-02, 7.34389485e-01, 8.86542850e-01,\n",
       "        9.76346370e-01, 9.60097059e-01, 6.07289295e-01, 7.32509687e-01,\n",
       "        1.65925262e-01, 7.25845388e-01, 8.82093566e-01, 1.02716591e-01,\n",
       "        7.91101920e-01, 7.61616868e-01, 2.18886992e-01, 1.72206054e-01],\n",
       "       [9.91183277e-01, 2.99565650e-01, 5.26286200e-01, 4.93788592e-02,\n",
       "        9.80123130e-01, 2.68349413e-01, 9.78694729e-01, 6.68147453e-01,\n",
       "        3.65917879e-01, 5.04915341e-01, 8.45582976e-01, 4.85649400e-01,\n",
       "        1.29426908e-01, 9.48103840e-01, 6.97338218e-01, 8.84667330e-01,\n",
       "        7.21389472e-01, 5.84530056e-01, 7.62048091e-01, 1.13907446e-01,\n",
       "        4.24324618e-01, 6.41195767e-01, 5.75158865e-01, 9.55286570e-01,\n",
       "        8.59603437e-01, 8.58282144e-02, 2.57081988e-01, 3.93579707e-01],\n",
       "       [8.98373228e-01, 8.10178004e-01, 6.42700531e-01, 3.84655006e-01,\n",
       "        9.52281195e-01, 3.67355310e-01, 1.10048482e-01, 5.84920482e-01,\n",
       "        3.19170892e-01, 5.50141984e-01, 5.89124215e-01, 9.97307611e-01,\n",
       "        1.17573414e-01, 3.76813610e-01, 4.56090854e-01, 9.97655676e-01,\n",
       "        9.31773682e-01, 2.68301716e-01, 2.54094937e-01, 1.33227188e-01,\n",
       "        6.33970708e-02, 5.31762461e-02, 8.02253335e-01, 2.10119284e-01,\n",
       "        9.48356136e-01, 4.84037188e-01, 3.47245350e-01, 2.99921113e-01],\n",
       "       [6.68679730e-01, 6.54440090e-01, 4.19136947e-01, 1.44307871e-01,\n",
       "        7.35538834e-01, 2.67405600e-01, 7.11853616e-01, 3.39929025e-01,\n",
       "        2.37173116e-01, 2.31490481e-02, 2.99333527e-01, 8.94734643e-01,\n",
       "        8.66824890e-01, 3.48941091e-01, 1.86170592e-01, 8.97522213e-01,\n",
       "        7.67854906e-02, 7.17929140e-02, 2.15884760e-01, 4.08049265e-01,\n",
       "        7.24382078e-01, 6.04951549e-01, 4.67337329e-01, 8.42083940e-01,\n",
       "        2.99811872e-01, 7.39649702e-01, 4.74773373e-01, 4.75563442e-03],\n",
       "       [1.24938547e-01, 1.64911275e-01, 8.99388638e-01, 8.18480294e-01,\n",
       "        9.26656137e-01, 6.35190210e-01, 8.45528526e-01, 7.36149114e-02,\n",
       "        6.51620123e-02, 8.61725419e-01, 6.69259712e-01, 5.99166649e-01,\n",
       "        5.04573462e-02, 8.82871691e-02, 1.95738340e-01, 3.92022891e-02,\n",
       "        6.79872014e-01, 9.71393925e-01, 7.77563668e-01, 3.16227979e-01,\n",
       "        7.96038654e-01, 8.72195045e-01, 7.70547044e-01, 6.55186690e-01,\n",
       "        1.44033363e-01, 5.82318352e-01, 8.87563243e-01, 6.16884570e-01],\n",
       "       [7.30531553e-01, 4.62802002e-01, 9.94410904e-01, 3.90251638e-01,\n",
       "        5.64977060e-01, 1.67160327e-01, 2.67825445e-01, 9.95866146e-01,\n",
       "        8.08811404e-01, 2.56823512e-01, 8.34068466e-01, 2.93932846e-01,\n",
       "        8.89915682e-01, 8.56638558e-02, 5.99260036e-02, 1.10285673e-01,\n",
       "        8.48286336e-01, 5.35055500e-02, 1.02488116e-01, 8.30266790e-01,\n",
       "        2.79489003e-01, 9.82492879e-01, 8.98581552e-01, 1.04131590e-01,\n",
       "        3.38495007e-01, 8.94206053e-01, 3.54508369e-01, 7.66872676e-01],\n",
       "       [9.74124118e-01, 3.26154104e-01, 2.93158372e-01, 1.08263517e-02,\n",
       "        2.73257905e-01, 9.34407630e-01, 8.60200901e-01, 9.70068430e-01,\n",
       "        4.36240185e-01, 2.51798374e-02, 1.52361842e-01, 9.25329560e-01,\n",
       "        2.85235008e-01, 4.18967957e-01, 5.37122433e-01, 1.10963147e-01,\n",
       "        2.25015494e-01, 2.25655911e-01, 8.25823664e-01, 9.47541493e-01,\n",
       "        6.31456556e-01, 7.83629283e-01, 7.52799556e-01, 4.11103667e-01,\n",
       "        8.42876581e-02, 5.12621600e-01, 6.94373272e-01, 9.76172778e-01],\n",
       "       [6.26721244e-01, 9.86661435e-01, 7.23616061e-01, 9.86535925e-01,\n",
       "        6.36988021e-01, 9.01876588e-01, 2.63703337e-01, 9.16215881e-01,\n",
       "        9.24640995e-01, 2.54395223e-01, 5.45686911e-01, 3.47766575e-01,\n",
       "        3.84770957e-01, 3.31182522e-01, 9.80796261e-01, 3.37144183e-01,\n",
       "        4.99442512e-01, 2.11354662e-01, 5.14167900e-02, 3.52337708e-01,\n",
       "        4.90218645e-01, 6.62608067e-02, 9.91509370e-01, 4.73926767e-02,\n",
       "        1.60713872e-01, 1.63014923e-01, 9.29049059e-01, 1.10936984e-02],\n",
       "       [6.52153833e-01, 4.70431555e-01, 9.05789077e-01, 9.19024415e-02,\n",
       "        5.67460169e-01, 3.13142963e-01, 4.59224707e-02, 2.84648384e-02,\n",
       "        6.15996687e-01, 2.67650060e-01, 3.18316919e-01, 3.70369282e-01,\n",
       "        6.95856418e-01, 8.27504740e-01, 5.24797914e-01, 5.31107685e-02,\n",
       "        4.22035663e-01, 4.02907157e-01, 7.36142203e-01, 5.21769960e-01,\n",
       "        7.59734676e-01, 3.66871430e-01, 6.29779461e-01, 4.41416895e-01,\n",
       "        9.75567169e-01, 7.87917904e-01, 1.18531271e-01, 7.09003326e-01],\n",
       "       [3.93456320e-03, 8.96670771e-01, 4.14019661e-02, 8.72507818e-01,\n",
       "        9.62532104e-01, 4.25604273e-01, 1.75301714e-01, 7.54167516e-01,\n",
       "        6.48633192e-01, 3.80139654e-01, 8.83266120e-01, 3.30259180e-03,\n",
       "        2.99401467e-01, 6.83130540e-01, 2.32585905e-01, 3.23234221e-01,\n",
       "        8.13452693e-01, 7.05219063e-02, 5.34222428e-01, 2.01264241e-01,\n",
       "        9.94612134e-01, 4.13084999e-01, 5.73570032e-01, 1.97571868e-01,\n",
       "        1.70254610e-01, 3.36316120e-02, 7.69588421e-01, 5.09291886e-01],\n",
       "       [3.49352698e-02, 3.05238666e-01, 6.86837255e-01, 3.81428576e-01,\n",
       "        4.77864176e-01, 5.91192471e-02, 9.02024864e-01, 2.04654264e-01,\n",
       "        6.21395358e-01, 8.87071061e-01, 1.39249278e-01, 2.93589113e-01,\n",
       "        1.16455538e-01, 6.70833134e-02, 4.32736422e-01, 4.70394018e-01,\n",
       "        5.32555587e-01, 5.98650557e-01, 5.81327142e-01, 4.26489591e-01,\n",
       "        4.40997623e-02, 9.55765040e-01, 7.79774426e-01, 9.67108171e-01,\n",
       "        7.29872141e-01, 3.67275971e-01, 8.30598825e-01, 7.94943956e-01],\n",
       "       [2.25075100e-01, 3.02555337e-01, 3.97273188e-01, 7.06103367e-01,\n",
       "        4.34655134e-01, 9.24443511e-01, 5.17193733e-01, 1.71542019e-01,\n",
       "        4.44815949e-01, 2.78883042e-01, 1.52584465e-01, 4.06146094e-01,\n",
       "        8.79576908e-01, 4.00945907e-02, 3.01309737e-01, 9.34882185e-01,\n",
       "        5.57135156e-01, 6.95467183e-01, 7.63823150e-01, 1.13552313e-01,\n",
       "        8.99063125e-01, 4.21814558e-02, 4.49986544e-01, 9.77386658e-01,\n",
       "        9.41450370e-01, 8.95603660e-01, 9.59213779e-01, 8.39220264e-01],\n",
       "       [9.95027364e-01, 9.92942256e-01, 5.84524901e-01, 1.95098914e-01,\n",
       "        5.50631637e-01, 9.68402407e-01, 1.18772361e-01, 7.67563032e-01,\n",
       "        5.46876617e-01, 1.89881180e-01, 9.26195907e-01, 9.73823217e-01,\n",
       "        1.60619840e-01, 9.57370798e-01, 1.31921927e-01, 6.07011205e-01,\n",
       "        7.69566381e-01, 4.57919840e-01, 7.33909847e-01, 5.48117550e-01,\n",
       "        3.19269934e-01, 9.12366185e-02, 3.83055841e-01, 9.30141153e-01,\n",
       "        8.14312909e-01, 4.04478371e-01, 9.26543733e-01, 5.44036110e-01],\n",
       "       [5.10643968e-01, 4.23510434e-01, 4.56364280e-01, 4.34505166e-01,\n",
       "        9.10749060e-01, 2.16893764e-01, 5.06325869e-01, 6.02613753e-01,\n",
       "        2.10519639e-01, 1.85444684e-01, 1.82153603e-01, 1.24733025e-01,\n",
       "        8.55134093e-01, 6.39692534e-01, 8.47678580e-01, 4.83319461e-01,\n",
       "        8.01395361e-01, 7.47749269e-01, 6.79459274e-01, 1.51254038e-01,\n",
       "        1.26974978e-01, 6.43272007e-01, 1.42733136e-02, 5.15238730e-01,\n",
       "        9.04661758e-01, 7.67411262e-01, 5.99970798e-01, 8.82958758e-01],\n",
       "       [7.10961456e-01, 8.16316179e-01, 6.78992158e-02, 2.99224545e-01,\n",
       "        8.83658631e-01, 8.41968716e-01, 2.87901813e-01, 5.09095988e-01,\n",
       "        2.92209056e-01, 9.17131731e-02, 9.39018923e-01, 2.38189622e-01,\n",
       "        6.13963410e-01, 6.94804996e-01, 4.51271322e-01, 2.73689458e-01,\n",
       "        6.77301448e-01, 8.27044289e-01, 2.89596655e-01, 2.36691620e-01,\n",
       "        3.18999345e-01, 9.57058947e-01, 6.91832568e-01, 6.85378228e-01,\n",
       "        1.27202713e-01, 2.22396932e-01, 3.48753364e-01, 8.53845952e-01],\n",
       "       [3.14109109e-01, 9.44757093e-01, 1.25449607e-01, 2.38825852e-01,\n",
       "        6.39061682e-01, 8.66674353e-01, 5.24151629e-01, 9.23000749e-01,\n",
       "        7.15555233e-01, 5.60505874e-01, 2.28821361e-01, 4.99951572e-01,\n",
       "        4.59477380e-01, 9.29535569e-01, 7.60878360e-01, 5.13058285e-01,\n",
       "        6.62604756e-01, 8.83906341e-01, 8.05526907e-01, 8.06149775e-02,\n",
       "        4.79829025e-01, 7.55077324e-01, 3.70613990e-01, 5.67992104e-01,\n",
       "        7.18973738e-01, 4.11721922e-01, 7.36558975e-01, 8.73127882e-01],\n",
       "       [6.69270452e-01, 1.99534479e-01, 5.68209585e-01, 5.28925107e-01,\n",
       "        9.81503671e-01, 6.54795381e-01, 3.28058988e-01, 2.33607192e-01,\n",
       "        2.59145135e-01, 2.60915942e-01, 7.89961094e-04, 8.63444169e-01,\n",
       "        8.57948418e-01, 4.02970282e-01, 7.30255227e-01, 7.49101222e-01,\n",
       "        3.61811576e-01, 9.78078651e-01, 5.90592103e-01, 1.18587523e-01,\n",
       "        4.14723289e-01, 7.63117837e-01, 8.38906328e-01, 7.46479042e-01,\n",
       "        6.66983889e-01, 8.99403418e-01, 4.40716109e-01, 1.76086795e-01],\n",
       "       [2.47821004e-01, 3.96820655e-01, 1.69292662e-01, 2.98796241e-02,\n",
       "        7.98725116e-01, 4.70518193e-01, 7.22087879e-01, 8.44047608e-01,\n",
       "        7.30446959e-02, 9.30154199e-01, 8.55678507e-01, 3.38097714e-01,\n",
       "        7.49867011e-01, 3.26613811e-01, 9.20366098e-01, 7.27564909e-01,\n",
       "        4.64064407e-01, 9.92916751e-01, 9.68663648e-01, 3.48472729e-01,\n",
       "        8.21696664e-01, 2.64715834e-01, 8.72673491e-01, 9.37826911e-03,\n",
       "        4.13383843e-01, 8.51204870e-01, 2.57853511e-01, 6.67650934e-02],\n",
       "       [6.87607603e-01, 4.30612602e-01, 7.64326515e-01, 7.16018323e-01,\n",
       "        8.77836147e-01, 4.85874783e-01, 4.94466473e-01, 9.93378445e-01,\n",
       "        3.51539333e-01, 8.68921544e-01, 3.78839367e-01, 3.09928150e-02,\n",
       "        5.08680788e-01, 4.25594342e-01, 4.56158629e-02, 3.61307389e-02,\n",
       "        8.34521464e-01, 6.89400928e-01, 3.54471265e-01, 3.70876710e-01,\n",
       "        2.70491236e-01, 7.56643723e-01, 2.77347288e-01, 3.32264341e-01,\n",
       "        6.35897587e-02, 9.73149091e-01, 9.23684764e-01, 6.67110150e-01],\n",
       "       [1.30081408e-01, 8.66962585e-01, 7.44222355e-01, 2.04691575e-02,\n",
       "        8.30229173e-01, 2.71929152e-01, 2.59770140e-01, 8.73478214e-01,\n",
       "        3.22942180e-02, 6.31556391e-01, 5.49967204e-01, 4.91517955e-01,\n",
       "        6.56383105e-01, 8.03531557e-01, 7.19656630e-02, 4.04062838e-01,\n",
       "        2.77559260e-01, 5.76218884e-01, 3.86220804e-01, 9.74676778e-01,\n",
       "        7.92176738e-01, 8.36897651e-01, 2.04380734e-02, 5.25620763e-01,\n",
       "        2.61821273e-01, 9.72659647e-01, 9.21193147e-01, 8.38349535e-01],\n",
       "       [1.21759877e-01, 5.73712527e-01, 5.17301586e-01, 8.16756301e-01,\n",
       "        9.56303888e-02, 2.22705330e-01, 8.24460148e-01, 7.74013995e-01,\n",
       "        1.44667271e-02, 3.60542717e-01, 1.13153796e-02, 8.42272504e-01,\n",
       "        3.21917521e-01, 2.93176769e-01, 4.39596675e-01, 5.90079327e-01,\n",
       "        6.95190606e-01, 8.57764309e-01, 4.39779361e-02, 2.34475046e-01,\n",
       "        9.23774325e-01, 5.62962500e-01, 8.83878708e-02, 7.19880174e-01,\n",
       "        3.66386539e-01, 4.90704628e-01, 6.54887237e-01, 2.41924833e-01],\n",
       "       [4.30607316e-01, 8.50645178e-01, 3.83752218e-01, 8.07193188e-01,\n",
       "        2.29036054e-01, 3.50575400e-01, 4.29780320e-01, 6.49158155e-02,\n",
       "        4.83386384e-01, 1.12069059e-03, 1.40539110e-01, 2.83779274e-02,\n",
       "        1.58863019e-01, 2.72489296e-01, 4.93321667e-01, 8.91317800e-01,\n",
       "        6.06094626e-01, 6.85104491e-01, 4.27485808e-01, 8.23814007e-02,\n",
       "        5.33927677e-01, 9.14632718e-01, 1.31645760e-01, 2.48447007e-01,\n",
       "        1.24919444e-01, 2.98597112e-01, 3.49603064e-01, 4.18238782e-01],\n",
       "       [3.44328556e-01, 5.49074463e-01, 1.78456168e-01, 3.43263532e-02,\n",
       "        1.61046069e-01, 8.69154034e-01, 9.89366323e-01, 3.09630021e-01,\n",
       "        1.56562865e-01, 8.40967237e-01, 3.72818502e-01, 5.47296474e-01,\n",
       "        1.72672423e-01, 3.70483265e-01, 8.73434638e-01, 2.91186187e-01,\n",
       "        9.64862695e-01, 5.25278876e-01, 6.63976623e-01, 9.48297193e-01,\n",
       "        4.76113973e-01, 8.16159441e-01, 8.55971918e-01, 6.14893071e-01,\n",
       "        4.65115190e-01, 7.17584972e-01, 9.89765478e-01, 4.17787252e-01],\n",
       "       [7.67645603e-01, 2.03649349e-01, 6.72814456e-01, 5.72401147e-01,\n",
       "        9.83094886e-01, 1.45650698e-01, 9.31951936e-01, 7.41581434e-01,\n",
       "        4.14844030e-01, 5.73314333e-01, 8.24590086e-01, 9.50060176e-01,\n",
       "        5.54007358e-01, 2.31935860e-01, 8.00961977e-01, 2.35744438e-01,\n",
       "        7.39587680e-01, 8.91745915e-01, 7.28994376e-01, 2.07608052e-01,\n",
       "        8.46257546e-01, 1.79368450e-01, 6.60730583e-01, 4.89870362e-01,\n",
       "        1.77606753e-02, 1.17271072e-01, 1.43177773e-01, 9.41097759e-01],\n",
       "       [4.10122902e-01, 2.08494351e-01, 5.12786758e-01, 5.61935107e-01,\n",
       "        3.42553274e-01, 8.51748587e-01, 7.30034006e-01, 7.28945382e-01,\n",
       "        7.54624473e-01, 7.75052674e-01, 9.54294785e-01, 8.13985716e-01,\n",
       "        3.38120185e-01, 5.14094615e-01, 3.33435427e-01, 1.62329884e-01,\n",
       "        2.40644042e-01, 1.74293621e-01, 5.61518602e-01, 5.35084859e-01,\n",
       "        3.40062210e-01, 5.43232404e-01, 2.57921963e-02, 1.76547893e-01,\n",
       "        1.92642493e-01, 6.25077423e-01, 9.50990134e-01, 2.77014264e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ac92ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "#im2col\n",
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1,3,7,7)\n",
    "coll = im2col(x1,5,5,stride=1,pad=0)\n",
    "print(coll.shape)\n",
    "\n",
    "x2 = np.random.rand(10,3,7,7)\n",
    "col2 = im2col(x2,5,5,stride=1,pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6302272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolutionクラスの実装\n",
    "class Convolution:\n",
    "    def __init__(self,W,b,stride=1,pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self,x):\n",
    "        FN,C,FH,FW = self.W.shape\n",
    "        N,C,H,W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_W = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x,FH,FW,self.stride,self.pad)\n",
    "        col_W = self.W.reshape(FN,-1).T\n",
    "        out = np.dot(col,col_W) + self.b\n",
    "\n",
    "        out = out.reshape(N,out_h,out_W,-1).transpose(0,3,1,2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "326e9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Poolingレイヤの実装\n",
    "class Pooling:\n",
    "    def __init__(self,pool_h,pool_w,stride=1,pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self,x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x,self.pool_h,self.pool_w,self.stride,self.pad)\n",
    "        col = col.reshape(-1,self.pool_h*self.pool_w)\n",
    "\n",
    "        out = np.max(col,axis=1)\n",
    "\n",
    "        out = out.reshape(N,out_h,out_w,C).transpose(0,3,1,2)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a84abe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a71485ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reluレイヤの実装\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self,dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b46f7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affineレイヤの実装\n",
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        out = np.dot(x,self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self,dout):\n",
    "        dx = np.dot(dout,self.W.T)\n",
    "        self.dW = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout,axis=0)\n",
    "\n",
    "\n",
    "        return dx         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6889b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "#交差エントロピー損失\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c665beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax-with-Lossレイヤの実装\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.losss = cross_entropy_error(self.y,self.t)\n",
    "\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4420554",
   "metadata": {},
   "source": [
    "### CNNの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "425b51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self,input_dim=(1,28,28),\n",
    "                 conv_param={\"filter_num\":30,\"filter_size\":5,\"pad\":0,\"stride\":1},\n",
    "                 hidden_size=100,output_size=10,weight_init_std=0.01):\n",
    "        filter_num = conv_param[\"filter_num\"]\n",
    "        filter_size = conv_param[\"filter_size\"]\n",
    "        filter_pad = conv_param[\"pad\"]\n",
    "        filter_stride = conv_param[\"stride\"]\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        #重みパラメーターの初期化\n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.rand(filter_num,input_dim[0],\n",
    "                                                             filter_size,filter_size)\n",
    "        self.params[\"b1\"] = np.zeros(filter_num)\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(pool_output_size,hidden_size)\n",
    "        self.params[\"b2\"] = np.zeros(hidden_size)\n",
    "        self.params[\"W3\"] = weight_init_std * np.random.randn(hidden_size,output_size)\n",
    "        self.params[\"b3\"] = np.zeros(output_size)\n",
    "\n",
    "        #レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers[\"Conv1\"] = Convolution(self.params[\"W1\"],\n",
    "                                           self.params[\"b1\"],\n",
    "                                           conv_param[\"stride\"],\n",
    "                                           conv_param[\"pad\"])\n",
    "        self.layers[\"Relu1\"] = Relu()\n",
    "        self.layers[\"Pool1\"] = Pooling(pool_h=2,pool_w=2,stride=2)\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"W2\"],self.params[\"b2\"])\n",
    "        self.layers[\"Relu2\"] = Relu()\n",
    "        self.layers[\"Affine2\"] = Affine(self.params[\"W3\"],self.params[\"b3\"])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y,t)\n",
    "\n",
    "    def gradient(self,x,t):\n",
    "        #forward\n",
    "        self.loss(x,t)\n",
    "        #backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "\n",
    "        grads = {}\n",
    "        grads[\"W1\"] = self.layers[\"Conv1\"].dW\n",
    "        grads[\"b1\"] = self.layers[\"Conv1\"].db\n",
    "        grads[\"W2\"] = self.layers[\"Affine1\"].dW\n",
    "        grads[\"b2\"] = self.layers[\"Affine1\"].db\n",
    "        grads[\"W3\"] = self.layers[\"Affine2\"].dW\n",
    "        grads[\"b3\"] = self.layers[\"Affine2\"].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6bb0571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299541594303412\n",
      "=== epoch:1, train acc:0.2, test acc:0.18 ===\n",
      "train loss:2.297906325725518\n",
      "train loss:2.294529462744023\n",
      "train loss:2.289038575357394\n",
      "train loss:2.279316988342388\n",
      "train loss:2.2674971148215874\n",
      "train loss:2.2619298868689266\n",
      "train loss:2.2512911974321828\n",
      "train loss:2.2179676551169267\n",
      "train loss:2.192993388399168\n",
      "train loss:2.1372087440416636\n",
      "train loss:2.1394170686006975\n",
      "train loss:2.0494828763303694\n",
      "train loss:2.059387417643817\n",
      "train loss:2.065578013096769\n",
      "train loss:2.01613774792426\n",
      "train loss:1.91279296740881\n",
      "train loss:1.9009824994865965\n",
      "train loss:1.876666114766724\n",
      "train loss:1.7804612670909181\n",
      "train loss:1.674089631532683\n",
      "train loss:1.5328407769186765\n",
      "train loss:1.5389739853061415\n",
      "train loss:1.394734103117456\n",
      "train loss:1.3869337520691287\n",
      "train loss:1.2499890513443688\n",
      "train loss:1.3350416335333386\n",
      "train loss:1.0419565383797365\n",
      "train loss:1.1531730002376197\n",
      "train loss:1.0287139438063906\n",
      "train loss:1.0766121447468615\n",
      "train loss:0.9332653426282078\n",
      "train loss:0.7722707441323813\n",
      "train loss:0.8311809750889484\n",
      "train loss:0.7894970146535382\n",
      "train loss:0.7150153611669754\n",
      "train loss:0.876104094847965\n",
      "train loss:0.8836816082576557\n",
      "train loss:0.7486711566568703\n",
      "train loss:0.758837944584606\n",
      "train loss:0.832096607480901\n",
      "train loss:0.6727582582930489\n",
      "train loss:0.650900940532464\n",
      "train loss:0.8344723156773735\n",
      "train loss:0.5419486406578222\n",
      "train loss:0.5400609971488198\n",
      "train loss:0.8480006144076236\n",
      "train loss:1.0426144980117171\n",
      "train loss:0.466342462201085\n",
      "train loss:0.6797877467119255\n",
      "train loss:0.5382097557887952\n",
      "train loss:0.5210952245380602\n",
      "train loss:0.4538105372247971\n",
      "train loss:0.6231763361348608\n",
      "train loss:0.6587980664025639\n",
      "train loss:0.7386492720572875\n",
      "train loss:0.6364925119286715\n",
      "train loss:0.46131282015910907\n",
      "train loss:0.5737214079484716\n",
      "train loss:0.6092308671017014\n",
      "train loss:0.46130199618493\n",
      "train loss:0.5987592942112352\n",
      "train loss:0.5121292779167274\n",
      "train loss:0.501676247586548\n",
      "train loss:0.4933306659216922\n",
      "train loss:0.5633364819684256\n",
      "train loss:0.4279634940404383\n",
      "train loss:0.6086118910214037\n",
      "train loss:0.2753287448325659\n",
      "train loss:0.5068808377803151\n",
      "train loss:0.5284873269158389\n",
      "train loss:0.3359437271634627\n",
      "train loss:0.5342348182175921\n",
      "train loss:0.4700996527792241\n",
      "train loss:0.35639155957820956\n",
      "train loss:0.3070610371313134\n",
      "train loss:0.6547669253622855\n",
      "train loss:0.3504928874024922\n",
      "train loss:0.4554925380795236\n",
      "train loss:0.5599696201302419\n",
      "train loss:0.38362289214007406\n",
      "train loss:0.34095540653192236\n",
      "train loss:0.3290077407675003\n",
      "train loss:0.41512893274677887\n",
      "train loss:0.5166676377356407\n",
      "train loss:0.25631585041799493\n",
      "train loss:0.2657285083042689\n",
      "train loss:0.45896111980915455\n",
      "train loss:0.3570243302583188\n",
      "train loss:0.3380407788910668\n",
      "train loss:0.45834438173776604\n",
      "train loss:0.3222528788841975\n",
      "train loss:0.37796897342896346\n",
      "train loss:0.6822260537468989\n",
      "train loss:0.28997377687606096\n",
      "train loss:0.5016341220784661\n",
      "train loss:0.4291541509952188\n",
      "train loss:0.3818912960481377\n",
      "train loss:0.2909081856680053\n",
      "train loss:0.37261178481843055\n",
      "train loss:0.27863229645353527\n",
      "train loss:0.38605350646697756\n",
      "train loss:0.42267385969871313\n",
      "train loss:0.4100910687928216\n",
      "train loss:0.29935676575220965\n",
      "train loss:0.2783321191221949\n",
      "train loss:0.3939878552444745\n",
      "train loss:0.26584778536570314\n",
      "train loss:0.4522693541933197\n",
      "train loss:0.3446804922218294\n",
      "train loss:0.5100700823954878\n",
      "train loss:0.3334895629021255\n",
      "train loss:0.3747422230784922\n",
      "train loss:0.42437142111226916\n",
      "train loss:0.49939596949875714\n",
      "train loss:0.22966263566039577\n",
      "train loss:0.3618583917003619\n",
      "train loss:0.3535170108104741\n",
      "train loss:0.17204819383642345\n",
      "train loss:0.31250766227344534\n",
      "train loss:0.2727652763653569\n",
      "train loss:0.4349482319895459\n",
      "train loss:0.39157901496963554\n",
      "train loss:0.2888430758630539\n",
      "train loss:0.24025107666105722\n",
      "train loss:0.30941082454633084\n",
      "train loss:0.4008192110841254\n",
      "train loss:0.4123163446033179\n",
      "train loss:0.2492568807126391\n",
      "train loss:0.28938600245847895\n",
      "train loss:0.27148233524327287\n",
      "train loss:0.22447870944045087\n",
      "train loss:0.21629065950127838\n",
      "train loss:0.47179025672165\n",
      "train loss:0.20367283063043817\n",
      "train loss:0.19677282422779882\n",
      "train loss:0.38607369432675853\n",
      "train loss:0.2592192094114622\n",
      "train loss:0.2253823492955766\n",
      "train loss:0.3294600084534902\n",
      "train loss:0.2685117302188705\n",
      "train loss:0.19819350647283174\n",
      "train loss:0.1955011827961847\n",
      "train loss:0.27344741560700564\n",
      "train loss:0.2711469057151242\n",
      "train loss:0.2808234869706181\n",
      "train loss:0.3191636821121284\n",
      "train loss:0.19882532471527362\n",
      "train loss:0.4132326405066465\n",
      "train loss:0.344325315602916\n",
      "train loss:0.2476932978992304\n",
      "train loss:0.34797377850907163\n",
      "train loss:0.3351298401784231\n",
      "train loss:0.33019866005734644\n",
      "train loss:0.4935887333405002\n",
      "train loss:0.21171700964769763\n",
      "train loss:0.45489962949587404\n",
      "train loss:0.2908292824643931\n",
      "train loss:0.34425390134797107\n",
      "train loss:0.2366680580883376\n",
      "train loss:0.3866732906821205\n",
      "train loss:0.5261433109117476\n",
      "train loss:0.41262404091771593\n",
      "train loss:0.2404727418713176\n",
      "train loss:0.24979590600256682\n",
      "train loss:0.18939858500839749\n",
      "train loss:0.32885243611757714\n",
      "train loss:0.22824229278937205\n",
      "train loss:0.3909574869415978\n",
      "train loss:0.2716417588693534\n",
      "train loss:0.34026541094698765\n",
      "train loss:0.3270092517598823\n",
      "train loss:0.32832733295997796\n",
      "train loss:0.16188213542060775\n",
      "train loss:0.17290562975978735\n",
      "train loss:0.2307532383734074\n",
      "train loss:0.3438908618083969\n",
      "train loss:0.260080839338566\n",
      "train loss:0.35474792758831547\n",
      "train loss:0.23192578934846264\n",
      "train loss:0.2461291373772011\n",
      "train loss:0.2916188008907058\n",
      "train loss:0.28414246158487744\n",
      "train loss:0.16160232977213412\n",
      "train loss:0.22674803699751864\n",
      "train loss:0.27832472542829556\n",
      "train loss:0.2630336291422322\n",
      "train loss:0.32043613016706823\n",
      "train loss:0.2202450499036033\n",
      "train loss:0.24694871108442967\n",
      "train loss:0.25793177278996815\n",
      "train loss:0.20517125551823134\n",
      "train loss:0.3063737330098631\n",
      "train loss:0.28764968194156265\n",
      "train loss:0.3331085374803239\n",
      "train loss:0.26088184131196324\n",
      "train loss:0.2606854125438107\n",
      "train loss:0.276454536773001\n",
      "train loss:0.23793267465868187\n",
      "train loss:0.23688305911684515\n",
      "train loss:0.23892485149069498\n",
      "train loss:0.3596099377590833\n",
      "train loss:0.21865683052886642\n",
      "train loss:0.19500322065956005\n",
      "train loss:0.2547504036237837\n",
      "train loss:0.24744299282641383\n",
      "train loss:0.23859995869520528\n",
      "train loss:0.28040034397043456\n",
      "train loss:0.18343615412577294\n",
      "train loss:0.15387314766851137\n",
      "train loss:0.14377270161435762\n",
      "train loss:0.20839479730284252\n",
      "train loss:0.2713556997827123\n",
      "train loss:0.13589630498903638\n",
      "train loss:0.13970469144676664\n",
      "train loss:0.2987496814838168\n",
      "train loss:0.36438172682031206\n",
      "train loss:0.36305441064950067\n",
      "train loss:0.26790948324245056\n",
      "train loss:0.19041927760501826\n",
      "train loss:0.2674187187239104\n",
      "train loss:0.3096236641700861\n",
      "train loss:0.20064897217776445\n",
      "train loss:0.20974385374934054\n",
      "train loss:0.21329532006847415\n",
      "train loss:0.24936038182505765\n",
      "train loss:0.4667663224122912\n",
      "train loss:0.14466496682349908\n",
      "train loss:0.14417126989728202\n",
      "train loss:0.4499958134798796\n",
      "train loss:0.2549896074595399\n",
      "train loss:0.18600695028747605\n",
      "train loss:0.1840513326734595\n",
      "train loss:0.08828503570637976\n",
      "train loss:0.31725682912759934\n",
      "train loss:0.2522092513803429\n",
      "train loss:0.39695887373755406\n",
      "train loss:0.20999103382114417\n",
      "train loss:0.3129644977927058\n",
      "train loss:0.2664048987689377\n",
      "train loss:0.27161623955326225\n",
      "train loss:0.1867180597621703\n",
      "train loss:0.3002421345548585\n",
      "train loss:0.2389719842629849\n",
      "train loss:0.281084425769493\n",
      "train loss:0.1746802025206424\n",
      "train loss:0.2297742818708702\n",
      "train loss:0.2067940121490456\n",
      "train loss:0.1995575170706797\n",
      "train loss:0.3923529844153881\n",
      "train loss:0.2719016028155346\n",
      "train loss:0.15270354256042695\n",
      "train loss:0.2221230296480987\n",
      "train loss:0.2455210786346471\n",
      "train loss:0.19475501880588925\n",
      "train loss:0.14674551442583328\n",
      "train loss:0.09867070966194966\n",
      "train loss:0.1265118920215924\n",
      "train loss:0.17386403263159328\n",
      "train loss:0.2086405220116589\n",
      "train loss:0.24522513111738625\n",
      "train loss:0.2115661167392096\n",
      "train loss:0.20326474095142566\n",
      "train loss:0.1740543302737165\n",
      "train loss:0.10383434943632082\n",
      "train loss:0.2115188953837919\n",
      "train loss:0.35861908779931445\n",
      "train loss:0.2860603026752307\n",
      "train loss:0.3288451639282755\n",
      "train loss:0.16713160428586665\n",
      "train loss:0.24745911291887387\n",
      "train loss:0.18867779132051166\n",
      "train loss:0.0955317637333839\n",
      "train loss:0.31231983340810304\n",
      "train loss:0.19330957009410443\n",
      "train loss:0.20756551957522973\n",
      "train loss:0.22748251217858229\n",
      "train loss:0.14134102393319892\n",
      "train loss:0.20863857063588537\n",
      "train loss:0.16165652942658648\n",
      "train loss:0.08524882556703757\n",
      "train loss:0.17477696877574453\n",
      "train loss:0.15189230159830822\n",
      "train loss:0.16055171099567417\n",
      "train loss:0.1876443645783567\n",
      "train loss:0.1509358007254334\n",
      "train loss:0.2497567297622744\n",
      "train loss:0.20432668588932695\n",
      "train loss:0.13666677911867275\n",
      "train loss:0.09561085675330717\n",
      "train loss:0.2545497475124189\n",
      "train loss:0.189031450273395\n",
      "train loss:0.17609356217846212\n",
      "train loss:0.28299130043757825\n",
      "train loss:0.49298306939872544\n",
      "train loss:0.1899626508872021\n",
      "train loss:0.1455387664814799\n",
      "train loss:0.1734000143941108\n",
      "train loss:0.17211467015707133\n",
      "train loss:0.2853872238144353\n",
      "train loss:0.2406548805470538\n",
      "train loss:0.10121806384353338\n",
      "train loss:0.2795332123448355\n",
      "train loss:0.11168758039373144\n",
      "train loss:0.15939555326389476\n",
      "train loss:0.18836925730979687\n",
      "train loss:0.10536731687771148\n",
      "train loss:0.10233998495924566\n",
      "train loss:0.1888150773167687\n",
      "train loss:0.09275466056336577\n",
      "train loss:0.08358282715979527\n",
      "train loss:0.21727048034515342\n",
      "train loss:0.08934546867212491\n",
      "train loss:0.19864323218592578\n",
      "train loss:0.2506357956329336\n",
      "train loss:0.17520522905165145\n",
      "train loss:0.1670640502855137\n",
      "train loss:0.1298141573440655\n",
      "train loss:0.25974333768672947\n",
      "train loss:0.13054862634269299\n",
      "train loss:0.1369483848656994\n",
      "train loss:0.21963618342502755\n",
      "train loss:0.28785017739558866\n",
      "train loss:0.46178381819330455\n",
      "train loss:0.14580764575463143\n",
      "train loss:0.1656874984503135\n",
      "train loss:0.2354596394795648\n",
      "train loss:0.11199277363789611\n",
      "train loss:0.2011073086252158\n",
      "train loss:0.3455937402480667\n",
      "train loss:0.08319095131107396\n",
      "train loss:0.13699357805734189\n",
      "train loss:0.1258610502945155\n",
      "train loss:0.10520664588715391\n",
      "train loss:0.4201164958734727\n",
      "train loss:0.14806699452175287\n",
      "train loss:0.1761879734291854\n",
      "train loss:0.28970397750102805\n",
      "train loss:0.2640304894260142\n",
      "train loss:0.20882178416560054\n",
      "train loss:0.18457138015174906\n",
      "train loss:0.16527677107396627\n",
      "train loss:0.20052996144518612\n",
      "train loss:0.19549592636587854\n",
      "train loss:0.08278431471851157\n",
      "train loss:0.12637039328073546\n",
      "train loss:0.20680119386054\n",
      "train loss:0.2796469915490894\n",
      "train loss:0.1673070187729243\n",
      "train loss:0.24030007846749235\n",
      "train loss:0.31784609210034376\n",
      "train loss:0.24347191089401438\n",
      "train loss:0.11952198103339988\n",
      "train loss:0.19103437304205273\n",
      "train loss:0.11086421670417985\n",
      "train loss:0.10242862573944793\n",
      "train loss:0.16638167631189962\n",
      "train loss:0.14502502214639543\n",
      "train loss:0.1825709884995774\n",
      "train loss:0.07594619121379251\n",
      "train loss:0.1331515157000515\n",
      "train loss:0.14125595718313919\n",
      "train loss:0.1793945030969788\n",
      "train loss:0.14573634760179316\n",
      "train loss:0.25911137224275244\n",
      "train loss:0.14212097366708598\n",
      "train loss:0.19511057850506325\n",
      "train loss:0.23097032575897114\n",
      "train loss:0.23905690339420527\n",
      "train loss:0.11605005244248727\n",
      "train loss:0.13979676618474476\n",
      "train loss:0.20447001544802543\n",
      "train loss:0.14175246939075278\n",
      "train loss:0.2466427105409622\n",
      "train loss:0.1472560504959376\n",
      "train loss:0.2374780339205317\n",
      "train loss:0.16404634894099762\n",
      "train loss:0.1372911345213184\n",
      "train loss:0.1234959563606855\n",
      "train loss:0.13548317380632435\n",
      "train loss:0.18511302774031713\n",
      "train loss:0.17326213327731196\n",
      "train loss:0.1651101274360298\n",
      "train loss:0.14881672838163168\n",
      "train loss:0.1629816548050251\n",
      "train loss:0.14974465443157484\n",
      "train loss:0.1469392868317416\n",
      "train loss:0.28889732096157233\n",
      "train loss:0.20328817358183401\n",
      "train loss:0.18256515991951353\n",
      "train loss:0.12078404240484923\n",
      "train loss:0.10001473435517019\n",
      "train loss:0.11166291470809575\n",
      "train loss:0.17484202830617362\n",
      "train loss:0.12115872916212299\n",
      "train loss:0.24088930141852813\n",
      "train loss:0.0893798280798101\n",
      "train loss:0.10819609543866246\n",
      "train loss:0.15171860205899487\n",
      "train loss:0.08673772659620547\n",
      "train loss:0.16635405915648935\n",
      "train loss:0.12761470732316366\n",
      "train loss:0.1170795426466991\n",
      "train loss:0.13843697000472432\n",
      "train loss:0.16291935097855417\n",
      "train loss:0.1847300938281079\n",
      "train loss:0.18907456126498837\n",
      "train loss:0.18410897096936604\n",
      "train loss:0.12625099845144575\n",
      "train loss:0.13052396456892176\n",
      "train loss:0.08815696537496484\n",
      "train loss:0.046478532374017625\n",
      "train loss:0.14157154330158164\n",
      "train loss:0.13074638340035324\n",
      "train loss:0.11233625422882602\n",
      "train loss:0.32559749507978547\n",
      "train loss:0.11853898440564103\n",
      "train loss:0.20779821651908467\n",
      "train loss:0.24718775410264257\n",
      "train loss:0.15982849201177823\n",
      "train loss:0.1768462415559987\n",
      "train loss:0.13541651782657177\n",
      "train loss:0.16462472273731066\n",
      "train loss:0.1121789472383356\n",
      "train loss:0.08317622978976923\n",
      "train loss:0.07940316586100633\n",
      "train loss:0.15283488127636471\n",
      "train loss:0.056407232054057295\n",
      "train loss:0.1566324716399244\n",
      "train loss:0.24199788587346405\n",
      "train loss:0.1980851431759059\n",
      "train loss:0.12356994239748975\n",
      "train loss:0.28003323958033255\n",
      "train loss:0.12000080250551555\n",
      "train loss:0.08471905841268523\n",
      "train loss:0.1401337680985476\n",
      "train loss:0.1406849575086453\n",
      "train loss:0.18405089758705098\n",
      "train loss:0.27657037960098935\n",
      "train loss:0.10596707268920287\n",
      "train loss:0.19233065450315273\n",
      "train loss:0.24830146446063753\n",
      "train loss:0.11782272273380982\n",
      "train loss:0.11866827859171603\n",
      "train loss:0.11609920166869817\n",
      "train loss:0.17152487676671163\n",
      "train loss:0.09949862449975644\n",
      "train loss:0.15793224693153365\n",
      "train loss:0.12345175192896429\n",
      "train loss:0.07829913862116848\n",
      "train loss:0.10785607603797988\n",
      "train loss:0.08977827363178585\n",
      "train loss:0.12142393950361571\n",
      "train loss:0.09126728454751458\n",
      "train loss:0.15153220589999747\n",
      "train loss:0.13606843412910444\n",
      "train loss:0.14648601681136392\n",
      "train loss:0.23232464347513507\n",
      "train loss:0.14993544409892404\n",
      "train loss:0.12932809071851412\n",
      "train loss:0.1509460922367216\n",
      "train loss:0.19733491897951963\n",
      "train loss:0.10627084126619886\n",
      "train loss:0.14196332765492511\n",
      "train loss:0.2325388192317815\n",
      "train loss:0.07021728186103741\n",
      "train loss:0.16317470914765875\n",
      "train loss:0.10986289669226496\n",
      "train loss:0.10948885577123059\n",
      "train loss:0.16874125306312218\n",
      "train loss:0.17567449379689962\n",
      "train loss:0.12708399152078298\n",
      "train loss:0.07776214983703053\n",
      "train loss:0.15953125719704933\n",
      "train loss:0.1534522099457602\n",
      "train loss:0.18905910247332247\n",
      "train loss:0.09454158437426768\n",
      "train loss:0.1098860104511716\n",
      "train loss:0.1315324370417984\n",
      "train loss:0.15725260838778587\n",
      "train loss:0.18665480542131768\n",
      "train loss:0.13341505138350956\n",
      "train loss:0.07479053808081873\n",
      "train loss:0.07769368477902872\n",
      "train loss:0.17449533398012318\n",
      "train loss:0.25038254538162014\n",
      "train loss:0.12131560584403435\n",
      "train loss:0.05454808986310873\n",
      "train loss:0.13643887237063487\n",
      "train loss:0.09032920372837829\n",
      "train loss:0.05721825843936336\n",
      "train loss:0.135010368740166\n",
      "train loss:0.31593140450242035\n",
      "train loss:0.039693078141642454\n",
      "train loss:0.2853794753051938\n",
      "train loss:0.17459291992617262\n",
      "train loss:0.05781627112740691\n",
      "train loss:0.10030632745180608\n",
      "train loss:0.10353300698093236\n",
      "train loss:0.09645503138044204\n",
      "train loss:0.1732128469560192\n",
      "train loss:0.13884529825612218\n",
      "train loss:0.1998629825984274\n",
      "train loss:0.12659119095024582\n",
      "train loss:0.07903729792585411\n",
      "train loss:0.09069675674427437\n",
      "train loss:0.10466868046493343\n",
      "train loss:0.11816975694403753\n",
      "train loss:0.19038024412313967\n",
      "train loss:0.12289711982342527\n",
      "train loss:0.10613493139501086\n",
      "train loss:0.1867743015336593\n",
      "train loss:0.10738263672228193\n",
      "train loss:0.12038114105104655\n",
      "train loss:0.12652347967144886\n",
      "train loss:0.15828352434111057\n",
      "train loss:0.10646343784047886\n",
      "train loss:0.1373871669042984\n",
      "train loss:0.09385160427708693\n",
      "train loss:0.12439543295139903\n",
      "train loss:0.1029565159855022\n",
      "train loss:0.11125354020448879\n",
      "train loss:0.16349225473463\n",
      "train loss:0.07632783185342121\n",
      "train loss:0.061570951370587323\n",
      "train loss:0.11170782001548751\n",
      "train loss:0.15684892818320278\n",
      "train loss:0.18338162679711303\n",
      "train loss:0.10029404540905971\n",
      "train loss:0.20874093018434986\n",
      "train loss:0.1177047126591369\n",
      "train loss:0.12207917088381202\n",
      "train loss:0.18174007226093689\n",
      "train loss:0.11282580476074607\n",
      "train loss:0.16397225755643532\n",
      "train loss:0.09953278710556113\n",
      "train loss:0.22229038077499305\n",
      "train loss:0.12055337147574638\n",
      "train loss:0.10887473814020074\n",
      "train loss:0.08460493407301037\n",
      "train loss:0.09616228380208272\n",
      "train loss:0.14630644460339307\n",
      "train loss:0.17104846450252445\n",
      "train loss:0.09089640691944634\n",
      "train loss:0.15411252517145604\n",
      "train loss:0.0499085601293354\n",
      "train loss:0.11095475002965655\n",
      "train loss:0.1572844367257257\n",
      "train loss:0.09465728274513169\n",
      "train loss:0.14390087040742977\n",
      "train loss:0.04540741838141806\n",
      "train loss:0.06324360000095121\n",
      "train loss:0.10817786071465849\n",
      "train loss:0.2663972489628225\n",
      "train loss:0.09400794467183998\n",
      "train loss:0.12251853751694526\n",
      "train loss:0.11198233231912182\n",
      "train loss:0.16867889741278463\n",
      "train loss:0.13363734730195878\n",
      "train loss:0.17580652490896875\n",
      "train loss:0.11783541333274955\n",
      "train loss:0.15589047706961712\n",
      "train loss:0.20492383838134823\n",
      "train loss:0.08406053403775143\n",
      "train loss:0.1678334874510475\n",
      "train loss:0.09920502974930509\n",
      "train loss:0.13484513320372538\n",
      "train loss:0.2832577669937279\n",
      "train loss:0.1505606396148549\n",
      "train loss:0.08249224847111883\n",
      "train loss:0.1181307905305908\n",
      "train loss:0.1009918536085797\n",
      "train loss:0.12398652402857496\n",
      "train loss:0.1924428095952961\n",
      "train loss:0.08829548648639969\n",
      "train loss:0.06326488501920806\n",
      "train loss:0.1475335829925665\n",
      "train loss:0.10856797616561162\n",
      "train loss:0.10275395525873324\n",
      "train loss:0.05034733581379755\n",
      "train loss:0.13790066012272123\n",
      "train loss:0.06861309413739124\n",
      "train loss:0.07317121346304678\n",
      "train loss:0.26296599059740716\n",
      "train loss:0.15806788103863303\n",
      "train loss:0.09667299194074142\n",
      "train loss:0.07714142081467636\n",
      "train loss:0.29895256074600723\n",
      "train loss:0.10379428619716456\n",
      "train loss:0.1941204840998596\n",
      "train loss:0.06622534287785818\n",
      "train loss:0.1040538716222879\n",
      "train loss:0.12885055310561486\n",
      "train loss:0.10482194086799765\n",
      "train loss:0.07346657459649317\n",
      "train loss:0.1064240971845809\n",
      "train loss:0.11155913829631482\n",
      "train loss:0.11089100480126447\n",
      "train loss:0.16405037306820727\n",
      "train loss:0.04193652977444093\n",
      "train loss:0.11993427657320492\n",
      "=== epoch:2, train acc:0.972, test acc:0.969 ===\n",
      "train loss:0.04713074766934013\n",
      "train loss:0.08640404331709005\n",
      "train loss:0.08328364925352132\n",
      "train loss:0.059144236769106955\n",
      "train loss:0.07493276805913548\n",
      "train loss:0.09839410779564281\n",
      "train loss:0.10154186906767691\n",
      "train loss:0.06024227393428861\n",
      "train loss:0.11356184995619226\n",
      "train loss:0.12399671904445553\n",
      "train loss:0.10643179714636285\n",
      "train loss:0.04574406067707467\n",
      "train loss:0.11031283803529082\n",
      "train loss:0.08838389785391923\n",
      "train loss:0.08933265223108502\n",
      "train loss:0.11673214486380966\n",
      "train loss:0.17108546776178776\n",
      "train loss:0.12363653834558205\n",
      "train loss:0.02927201190603174\n",
      "train loss:0.10547008167422786\n",
      "train loss:0.12411227001087653\n",
      "train loss:0.13598313587368774\n",
      "train loss:0.14738408058105196\n",
      "train loss:0.08507523197408137\n",
      "train loss:0.13326749897934134\n",
      "train loss:0.10867273967389952\n",
      "train loss:0.21454481654665825\n",
      "train loss:0.08376658239771449\n",
      "train loss:0.17358354514406357\n",
      "train loss:0.09504891861336741\n",
      "train loss:0.2023004885854776\n",
      "train loss:0.14814805865389874\n",
      "train loss:0.15965846692329927\n",
      "train loss:0.0750526460497294\n",
      "train loss:0.07238916355344163\n",
      "train loss:0.17931687713644778\n",
      "train loss:0.10738901096669125\n",
      "train loss:0.12756636777278876\n",
      "train loss:0.11146664565472184\n",
      "train loss:0.12516574937084068\n",
      "train loss:0.1336264672794753\n",
      "train loss:0.10648127448786178\n",
      "train loss:0.2049020149647814\n",
      "train loss:0.14782863234244847\n",
      "train loss:0.061177283360552455\n",
      "train loss:0.08885959295873905\n",
      "train loss:0.16127458533486666\n",
      "train loss:0.10308244740438796\n",
      "train loss:0.1160308659203575\n",
      "train loss:0.10676656430372237\n",
      "train loss:0.13483017004211406\n",
      "train loss:0.0648124449159539\n",
      "train loss:0.06520016144515124\n",
      "train loss:0.09983234925379827\n",
      "train loss:0.0743497943643899\n",
      "train loss:0.06161079102152141\n",
      "train loss:0.04866396934698645\n",
      "train loss:0.12904469541700872\n",
      "train loss:0.11334377309700519\n",
      "train loss:0.09258737432689988\n",
      "train loss:0.0921296202679999\n",
      "train loss:0.1420125997613082\n",
      "train loss:0.04766653289571992\n",
      "train loss:0.0989470381702032\n",
      "train loss:0.06912726691189414\n",
      "train loss:0.14497725057919783\n",
      "train loss:0.0799027891637416\n",
      "train loss:0.049676284503122885\n",
      "train loss:0.12040267166800737\n",
      "train loss:0.08117732145859881\n",
      "train loss:0.1445185881293815\n",
      "train loss:0.1444995661422093\n",
      "train loss:0.08007337328788276\n",
      "train loss:0.10276049547757926\n",
      "train loss:0.09248991597663778\n",
      "train loss:0.09903292611741076\n",
      "train loss:0.07959263257278175\n",
      "train loss:0.06563272713976633\n",
      "train loss:0.08973104152096231\n",
      "train loss:0.11286604783796929\n",
      "train loss:0.10218408796344926\n",
      "train loss:0.06770029430074243\n",
      "train loss:0.0449130061217161\n",
      "train loss:0.0630191753765041\n",
      "train loss:0.07708278740940078\n",
      "train loss:0.10172235948712048\n",
      "train loss:0.06230776477497712\n",
      "train loss:0.056892659323779265\n",
      "train loss:0.07906665118827248\n",
      "train loss:0.06887623260773747\n",
      "train loss:0.11242688610902882\n",
      "train loss:0.1296493406428114\n",
      "train loss:0.12577996911887993\n",
      "train loss:0.040705993073127285\n",
      "train loss:0.05353239711235755\n",
      "train loss:0.11278100579807132\n",
      "train loss:0.17764394089138125\n",
      "train loss:0.16922891901228485\n",
      "train loss:0.09122142081361236\n",
      "train loss:0.09014036597190779\n",
      "train loss:0.0915409049752718\n",
      "train loss:0.08045525433870551\n",
      "train loss:0.11204358764556924\n",
      "train loss:0.09849136854148516\n",
      "train loss:0.03782035894743684\n",
      "train loss:0.09569051343944297\n",
      "train loss:0.12876887379919605\n",
      "train loss:0.09715545206730372\n",
      "train loss:0.053059422327662684\n",
      "train loss:0.18834428639721387\n",
      "train loss:0.16097685095108283\n",
      "train loss:0.1054477806955514\n",
      "train loss:0.05503521140216302\n",
      "train loss:0.05671387721395876\n",
      "train loss:0.09011250105991349\n",
      "train loss:0.054229011616220416\n",
      "train loss:0.043839088234745006\n",
      "train loss:0.1446955152747075\n",
      "train loss:0.09969276283295643\n",
      "train loss:0.08522572700401254\n",
      "train loss:0.08088281084293397\n",
      "train loss:0.023665755201417817\n",
      "train loss:0.034350815377151706\n",
      "train loss:0.07733006475294414\n",
      "train loss:0.0822179468685561\n",
      "train loss:0.08035076450981495\n",
      "train loss:0.08475867930735768\n",
      "train loss:0.07146230192278794\n",
      "train loss:0.08659498046258354\n",
      "train loss:0.13007212307743238\n",
      "train loss:0.06800895120791302\n",
      "train loss:0.07132096490020733\n",
      "train loss:0.0451365873109585\n",
      "train loss:0.05636148221723356\n",
      "train loss:0.11603502409794754\n",
      "train loss:0.06002394160653928\n",
      "train loss:0.059789940698999414\n",
      "train loss:0.0700644913406886\n",
      "train loss:0.11382977451434768\n",
      "train loss:0.08850591438288129\n",
      "train loss:0.17512804062604265\n",
      "train loss:0.16990947057868858\n",
      "train loss:0.07650614611595015\n",
      "train loss:0.23668368301419146\n",
      "train loss:0.12193176007645633\n",
      "train loss:0.041042496029454645\n",
      "train loss:0.10114396025574912\n",
      "train loss:0.11218526422832512\n",
      "train loss:0.07226430096756997\n",
      "train loss:0.10047854862048951\n",
      "train loss:0.07978983950846986\n",
      "train loss:0.16416334494806123\n",
      "train loss:0.06132461253723434\n",
      "train loss:0.07521765640756375\n",
      "train loss:0.18285349324703057\n",
      "train loss:0.0662414240313265\n",
      "train loss:0.09793304647978919\n",
      "train loss:0.086125886598377\n",
      "train loss:0.09411429622163961\n",
      "train loss:0.0766431673259713\n",
      "train loss:0.11810772743948596\n",
      "train loss:0.040220603061439764\n",
      "train loss:0.11320900486185045\n",
      "train loss:0.10557561661444018\n",
      "train loss:0.21543043909265286\n",
      "train loss:0.1676111070992927\n",
      "train loss:0.06337884441747489\n",
      "train loss:0.062427740989019494\n",
      "train loss:0.13558199071817\n",
      "train loss:0.06666371699513114\n",
      "train loss:0.29816770518090335\n",
      "train loss:0.08057316541964644\n",
      "train loss:0.05021493654832515\n",
      "train loss:0.07202171830269295\n",
      "train loss:0.028149014305706755\n",
      "train loss:0.05780146016475744\n",
      "train loss:0.03162623746800396\n",
      "train loss:0.032611975628453076\n",
      "train loss:0.16432345171975804\n",
      "train loss:0.11516511166182507\n",
      "train loss:0.04692701078292712\n",
      "train loss:0.10605700612862888\n",
      "train loss:0.09812162857769917\n",
      "train loss:0.0629699468940132\n",
      "train loss:0.061842864434294\n",
      "train loss:0.07222511584140996\n",
      "train loss:0.053091984853940996\n",
      "train loss:0.05588270131530555\n",
      "train loss:0.05165017635740111\n",
      "train loss:0.10927918226168049\n",
      "train loss:0.05367078707415627\n",
      "train loss:0.04909416925652636\n",
      "train loss:0.05478632082451762\n",
      "train loss:0.05763836917147575\n",
      "train loss:0.0898374397121735\n",
      "train loss:0.10672440551440238\n",
      "train loss:0.09453118629103707\n",
      "train loss:0.08883550997627256\n",
      "train loss:0.08200039135125445\n",
      "train loss:0.06390888078085237\n",
      "train loss:0.24759367241936464\n",
      "train loss:0.020492451253094967\n",
      "train loss:0.06838778215685892\n",
      "train loss:0.09053633894300755\n",
      "train loss:0.06108015438374864\n",
      "train loss:0.054742501747284934\n",
      "train loss:0.10302201413801834\n",
      "train loss:0.11116062189297915\n",
      "train loss:0.049932091252834744\n",
      "train loss:0.04695606014558415\n",
      "train loss:0.05022177746446015\n",
      "train loss:0.04965604054078796\n",
      "train loss:0.12267728883413899\n",
      "train loss:0.057471100409310534\n",
      "train loss:0.1205456888633749\n",
      "train loss:0.07809364053401271\n",
      "train loss:0.0267220116645957\n",
      "train loss:0.07731710976597798\n",
      "train loss:0.11657888116464958\n",
      "train loss:0.0903862845784289\n",
      "train loss:0.09719809763465456\n",
      "train loss:0.06881792063546449\n",
      "train loss:0.07431219452056627\n",
      "train loss:0.04723377800429117\n",
      "train loss:0.09763384909054983\n",
      "train loss:0.03791594359989158\n",
      "train loss:0.045365769456279785\n",
      "train loss:0.18083471031011666\n",
      "train loss:0.11869740015977527\n",
      "train loss:0.0884706270009372\n",
      "train loss:0.07445605125772069\n",
      "train loss:0.05904450551744496\n",
      "train loss:0.0629320739264259\n",
      "train loss:0.11462928396522783\n",
      "train loss:0.04550543972901936\n",
      "train loss:0.06534835730022637\n",
      "train loss:0.12063308671352697\n",
      "train loss:0.11070449840182323\n",
      "train loss:0.06198269910698312\n",
      "train loss:0.043457725202123966\n",
      "train loss:0.032817163793105356\n",
      "train loss:0.07517748010040966\n",
      "train loss:0.021368040755505545\n",
      "train loss:0.09954201272914355\n",
      "train loss:0.06035267065121696\n",
      "train loss:0.04328910380805736\n",
      "train loss:0.027161419156707434\n",
      "train loss:0.05217858976744221\n",
      "train loss:0.07077232712166315\n",
      "train loss:0.11199935665013455\n",
      "train loss:0.05705849854293282\n",
      "train loss:0.09216904942488975\n",
      "train loss:0.04639317318094936\n",
      "train loss:0.03607299909622871\n",
      "train loss:0.06001129728887413\n",
      "train loss:0.13506663668790264\n",
      "train loss:0.0812667047084485\n",
      "train loss:0.060918680402327084\n",
      "train loss:0.02598828836294215\n",
      "train loss:0.10643461850309503\n",
      "train loss:0.11197778438851\n",
      "train loss:0.16423040364826463\n",
      "train loss:0.15836191929152155\n",
      "train loss:0.050729817607071626\n",
      "train loss:0.08469422504344593\n",
      "train loss:0.044485504767598084\n",
      "train loss:0.03498074745094016\n",
      "train loss:0.11715441166407504\n",
      "train loss:0.23073785255465817\n",
      "train loss:0.1266060477202119\n",
      "train loss:0.03803575523690025\n",
      "train loss:0.06810522298292082\n",
      "train loss:0.023106936033832643\n",
      "train loss:0.09360784352620076\n",
      "train loss:0.13981596172721442\n",
      "train loss:0.1467724134924519\n",
      "train loss:0.10044793978427004\n",
      "train loss:0.04897267861860136\n",
      "train loss:0.08758362644908133\n",
      "train loss:0.036612189611399656\n",
      "train loss:0.047486262585327725\n",
      "train loss:0.08037677697797259\n",
      "train loss:0.14007421436059894\n",
      "train loss:0.13151848028813332\n",
      "train loss:0.08867460459220426\n",
      "train loss:0.09592908369216885\n",
      "train loss:0.05909259242461361\n",
      "train loss:0.10279825291596147\n",
      "train loss:0.08831765437845121\n",
      "train loss:0.07508075278538869\n",
      "train loss:0.07496025460739472\n",
      "train loss:0.11461227873005443\n",
      "train loss:0.0444404249104752\n",
      "train loss:0.0837794125655821\n",
      "train loss:0.10077918056554322\n",
      "train loss:0.05726249106212458\n",
      "train loss:0.04153275631306671\n",
      "train loss:0.13778821386823503\n",
      "train loss:0.05918185120033404\n",
      "train loss:0.054849587481025586\n",
      "train loss:0.1216474291167352\n",
      "train loss:0.07154265459448975\n",
      "train loss:0.05270258707831108\n",
      "train loss:0.018855768898565925\n",
      "train loss:0.11499457995577496\n",
      "train loss:0.022251808529228253\n",
      "train loss:0.11788484402810323\n",
      "train loss:0.07767907148380825\n",
      "train loss:0.056904486138348795\n",
      "train loss:0.042580006380474086\n",
      "train loss:0.07298425617880185\n",
      "train loss:0.06387761588822316\n",
      "train loss:0.11109363314062215\n",
      "train loss:0.08124631248442285\n",
      "train loss:0.0894082199754798\n",
      "train loss:0.05282784185038762\n",
      "train loss:0.03453668329804364\n",
      "train loss:0.05773177829679574\n",
      "train loss:0.10918932572519967\n",
      "train loss:0.021257263492974604\n",
      "train loss:0.03464242936778323\n",
      "train loss:0.05301435188600151\n",
      "train loss:0.03560685129182197\n",
      "train loss:0.08736078901728866\n",
      "train loss:0.05438994427603516\n",
      "train loss:0.11923645410854007\n",
      "train loss:0.020578796387409184\n",
      "train loss:0.05430331808170595\n",
      "train loss:0.0508584991423363\n",
      "train loss:0.11404605909143858\n",
      "train loss:0.14787955961877017\n",
      "train loss:0.13008673455414566\n",
      "train loss:0.049353000907536575\n",
      "train loss:0.04220891147120495\n",
      "train loss:0.06025662105503352\n",
      "train loss:0.049536171593880865\n",
      "train loss:0.051346314926247257\n",
      "train loss:0.103587192384035\n",
      "train loss:0.031004406316389347\n",
      "train loss:0.06030604227765765\n",
      "train loss:0.07378693345479442\n",
      "train loss:0.020986857499361588\n",
      "train loss:0.14862048890054297\n",
      "train loss:0.04174141440304406\n",
      "train loss:0.11497632017044487\n",
      "train loss:0.05173044849037182\n",
      "train loss:0.06535643124707816\n",
      "train loss:0.1156926362538476\n",
      "train loss:0.03839233866282355\n",
      "train loss:0.06471328342885156\n",
      "train loss:0.1275157346763329\n",
      "train loss:0.08434359540506847\n",
      "train loss:0.13121527319774404\n",
      "train loss:0.06440191381139232\n",
      "train loss:0.12244037243661424\n",
      "train loss:0.05056149456497673\n",
      "train loss:0.09831311470939143\n",
      "train loss:0.06442745238966721\n",
      "train loss:0.0619525873839518\n",
      "train loss:0.0546841567660446\n",
      "train loss:0.08867244677894817\n",
      "train loss:0.11676074591893043\n",
      "train loss:0.09246417667023793\n",
      "train loss:0.14010207680681463\n",
      "train loss:0.08328443820230501\n",
      "train loss:0.1655592772838369\n",
      "train loss:0.03442145885129383\n",
      "train loss:0.05494502063244501\n",
      "train loss:0.03861100805380011\n",
      "train loss:0.052464043345742555\n",
      "train loss:0.051733983504128105\n",
      "train loss:0.07323786626709264\n",
      "train loss:0.06142251594347244\n",
      "train loss:0.051971552829777126\n",
      "train loss:0.05957647040948757\n",
      "train loss:0.021815675523561843\n",
      "train loss:0.04957899217544235\n",
      "train loss:0.04333699490712346\n",
      "train loss:0.07807815374214952\n",
      "train loss:0.07859084026563454\n",
      "train loss:0.08822972407805298\n",
      "train loss:0.06217608311450985\n",
      "train loss:0.10422223399150365\n",
      "train loss:0.034212768472090094\n",
      "train loss:0.06750747356712349\n",
      "train loss:0.0824301401674791\n",
      "train loss:0.11595907695292633\n",
      "train loss:0.05629850854782795\n",
      "train loss:0.03455668340387813\n",
      "train loss:0.05140169878920573\n",
      "train loss:0.049280695691885514\n",
      "train loss:0.06642646101208975\n",
      "train loss:0.09300496286263385\n",
      "train loss:0.027757069958123468\n",
      "train loss:0.1502019889916242\n",
      "train loss:0.05477796104232018\n",
      "train loss:0.05356084566218488\n",
      "train loss:0.12554800364572039\n",
      "train loss:0.14286613380372803\n",
      "train loss:0.035840809797396345\n",
      "train loss:0.07562047666198507\n",
      "train loss:0.1447951393934081\n",
      "train loss:0.1094125811958177\n",
      "train loss:0.03793433693149093\n",
      "train loss:0.06571061065763605\n",
      "train loss:0.06334396681425565\n",
      "train loss:0.06327054305430661\n",
      "train loss:0.07194583260195045\n",
      "train loss:0.02384853995749194\n",
      "train loss:0.06264087930424\n",
      "train loss:0.13772016764342906\n",
      "train loss:0.06886989838675184\n",
      "train loss:0.0925720256712198\n",
      "train loss:0.09381904487045264\n",
      "train loss:0.022543357255065115\n",
      "train loss:0.05854726436946832\n",
      "train loss:0.08976132308383603\n",
      "train loss:0.031107669743974168\n",
      "train loss:0.04923041905775416\n",
      "train loss:0.07467261506012708\n",
      "train loss:0.08094686470540154\n",
      "train loss:0.0851543875322233\n",
      "train loss:0.14618423117178897\n",
      "train loss:0.28564367546046304\n",
      "train loss:0.03159138944183144\n",
      "train loss:0.050118676129209234\n",
      "train loss:0.02068661807842767\n",
      "train loss:0.07334679160871374\n",
      "train loss:0.05775693924913136\n",
      "train loss:0.08252537440932958\n",
      "train loss:0.0790012376887272\n",
      "train loss:0.029680091421134053\n",
      "train loss:0.04987663617536033\n",
      "train loss:0.16623069672472243\n",
      "train loss:0.10715798689164399\n",
      "train loss:0.025933876581830252\n",
      "train loss:0.030195313912826482\n",
      "train loss:0.05193914404793294\n",
      "train loss:0.10284665077109516\n",
      "train loss:0.09318622311688834\n",
      "train loss:0.11273734953417652\n",
      "train loss:0.07814490971532971\n",
      "train loss:0.08369941954187887\n",
      "train loss:0.026620392522159187\n",
      "train loss:0.04293923180881329\n",
      "train loss:0.08320325777875993\n",
      "train loss:0.10031828907414907\n",
      "train loss:0.048549062668607375\n",
      "train loss:0.039156621915823725\n",
      "train loss:0.06077676273399109\n",
      "train loss:0.039391385679312114\n",
      "train loss:0.059853805388961415\n",
      "train loss:0.12578314808948438\n",
      "train loss:0.07170598837686658\n",
      "train loss:0.13573504569691594\n",
      "train loss:0.08567838108585256\n",
      "train loss:0.038373601551773474\n",
      "train loss:0.09611267736896516\n",
      "train loss:0.043266411356291334\n",
      "train loss:0.05728340113523653\n",
      "train loss:0.0453249453671402\n",
      "train loss:0.0720997288043241\n",
      "train loss:0.1476042125215873\n",
      "train loss:0.019788811164529893\n",
      "train loss:0.023851895379898534\n",
      "train loss:0.07812813282805085\n",
      "train loss:0.031917260570128655\n",
      "train loss:0.03162197665723759\n",
      "train loss:0.053329354255327734\n",
      "train loss:0.10183428734604563\n",
      "train loss:0.05796341250834078\n",
      "train loss:0.06357408320085624\n",
      "train loss:0.03768002346892656\n",
      "train loss:0.0468796209350144\n",
      "train loss:0.0491822036961443\n",
      "train loss:0.12667217721889235\n",
      "train loss:0.08547372301013327\n",
      "train loss:0.20557174744661985\n",
      "train loss:0.04118486050747833\n",
      "train loss:0.025760561155850695\n",
      "train loss:0.03267612005743074\n",
      "train loss:0.06982494983556985\n",
      "train loss:0.17500526550359857\n",
      "train loss:0.1451559479006104\n",
      "train loss:0.0890028570825635\n",
      "train loss:0.030173672249435398\n",
      "train loss:0.05722182207812001\n",
      "train loss:0.022447697045983216\n",
      "train loss:0.11842764795540289\n",
      "train loss:0.08544639342269594\n",
      "train loss:0.09326566304811504\n",
      "train loss:0.038219382771572816\n",
      "train loss:0.041862513535612184\n",
      "train loss:0.05428118117427542\n",
      "train loss:0.026496603983620557\n",
      "train loss:0.08091760299379641\n",
      "train loss:0.06420656793023634\n",
      "train loss:0.0853655925713488\n",
      "train loss:0.03569143692739838\n",
      "train loss:0.08787534530719637\n",
      "train loss:0.05116159947504642\n",
      "train loss:0.07417693169075236\n",
      "train loss:0.131115333642589\n",
      "train loss:0.04033522362229151\n",
      "train loss:0.04724759881893217\n",
      "train loss:0.06914921904383216\n",
      "train loss:0.1406401374240393\n",
      "train loss:0.03675246115990979\n",
      "train loss:0.03206305358178441\n",
      "train loss:0.03928311711768233\n",
      "train loss:0.08216889945993687\n",
      "train loss:0.04131992540804249\n",
      "train loss:0.051354738908751266\n",
      "train loss:0.050329658737795914\n",
      "train loss:0.021669847677558095\n",
      "train loss:0.05450528068825921\n",
      "train loss:0.022976656908651746\n",
      "train loss:0.1034057547336528\n",
      "train loss:0.07005510670107638\n",
      "train loss:0.19818046978438317\n",
      "train loss:0.01591977492852498\n",
      "train loss:0.09936787266670466\n",
      "train loss:0.05458379412231959\n",
      "train loss:0.014992789974569409\n",
      "train loss:0.0322233648057448\n",
      "train loss:0.030336422095866743\n",
      "train loss:0.02146515371701084\n",
      "train loss:0.03820084685930835\n",
      "train loss:0.06298600660748015\n",
      "train loss:0.07887727512647631\n",
      "train loss:0.08112734229269396\n",
      "train loss:0.05226780386716192\n",
      "train loss:0.08902286771457323\n",
      "train loss:0.05054180278229659\n",
      "train loss:0.1240610964095073\n",
      "train loss:0.0784545185059998\n",
      "train loss:0.046921661394698055\n",
      "train loss:0.08832250296603153\n",
      "train loss:0.09965125164920074\n",
      "train loss:0.020074074549954298\n",
      "train loss:0.04190556851926675\n",
      "train loss:0.06497493698475945\n",
      "train loss:0.05804977186202675\n",
      "train loss:0.030145130048746364\n",
      "train loss:0.06776877615464225\n",
      "train loss:0.05299263868452794\n",
      "train loss:0.10624187160177737\n",
      "train loss:0.03739858139495354\n",
      "train loss:0.07254656121722586\n",
      "train loss:0.07776586028227027\n",
      "train loss:0.10359939335357259\n",
      "train loss:0.13332106258328355\n",
      "train loss:0.09927304472027376\n",
      "train loss:0.08409119424320027\n",
      "train loss:0.07895105304913473\n",
      "train loss:0.03225518333760953\n",
      "train loss:0.04995414999809115\n",
      "train loss:0.1593266971689874\n",
      "train loss:0.10267919065832563\n",
      "train loss:0.13686015953964245\n",
      "train loss:0.019705666960847103\n",
      "train loss:0.054752528597924705\n",
      "train loss:0.11461157277617497\n",
      "train loss:0.021515681833498515\n",
      "train loss:0.12380447025522909\n",
      "train loss:0.16490529225032424\n",
      "train loss:0.03093476316515582\n",
      "train loss:0.02687564311409064\n",
      "train loss:0.032557009083909565\n",
      "train loss:0.03189370695655316\n",
      "train loss:0.029529829400266133\n",
      "train loss:0.12360208193326441\n",
      "train loss:0.06563590973370517\n",
      "train loss:0.02512959681285282\n",
      "train loss:0.04527788959047999\n",
      "train loss:0.21082559324147088\n",
      "train loss:0.15753562749077277\n",
      "train loss:0.09269309291809208\n",
      "train loss:0.027493413707397264\n",
      "train loss:0.02332264560487028\n",
      "train loss:0.07988992540076474\n",
      "train loss:0.04726987597213744\n",
      "train loss:0.04153680795799303\n",
      "train loss:0.04101242013022375\n",
      "train loss:0.07061463385041984\n",
      "train loss:0.04626704299475762\n",
      "train loss:0.04344736625786497\n",
      "train loss:0.06881883537020303\n",
      "train loss:0.04488030038123598\n",
      "train loss:0.07431216971502973\n",
      "train loss:0.025590253518669646\n",
      "train loss:0.03344145298029922\n",
      "train loss:0.04256486423345173\n",
      "train loss:0.09169414932560356\n",
      "train loss:0.04823334047688958\n",
      "train loss:0.07812755336130262\n",
      "train loss:0.06674168748462331\n",
      "train loss:0.032365129437279\n",
      "train loss:0.030337301322713343\n",
      "train loss:0.03705895233172082\n",
      "=== epoch:3, train acc:0.977, test acc:0.974 ===\n",
      "train loss:0.04385838322850064\n",
      "train loss:0.04188821244356827\n",
      "train loss:0.08088423553688315\n",
      "train loss:0.05480421101146197\n",
      "train loss:0.04746921140439334\n",
      "train loss:0.04585551720743688\n",
      "train loss:0.09534439291712948\n",
      "train loss:0.07994396995050204\n",
      "train loss:0.044137135108758836\n",
      "train loss:0.21095548891663773\n",
      "train loss:0.08380972845023564\n",
      "train loss:0.07717004642839022\n",
      "train loss:0.05314049313518356\n",
      "train loss:0.05985329006909648\n",
      "train loss:0.042083625102228704\n",
      "train loss:0.038243917875791185\n",
      "train loss:0.03818325645026863\n",
      "train loss:0.07689706265886179\n",
      "train loss:0.14225196946500238\n",
      "train loss:0.08139546092512626\n",
      "train loss:0.05036400422876128\n",
      "train loss:0.033374669937967824\n",
      "train loss:0.16649431332517164\n",
      "train loss:0.03521362996232633\n",
      "train loss:0.05920205289952186\n",
      "train loss:0.07143673412982904\n",
      "train loss:0.08344331363537648\n",
      "train loss:0.05330475080746467\n",
      "train loss:0.12029413132100394\n",
      "train loss:0.02982242639823944\n",
      "train loss:0.16710880455184499\n",
      "train loss:0.040096981558091\n",
      "train loss:0.06386824638494296\n",
      "train loss:0.08663082382173609\n",
      "train loss:0.04192232870374926\n",
      "train loss:0.0846064237464727\n",
      "train loss:0.06856592178165428\n",
      "train loss:0.03967508344187688\n",
      "train loss:0.028808048582465284\n",
      "train loss:0.02947713575946339\n",
      "train loss:0.015140534735569578\n",
      "train loss:0.09735790119812608\n",
      "train loss:0.031548859489371715\n",
      "train loss:0.04192508589686665\n",
      "train loss:0.06462789104923371\n",
      "train loss:0.04980046542400474\n",
      "train loss:0.03515851868669731\n",
      "train loss:0.059081217570690035\n",
      "train loss:0.028103875093095785\n",
      "train loss:0.08764211290758013\n",
      "train loss:0.035667946516968585\n",
      "train loss:0.06195211795553007\n",
      "train loss:0.062458911863543846\n",
      "train loss:0.026578475784689623\n",
      "train loss:0.0127502690768472\n",
      "train loss:0.030975566410678775\n",
      "train loss:0.07680529626559154\n",
      "train loss:0.04540887181398156\n",
      "train loss:0.04998248871106458\n",
      "train loss:0.06803499140135184\n",
      "train loss:0.019697809679249702\n",
      "train loss:0.037843528378774825\n",
      "train loss:0.10508598139608745\n",
      "train loss:0.05265677864884103\n",
      "train loss:0.06879826703314192\n",
      "train loss:0.02956293323049562\n",
      "train loss:0.04414654889690251\n",
      "train loss:0.04839985738903841\n",
      "train loss:0.1268595138359522\n",
      "train loss:0.024060698308667326\n",
      "train loss:0.027944290688556524\n",
      "train loss:0.05550445725464777\n",
      "train loss:0.0703053776489953\n",
      "train loss:0.02903862123319333\n",
      "train loss:0.04970335803357961\n",
      "train loss:0.05085168313390042\n",
      "train loss:0.1042664382304798\n",
      "train loss:0.08963597120749352\n",
      "train loss:0.01783146201482726\n",
      "train loss:0.05611360946786387\n",
      "train loss:0.07499044012112693\n",
      "train loss:0.04489331903080115\n",
      "train loss:0.06991021397479986\n",
      "train loss:0.028035757080131583\n",
      "train loss:0.13179722687574374\n",
      "train loss:0.10471479584786988\n",
      "train loss:0.099858850747975\n",
      "train loss:0.022173735053367644\n",
      "train loss:0.132411991843053\n",
      "train loss:0.051866346882915096\n",
      "train loss:0.028128635842153034\n",
      "train loss:0.03603814247494331\n",
      "train loss:0.06861984610158353\n",
      "train loss:0.0505755344668602\n",
      "train loss:0.010071002330202503\n",
      "train loss:0.10633340527684709\n",
      "train loss:0.033105031184654944\n",
      "train loss:0.10375361050259568\n",
      "train loss:0.03751749587829914\n",
      "train loss:0.050248752763404514\n",
      "train loss:0.05006524875778975\n",
      "train loss:0.04789762601332231\n",
      "train loss:0.050761455967208845\n",
      "train loss:0.03965216589938539\n",
      "train loss:0.03267396382523427\n",
      "train loss:0.055289120903342835\n",
      "train loss:0.09751816132951456\n",
      "train loss:0.03888087097149842\n",
      "train loss:0.08068794168201661\n",
      "train loss:0.13613113143621908\n",
      "train loss:0.05422660376985476\n",
      "train loss:0.14574533652038205\n",
      "train loss:0.028511844809298713\n",
      "train loss:0.06151325552163138\n",
      "train loss:0.05094722359337548\n",
      "train loss:0.03432543037115991\n",
      "train loss:0.08116470235871058\n",
      "train loss:0.03302733639834208\n",
      "train loss:0.09171417382011737\n",
      "train loss:0.08559524074412547\n",
      "train loss:0.021436456566583037\n",
      "train loss:0.17294063041639074\n",
      "train loss:0.020059020012068808\n",
      "train loss:0.021786347877718618\n",
      "train loss:0.027421501807020177\n",
      "train loss:0.07819896684987136\n",
      "train loss:0.08026252798038513\n",
      "train loss:0.12183084190114514\n",
      "train loss:0.0436399850415367\n",
      "train loss:0.022523985627398\n",
      "train loss:0.052154925169225194\n",
      "train loss:0.05648385719469011\n",
      "train loss:0.0378618948719284\n",
      "train loss:0.047101863810243484\n",
      "train loss:0.04856135408356089\n",
      "train loss:0.11840145966309429\n",
      "train loss:0.04990740321504516\n",
      "train loss:0.06167828895219229\n",
      "train loss:0.1158678492663961\n",
      "train loss:0.04196395113296859\n",
      "train loss:0.0463527299159403\n",
      "train loss:0.09620545814546197\n",
      "train loss:0.06608010078711901\n",
      "train loss:0.09023200213682582\n",
      "train loss:0.058710296377227945\n",
      "train loss:0.034980621829438056\n",
      "train loss:0.04272760652577908\n",
      "train loss:0.07652564509081064\n",
      "train loss:0.03181814796642382\n",
      "train loss:0.09670855839404519\n",
      "train loss:0.03059484204449056\n",
      "train loss:0.0669699208388529\n",
      "train loss:0.06659939748919369\n",
      "train loss:0.04493476538072515\n",
      "train loss:0.02748917539320999\n",
      "train loss:0.057981432902853146\n",
      "train loss:0.06937516430057018\n",
      "train loss:0.03444914592916064\n",
      "train loss:0.037670392156692614\n",
      "train loss:0.08229745711911438\n",
      "train loss:0.04977843670252326\n",
      "train loss:0.06394810251740644\n",
      "train loss:0.022040891410786502\n",
      "train loss:0.09176814162808056\n",
      "train loss:0.03076005185359616\n",
      "train loss:0.08618558638179465\n",
      "train loss:0.02015586338020919\n",
      "train loss:0.08107833229696862\n",
      "train loss:0.03152752553582658\n",
      "train loss:0.09715228288131678\n",
      "train loss:0.017137486284270274\n",
      "train loss:0.03467535846799614\n",
      "train loss:0.027778074218804374\n",
      "train loss:0.11044487314794743\n",
      "train loss:0.1792390060886552\n",
      "train loss:0.027477777312021244\n",
      "train loss:0.0439233286078942\n",
      "train loss:0.030049708874142874\n",
      "train loss:0.07869036370483291\n",
      "train loss:0.05398784049050013\n",
      "train loss:0.05251863269472752\n",
      "train loss:0.004857290035118571\n",
      "train loss:0.025045852949813926\n",
      "train loss:0.043368222487700045\n",
      "train loss:0.029173773305413944\n",
      "train loss:0.12593704775874123\n",
      "train loss:0.10965949861666426\n",
      "train loss:0.12965548708248503\n",
      "train loss:0.08315521479880862\n",
      "train loss:0.06623456843548038\n",
      "train loss:0.014791050819765941\n",
      "train loss:0.02830691782114799\n",
      "train loss:0.015742173316874157\n",
      "train loss:0.1639065008948809\n",
      "train loss:0.08263013795254938\n",
      "train loss:0.04405010870366746\n",
      "train loss:0.06457565879830962\n",
      "train loss:0.08527279202604016\n",
      "train loss:0.02631428569682002\n",
      "train loss:0.017175695375867692\n",
      "train loss:0.024249870542694937\n",
      "train loss:0.039068590571907806\n",
      "train loss:0.023164907662230814\n",
      "train loss:0.0773974649446833\n",
      "train loss:0.04756208906881501\n",
      "train loss:0.035147348843358045\n",
      "train loss:0.01531314539906501\n",
      "train loss:0.03181967880435301\n",
      "train loss:0.08370832590655189\n",
      "train loss:0.1683780290391223\n",
      "train loss:0.03283111412865034\n",
      "train loss:0.018588379267449245\n",
      "train loss:0.09718870329683492\n",
      "train loss:0.2050302549164452\n",
      "train loss:0.014568774619010329\n",
      "train loss:0.023228600068369062\n",
      "train loss:0.09394212863578766\n",
      "train loss:0.05291126989448844\n",
      "train loss:0.014992368730291578\n",
      "train loss:0.09385280999821097\n",
      "train loss:0.027641663949903084\n",
      "train loss:0.03909327312099376\n",
      "train loss:0.024445160629184753\n",
      "train loss:0.07472986484857701\n",
      "train loss:0.04002377773862322\n",
      "train loss:0.09897512547084159\n",
      "train loss:0.08664208532588628\n",
      "train loss:0.028761718398867667\n",
      "train loss:0.05856407217145985\n",
      "train loss:0.05690541873561587\n",
      "train loss:0.10282688894328665\n",
      "train loss:0.06783260742646541\n",
      "train loss:0.09661376596369753\n",
      "train loss:0.032932936870405126\n",
      "train loss:0.028841592329749678\n",
      "train loss:0.03437096804472508\n",
      "train loss:0.03566541068385689\n",
      "train loss:0.022156168099914685\n",
      "train loss:0.019294479806680614\n",
      "train loss:0.06847663748737141\n",
      "train loss:0.030474373494186212\n",
      "train loss:0.06279106251474964\n",
      "train loss:0.0603740695677839\n",
      "train loss:0.052640976068401596\n",
      "train loss:0.017751082620964077\n",
      "train loss:0.08468378060920484\n",
      "train loss:0.08149747373910068\n",
      "train loss:0.025842880687677806\n",
      "train loss:0.07208011858550767\n",
      "train loss:0.06726452453867565\n",
      "train loss:0.061823925504579276\n",
      "train loss:0.037772819009681095\n",
      "train loss:0.017209446910255624\n",
      "train loss:0.03276228981415903\n",
      "train loss:0.1140973537212908\n",
      "train loss:0.04239305452164939\n",
      "train loss:0.09783551355799223\n",
      "train loss:0.03734125385310139\n",
      "train loss:0.09809635490088146\n",
      "train loss:0.03773231581538545\n",
      "train loss:0.04576621280506427\n",
      "train loss:0.05234987716074716\n",
      "train loss:0.04968594171260729\n",
      "train loss:0.021438167118817478\n",
      "train loss:0.04485218909999658\n",
      "train loss:0.0654874669344045\n",
      "train loss:0.037224260065845026\n",
      "train loss:0.00912144715726501\n",
      "train loss:0.035483980569232354\n",
      "train loss:0.044578138152886096\n",
      "train loss:0.016500199628041636\n",
      "train loss:0.023909652705970848\n",
      "train loss:0.0769325447868392\n",
      "train loss:0.049474440824004116\n",
      "train loss:0.11702829220538938\n",
      "train loss:0.0322008340207983\n",
      "train loss:0.06929767313912949\n",
      "train loss:0.018554680383358457\n",
      "train loss:0.02927546901906104\n",
      "train loss:0.060691710773018714\n",
      "train loss:0.046300255589437296\n",
      "train loss:0.08845233124227829\n",
      "train loss:0.1123986215878886\n",
      "train loss:0.018177509276023858\n",
      "train loss:0.02465393360112264\n",
      "train loss:0.024584567446843068\n",
      "train loss:0.0400259024532134\n",
      "train loss:0.05679102512912748\n",
      "train loss:0.026156915177361545\n",
      "train loss:0.12006276445078802\n",
      "train loss:0.048455568547972394\n",
      "train loss:0.031392956134672\n",
      "train loss:0.14118214644982052\n",
      "train loss:0.02069856267832064\n",
      "train loss:0.048948504394459504\n",
      "train loss:0.05842599344740667\n",
      "train loss:0.04795973183121215\n",
      "train loss:0.029286959887813525\n",
      "train loss:0.04578850857997556\n",
      "train loss:0.049883401848465604\n",
      "train loss:0.060149234192496265\n",
      "train loss:0.04985625693693906\n",
      "train loss:0.005175426864760183\n",
      "train loss:0.02585394914366441\n",
      "train loss:0.0687386580089422\n",
      "train loss:0.040977219387997026\n",
      "train loss:0.015722907208546556\n",
      "train loss:0.05333716482666011\n",
      "train loss:0.07560439578412294\n",
      "train loss:0.04410733833793386\n",
      "train loss:0.05307154900968419\n",
      "train loss:0.052081367290402775\n",
      "train loss:0.13694584868754267\n",
      "train loss:0.12412924150854922\n",
      "train loss:0.06409107488997548\n",
      "train loss:0.02718598786858064\n",
      "train loss:0.02516579204407875\n",
      "train loss:0.029303796727280576\n",
      "train loss:0.08443229320539009\n",
      "train loss:0.06910288952503461\n",
      "train loss:0.11562369609366423\n",
      "train loss:0.013197107570709852\n",
      "train loss:0.0605827573239291\n",
      "train loss:0.08416618454489383\n",
      "train loss:0.08594591795044496\n",
      "train loss:0.04651095393989948\n",
      "train loss:0.0504417661276696\n",
      "train loss:0.02873185179195491\n",
      "train loss:0.02247953290888133\n",
      "train loss:0.030108369472439108\n",
      "train loss:0.017863450881430317\n",
      "train loss:0.02682372103443395\n",
      "train loss:0.028582730242987228\n",
      "train loss:0.03564601997572289\n",
      "train loss:0.04164647000432919\n",
      "train loss:0.05222684762852331\n",
      "train loss:0.03460002404713764\n",
      "train loss:0.019792136439636024\n",
      "train loss:0.05004863596543409\n",
      "train loss:0.03517237864046491\n",
      "train loss:0.06133363248279915\n",
      "train loss:0.03844243622119983\n",
      "train loss:0.05980773961758377\n",
      "train loss:0.08719419071790964\n",
      "train loss:0.046145952700685174\n",
      "train loss:0.02682922723586374\n",
      "train loss:0.0280214076762973\n",
      "train loss:0.06662561933866509\n",
      "train loss:0.10099692536450727\n",
      "train loss:0.07135962375108064\n",
      "train loss:0.1424043524187716\n",
      "train loss:0.04075499925142093\n",
      "train loss:0.045822031855824515\n",
      "train loss:0.015161075056676658\n",
      "train loss:0.02770926155542203\n",
      "train loss:0.01875193552393423\n",
      "train loss:0.07753006016487879\n",
      "train loss:0.0691822858598571\n",
      "train loss:0.036468786967476526\n",
      "train loss:0.0492840854814441\n",
      "train loss:0.018652819035565548\n",
      "train loss:0.01277470536629143\n",
      "train loss:0.032710896023174535\n",
      "train loss:0.12124284191780137\n",
      "train loss:0.051483648306066755\n",
      "train loss:0.011640712099796249\n",
      "train loss:0.019967016036429384\n",
      "train loss:0.08409146460561734\n",
      "train loss:0.039777591460629626\n",
      "train loss:0.015073231875045091\n",
      "train loss:0.0806187301265338\n",
      "train loss:0.04419603017408018\n",
      "train loss:0.06602972659697484\n",
      "train loss:0.04374947013719166\n",
      "train loss:0.01365493496297565\n",
      "train loss:0.022276336327152937\n",
      "train loss:0.0313506536613056\n",
      "train loss:0.018804701238666365\n",
      "train loss:0.018257375976952473\n",
      "train loss:0.09873155935917556\n",
      "train loss:0.01689508861129403\n",
      "train loss:0.06003101435987748\n",
      "train loss:0.08350077643413513\n",
      "train loss:0.030330840316876675\n",
      "train loss:0.008655768110942284\n",
      "train loss:0.03565601601463973\n",
      "train loss:0.012935355515754279\n",
      "train loss:0.030049316268155133\n",
      "train loss:0.011580516011464792\n",
      "train loss:0.026297689824084185\n",
      "train loss:0.00571092763321514\n",
      "train loss:0.05366086772872266\n",
      "train loss:0.023529735513142306\n",
      "train loss:0.031391828969774875\n",
      "train loss:0.03981708197955719\n",
      "train loss:0.018042979591870904\n",
      "train loss:0.08358057533440333\n",
      "train loss:0.016166343846477024\n",
      "train loss:0.004898099402033862\n",
      "train loss:0.015429717333239392\n",
      "train loss:0.0683220747128296\n",
      "train loss:0.10624663649402467\n",
      "train loss:0.05441392787037924\n",
      "train loss:0.025043095430642284\n",
      "train loss:0.07540485075578579\n",
      "train loss:0.018049287081896327\n",
      "train loss:0.03840432713944991\n",
      "train loss:0.03130391410378885\n",
      "train loss:0.024952539258764637\n",
      "train loss:0.03803710467881208\n",
      "train loss:0.023639689367780353\n",
      "train loss:0.058621177659749765\n",
      "train loss:0.10850949478066718\n",
      "train loss:0.083518413845456\n",
      "train loss:0.02129086448177275\n",
      "train loss:0.03187962572484118\n",
      "train loss:0.036743368043274494\n",
      "train loss:0.151522035609953\n",
      "train loss:0.11733248362356428\n",
      "train loss:0.05605407436889478\n",
      "train loss:0.10371465419798387\n",
      "train loss:0.06507004027016289\n",
      "train loss:0.07791882069459084\n",
      "train loss:0.05684235919809733\n",
      "train loss:0.11770751299681737\n",
      "train loss:0.03192562090593051\n",
      "train loss:0.018505096527396347\n",
      "train loss:0.024833705622823155\n",
      "train loss:0.06505918804158312\n",
      "train loss:0.04824197250167746\n",
      "train loss:0.035238118579745\n",
      "train loss:0.015035750165300198\n",
      "train loss:0.033113862958924184\n",
      "train loss:0.02542792939319914\n",
      "train loss:0.02715241113472408\n",
      "train loss:0.06786465428287072\n",
      "train loss:0.043851502815673404\n",
      "train loss:0.05232543850885465\n",
      "train loss:0.1204538357697537\n",
      "train loss:0.017630756527641\n",
      "train loss:0.12066825083518505\n",
      "train loss:0.027192196131333862\n",
      "train loss:0.01902852698869662\n",
      "train loss:0.03213468111914399\n",
      "train loss:0.022039023413947024\n",
      "train loss:0.04894774904622112\n",
      "train loss:0.04476258596358788\n",
      "train loss:0.03275830973970336\n",
      "train loss:0.03362111682963456\n",
      "train loss:0.041295109537654404\n",
      "train loss:0.022640349667695653\n",
      "train loss:0.05569316594576639\n",
      "train loss:0.04455682527898739\n",
      "train loss:0.020984248747126545\n",
      "train loss:0.022588990624620938\n",
      "train loss:0.04399867937725516\n",
      "train loss:0.020173944174184864\n",
      "train loss:0.08500390364554584\n",
      "train loss:0.08718903824499558\n",
      "train loss:0.03244030172323904\n",
      "train loss:0.21568148842141752\n",
      "train loss:0.011396727581092354\n",
      "train loss:0.014368489815874746\n",
      "train loss:0.03431597335004072\n",
      "train loss:0.04624985524985677\n",
      "train loss:0.12182942068643884\n",
      "train loss:0.03987041441879427\n",
      "train loss:0.021284640370985704\n",
      "train loss:0.07407238249730881\n",
      "train loss:0.06699421402395277\n",
      "train loss:0.05151758736921039\n",
      "train loss:0.0659230851868013\n",
      "train loss:0.02731398236494693\n",
      "train loss:0.04920615578908513\n",
      "train loss:0.025801018290523905\n",
      "train loss:0.03883472222581295\n",
      "train loss:0.1002309253279672\n",
      "train loss:0.008820163153954178\n",
      "train loss:0.08306351081949455\n",
      "train loss:0.009906334300329655\n",
      "train loss:0.016669961670236533\n",
      "train loss:0.0363357720203759\n",
      "train loss:0.052224148755525245\n",
      "train loss:0.0188433465817908\n",
      "train loss:0.062382736971788984\n",
      "train loss:0.07770040023190435\n",
      "train loss:0.11448444168334794\n",
      "train loss:0.059200961361480474\n",
      "train loss:0.07531539765930007\n",
      "train loss:0.18709047409186752\n",
      "train loss:0.01796206301938321\n",
      "train loss:0.032816477591805955\n",
      "train loss:0.09384490248741428\n",
      "train loss:0.03662161730598727\n",
      "train loss:0.041226050242756385\n",
      "train loss:0.06575503252020842\n",
      "train loss:0.06105210464290596\n",
      "train loss:0.01778240929127648\n",
      "train loss:0.020856136682170193\n",
      "train loss:0.05199330755552997\n",
      "train loss:0.03105326957450754\n",
      "train loss:0.045846298447495284\n",
      "train loss:0.04107858973849734\n",
      "train loss:0.10936953260897525\n",
      "train loss:0.013997823156667874\n",
      "train loss:0.09936099351973801\n",
      "train loss:0.05120149167645395\n",
      "train loss:0.03716545676046172\n",
      "train loss:0.028947879534382288\n",
      "train loss:0.10280792445715986\n",
      "train loss:0.03946034054978127\n",
      "train loss:0.029623966829061397\n",
      "train loss:0.013041116577252489\n",
      "train loss:0.07419532452324337\n",
      "train loss:0.047163109026380684\n",
      "train loss:0.04748222352494455\n",
      "train loss:0.01207569860889149\n",
      "train loss:0.06128258595439614\n",
      "train loss:0.008023555448849324\n",
      "train loss:0.009065962114179423\n",
      "train loss:0.016497870821767\n",
      "train loss:0.02083229634916474\n",
      "train loss:0.015557235105291506\n",
      "train loss:0.026700629782380084\n",
      "train loss:0.01971657324064072\n",
      "train loss:0.023536658609463235\n",
      "train loss:0.031948181101220705\n",
      "train loss:0.05639869991756624\n",
      "train loss:0.04028733963384182\n",
      "train loss:0.08359359457487431\n",
      "train loss:0.026441249544302915\n",
      "train loss:0.021008349808599155\n",
      "train loss:0.017333069370122876\n",
      "train loss:0.05136853338617803\n",
      "train loss:0.03340525270299805\n",
      "train loss:0.052854193226757606\n",
      "train loss:0.09315478086962482\n",
      "train loss:0.009895853782040144\n",
      "train loss:0.012154955564627314\n",
      "train loss:0.044766206766265054\n",
      "train loss:0.02681532854536767\n",
      "train loss:0.09773992235109695\n",
      "train loss:0.049570152963009824\n",
      "train loss:0.03893514927020994\n",
      "train loss:0.013852818221735383\n",
      "train loss:0.08106063277220951\n",
      "train loss:0.010222446993191667\n",
      "train loss:0.04345282155941502\n",
      "train loss:0.028056289917043578\n",
      "train loss:0.011448906601966785\n",
      "train loss:0.015749041611944622\n",
      "train loss:0.11347365976951325\n",
      "train loss:0.008608365885484186\n",
      "train loss:0.02455498102820077\n",
      "train loss:0.009096493884310511\n",
      "train loss:0.06324739713135441\n",
      "train loss:0.05819151617224767\n",
      "train loss:0.09802986694837773\n",
      "train loss:0.03826829012175006\n",
      "train loss:0.015892195328280498\n",
      "train loss:0.02553372212326469\n",
      "train loss:0.06480388395425056\n",
      "train loss:0.053679600260795775\n",
      "train loss:0.02510098623576591\n",
      "train loss:0.03683484851355384\n",
      "train loss:0.05133883978793881\n",
      "train loss:0.0298389965847177\n",
      "train loss:0.05150628664896107\n",
      "train loss:0.03962253490943161\n",
      "train loss:0.048040365702613524\n",
      "train loss:0.0890400070733981\n",
      "train loss:0.061923651653936\n",
      "train loss:0.091296413016443\n",
      "train loss:0.038488710893765765\n",
      "train loss:0.05105179603310711\n",
      "train loss:0.00835985920201519\n",
      "train loss:0.02372184450840421\n",
      "train loss:0.011573740682451799\n",
      "train loss:0.026952435369232763\n",
      "train loss:0.030232758447065036\n",
      "train loss:0.016715040831093323\n",
      "train loss:0.03647914966579121\n",
      "train loss:0.05164067116353149\n",
      "train loss:0.01829642415050609\n",
      "train loss:0.14063600339437676\n",
      "train loss:0.05256792316999549\n",
      "train loss:0.050729047627315026\n",
      "train loss:0.08532695150883678\n",
      "train loss:0.0333784765313666\n",
      "train loss:0.02136210095568002\n",
      "train loss:0.02814299442237453\n",
      "train loss:0.004876178557466914\n",
      "train loss:0.08438156612805878\n",
      "train loss:0.018177122681963642\n",
      "train loss:0.04975358685925855\n",
      "train loss:0.030440977300183823\n",
      "train loss:0.10904828701189029\n",
      "train loss:0.04795784249657892\n",
      "train loss:0.03481790479248208\n",
      "train loss:0.017986041814925843\n",
      "=== epoch:4, train acc:0.981, test acc:0.979 ===\n",
      "train loss:0.04587180781638372\n",
      "train loss:0.03160182959305435\n",
      "train loss:0.035246576273233375\n",
      "train loss:0.06542899412065677\n",
      "train loss:0.016373148080315786\n",
      "train loss:0.08319621996441061\n",
      "train loss:0.03479273528509726\n",
      "train loss:0.08805339250729602\n",
      "train loss:0.044847911584180264\n",
      "train loss:0.09526896851559315\n",
      "train loss:0.04214964838885659\n",
      "train loss:0.028369152578999676\n",
      "train loss:0.10593780861805596\n",
      "train loss:0.12485938985617123\n",
      "train loss:0.06008136578002782\n",
      "train loss:0.017284372390579568\n",
      "train loss:0.08115596365258865\n",
      "train loss:0.018210093615561293\n",
      "train loss:0.016484715757180073\n",
      "train loss:0.025415287211789386\n",
      "train loss:0.00930792723084276\n",
      "train loss:0.013856151375222496\n",
      "train loss:0.021222558245129086\n",
      "train loss:0.06265322684128696\n",
      "train loss:0.03524812162185717\n",
      "train loss:0.022639253920421223\n",
      "train loss:0.009933327566829186\n",
      "train loss:0.05872654509488022\n",
      "train loss:0.027466096111273246\n",
      "train loss:0.104797787412669\n",
      "train loss:0.038533996443530574\n",
      "train loss:0.032348801623363725\n",
      "train loss:0.02834474286726628\n",
      "train loss:0.014774228375145606\n",
      "train loss:0.020850017331410044\n",
      "train loss:0.05452063618328963\n",
      "train loss:0.11166626152854409\n",
      "train loss:0.01892547904267222\n",
      "train loss:0.07919322242500505\n",
      "train loss:0.03346058123287668\n",
      "train loss:0.024158491281073925\n",
      "train loss:0.017215941667964575\n",
      "train loss:0.03704778635256611\n",
      "train loss:0.0480458290732286\n",
      "train loss:0.0682169839841633\n",
      "train loss:0.1768287063170416\n",
      "train loss:0.028310192073140193\n",
      "train loss:0.01722524008355109\n",
      "train loss:0.0489757836153755\n",
      "train loss:0.02591253433649925\n",
      "train loss:0.011615448853098367\n",
      "train loss:0.11921453739553886\n",
      "train loss:0.015201166375357567\n",
      "train loss:0.02012232838124249\n",
      "train loss:0.11912772004060868\n",
      "train loss:0.010753048818737326\n",
      "train loss:0.06122753219187558\n",
      "train loss:0.10013768856956157\n",
      "train loss:0.03218233001802909\n",
      "train loss:0.035166571892254035\n",
      "train loss:0.04342832777523359\n",
      "train loss:0.015179340367416241\n",
      "train loss:0.00986149498435436\n",
      "train loss:0.009991598272945199\n",
      "train loss:0.02145692977569334\n",
      "train loss:0.07155956124081764\n",
      "train loss:0.04603263780633217\n",
      "train loss:0.05007438114096788\n",
      "train loss:0.017084324080619145\n",
      "train loss:0.04363001824210414\n",
      "train loss:0.022943427269227757\n",
      "train loss:0.05123076100944588\n",
      "train loss:0.03653843940341595\n",
      "train loss:0.011575421945481907\n",
      "train loss:0.07699757633608095\n",
      "train loss:0.020126724221188413\n",
      "train loss:0.055365661171196366\n",
      "train loss:0.010526426214148626\n",
      "train loss:0.01976038940527437\n",
      "train loss:0.00940565644430576\n",
      "train loss:0.08412734085364107\n",
      "train loss:0.04273835499660736\n",
      "train loss:0.01041494511634194\n",
      "train loss:0.07457109634986578\n",
      "train loss:0.10337260757551142\n",
      "train loss:0.09783604080580162\n",
      "train loss:0.05999255673372891\n",
      "train loss:0.058748539750095974\n",
      "train loss:0.05846776673262194\n",
      "train loss:0.06255526360262383\n",
      "train loss:0.03823190116332677\n",
      "train loss:0.0431299644569536\n",
      "train loss:0.0306907387989722\n",
      "train loss:0.034868794975591456\n",
      "train loss:0.07140847266093518\n",
      "train loss:0.011424514606375091\n",
      "train loss:0.013048965533564367\n",
      "train loss:0.016397124291160813\n",
      "train loss:0.06993609815355185\n",
      "train loss:0.0310654672445867\n",
      "train loss:0.04480175570929619\n",
      "train loss:0.04442648746012768\n",
      "train loss:0.057176743878595754\n",
      "train loss:0.05228159373349627\n",
      "train loss:0.01914114176096619\n",
      "train loss:0.017461250504089237\n",
      "train loss:0.055910604958128426\n",
      "train loss:0.025301090555563684\n",
      "train loss:0.014323790009816009\n",
      "train loss:0.03689036093906368\n",
      "train loss:0.0541456203443847\n",
      "train loss:0.07593665807604873\n",
      "train loss:0.037194071683718154\n",
      "train loss:0.06318083005308579\n",
      "train loss:0.05775132627429904\n",
      "train loss:0.04455104394012332\n",
      "train loss:0.015773812108146018\n",
      "train loss:0.051826129303597035\n",
      "train loss:0.13379987495949605\n",
      "train loss:0.025197776677698215\n",
      "train loss:0.07429501360835333\n",
      "train loss:0.020543183905021002\n",
      "train loss:0.019651176255229937\n",
      "train loss:0.03424466024188391\n",
      "train loss:0.013966116122217525\n",
      "train loss:0.00723110451155028\n",
      "train loss:0.007454933809015391\n",
      "train loss:0.013044904321598827\n",
      "train loss:0.01695927305149664\n",
      "train loss:0.02908499987044678\n",
      "train loss:0.024127647224028593\n",
      "train loss:0.008811215839339222\n",
      "train loss:0.019165020592732073\n",
      "train loss:0.01967063294405998\n",
      "train loss:0.036091484394972895\n",
      "train loss:0.012259827085709642\n",
      "train loss:0.05287058578431484\n",
      "train loss:0.016412510158959857\n",
      "train loss:0.047447711434260666\n",
      "train loss:0.026635924550440473\n",
      "train loss:0.013444980027208151\n",
      "train loss:0.009858094957484664\n",
      "train loss:0.03975123654647905\n",
      "train loss:0.039482439372729335\n",
      "train loss:0.011794823225071482\n",
      "train loss:0.006982584903642166\n",
      "train loss:0.0764673241653965\n",
      "train loss:0.0029611816504630153\n",
      "train loss:0.02751643442093956\n",
      "train loss:0.01566970813379567\n",
      "train loss:0.011960193661156561\n",
      "train loss:0.08543736766275839\n",
      "train loss:0.008090624417607648\n",
      "train loss:0.051353542988734874\n",
      "train loss:0.0365243785652719\n",
      "train loss:0.01339398757793877\n",
      "train loss:0.026027073981108595\n",
      "train loss:0.02709059968402247\n",
      "train loss:0.014776214105056973\n",
      "train loss:0.0276411811529132\n",
      "train loss:0.006808349605392383\n",
      "train loss:0.10120209988879504\n",
      "train loss:0.04461111521021754\n",
      "train loss:0.03896653372176417\n",
      "train loss:0.019538514855687198\n",
      "train loss:0.0760990895285457\n",
      "train loss:0.042262155951907944\n",
      "train loss:0.0814268834165875\n",
      "train loss:0.035629998682197794\n",
      "train loss:0.031251257935711\n",
      "train loss:0.010220104635705766\n",
      "train loss:0.01962871229938952\n",
      "train loss:0.019555783502755004\n",
      "train loss:0.08107271354291717\n",
      "train loss:0.09947462685805718\n",
      "train loss:0.019021818175419748\n",
      "train loss:0.010028320908343198\n",
      "train loss:0.09876480602897349\n",
      "train loss:0.03889562645826354\n",
      "train loss:0.005011382280516966\n",
      "train loss:0.006691823086514958\n",
      "train loss:0.04765201996545761\n",
      "train loss:0.02090533927374697\n",
      "train loss:0.03511812135704742\n",
      "train loss:0.015153478680954145\n",
      "train loss:0.04368775241938385\n",
      "train loss:0.025711303302438343\n",
      "train loss:0.011559047661126021\n",
      "train loss:0.01594198555329345\n",
      "train loss:0.015789701392791667\n",
      "train loss:0.026541643920272567\n",
      "train loss:0.039437145150260325\n",
      "train loss:0.010929754719524915\n",
      "train loss:0.03791832351489401\n",
      "train loss:0.058923461839025\n",
      "train loss:0.033138844062058706\n",
      "train loss:0.015356238527344019\n",
      "train loss:0.07090721231590284\n",
      "train loss:0.012769377402800495\n",
      "train loss:0.009689630757331742\n",
      "train loss:0.010575269313567653\n",
      "train loss:0.031102747889982133\n",
      "train loss:0.02083329859293375\n",
      "train loss:0.02158605318282866\n",
      "train loss:0.04110108869420963\n",
      "train loss:0.11231907084330776\n",
      "train loss:0.011731157857135117\n",
      "train loss:0.018178045245408482\n",
      "train loss:0.03552187675734573\n",
      "train loss:0.008645030619191666\n",
      "train loss:0.06751866118052174\n",
      "train loss:0.07421798408139037\n",
      "train loss:0.028050211689081186\n",
      "train loss:0.023079403309245166\n",
      "train loss:0.06887223483358323\n",
      "train loss:0.0586734430780826\n",
      "train loss:0.03104679355993547\n",
      "train loss:0.046190067865290674\n",
      "train loss:0.048926023407371705\n",
      "train loss:0.019417287135327102\n",
      "train loss:0.013169023137522128\n",
      "train loss:0.031177349443211693\n",
      "train loss:0.10518304018624587\n",
      "train loss:0.023017614151153002\n",
      "train loss:0.03886432872711103\n",
      "train loss:0.08002049260202593\n",
      "train loss:0.049270659400558914\n",
      "train loss:0.010315054083461884\n",
      "train loss:0.06727707236722046\n",
      "train loss:0.060031814940826794\n",
      "train loss:0.028840379020587437\n",
      "train loss:0.12810797256432888\n",
      "train loss:0.021991505873610085\n",
      "train loss:0.056793163039932405\n",
      "train loss:0.1181321821295242\n",
      "train loss:0.02654008773016125\n",
      "train loss:0.0805190047925786\n",
      "train loss:0.05340347048993764\n",
      "train loss:0.06688779885436481\n",
      "train loss:0.013120117330387562\n",
      "train loss:0.010065443207898952\n",
      "train loss:0.03072765752222101\n",
      "train loss:0.06033686258553448\n",
      "train loss:0.009017585186395767\n",
      "train loss:0.04750314448192986\n",
      "train loss:0.10194425431060786\n",
      "train loss:0.07466411265064128\n",
      "train loss:0.03573661156156857\n",
      "train loss:0.11988450656294901\n",
      "train loss:0.06612181161763635\n",
      "train loss:0.053630346945101215\n",
      "train loss:0.0514289873348834\n",
      "train loss:0.008361396075623957\n",
      "train loss:0.06683295296417695\n",
      "train loss:0.11896458652040719\n",
      "train loss:0.012627008250693439\n",
      "train loss:0.035428868033354564\n",
      "train loss:0.040202456629159763\n",
      "train loss:0.06091007196407757\n",
      "train loss:0.016318850585073465\n",
      "train loss:0.019532025125258045\n",
      "train loss:0.03997685410195383\n",
      "train loss:0.02075401281210709\n",
      "train loss:0.04422039283802423\n",
      "train loss:0.012119149881053346\n",
      "train loss:0.11093364509054615\n",
      "train loss:0.09688535997981364\n",
      "train loss:0.0909467311627337\n",
      "train loss:0.039774775334990804\n",
      "train loss:0.07240709309243198\n",
      "train loss:0.04203012057242675\n",
      "train loss:0.04477364073233365\n",
      "train loss:0.10474994578787022\n",
      "train loss:0.014742969850621377\n",
      "train loss:0.0403089291260763\n",
      "train loss:0.011040461294943658\n",
      "train loss:0.023069512494683232\n",
      "train loss:0.08327178723975573\n",
      "train loss:0.019342122722437634\n",
      "train loss:0.07604003855204466\n",
      "train loss:0.026171858462365253\n",
      "train loss:0.08124215244621194\n",
      "train loss:0.035592779835453184\n",
      "train loss:0.01525743846904774\n",
      "train loss:0.01843046644182361\n",
      "train loss:0.08541495456192523\n",
      "train loss:0.011163484748767417\n",
      "train loss:0.02893687120440214\n",
      "train loss:0.006601141308716521\n",
      "train loss:0.06625175958342473\n",
      "train loss:0.03241056785330602\n",
      "train loss:0.04561243955717619\n",
      "train loss:0.069952573797426\n",
      "train loss:0.05054414320771597\n",
      "train loss:0.03378940450007587\n",
      "train loss:0.03670007429103749\n",
      "train loss:0.009222299385840127\n",
      "train loss:0.04070259946691107\n",
      "train loss:0.03347949940419583\n",
      "train loss:0.016666151627061688\n",
      "train loss:0.02083770759668854\n",
      "train loss:0.008975028148862293\n",
      "train loss:0.020463420719883593\n",
      "train loss:0.006902199569776272\n",
      "train loss:0.04175174003211253\n",
      "train loss:0.02216856293886484\n",
      "train loss:0.0284972177974819\n",
      "train loss:0.026847823498265387\n",
      "train loss:0.06833991778937856\n",
      "train loss:0.07765933804018171\n",
      "train loss:0.035403697241240276\n",
      "train loss:0.049118356207636014\n",
      "train loss:0.00921860372180353\n",
      "train loss:0.1164640519646124\n",
      "train loss:0.06368125543189185\n",
      "train loss:0.046578060181772116\n",
      "train loss:0.0077832783903877935\n",
      "train loss:0.0279784269232511\n",
      "train loss:0.029591223636375105\n",
      "train loss:0.006458040756524084\n",
      "train loss:0.04557446331961373\n",
      "train loss:0.025260310815372788\n",
      "train loss:0.022813853749017968\n",
      "train loss:0.0389767321466805\n",
      "train loss:0.015102263602556586\n",
      "train loss:0.03169169900441369\n",
      "train loss:0.011845284126667239\n",
      "train loss:0.015346612796750883\n",
      "train loss:0.01151755255922448\n",
      "train loss:0.014847510357698854\n",
      "train loss:0.017330905212933987\n",
      "train loss:0.04031921872088476\n",
      "train loss:0.014666721975454301\n",
      "train loss:0.006766535528666889\n",
      "train loss:0.01835347915234053\n",
      "train loss:0.02379398000022502\n",
      "train loss:0.003693626999171259\n",
      "train loss:0.02056227843179038\n",
      "train loss:0.06538264944739311\n",
      "train loss:0.008104636399385664\n",
      "train loss:0.008386670889199945\n",
      "train loss:0.04559835537180592\n",
      "train loss:0.012678934427728532\n",
      "train loss:0.09515403932437204\n",
      "train loss:0.05395764440070267\n",
      "train loss:0.03531491724386397\n",
      "train loss:0.03638754333538642\n",
      "train loss:0.007773688243821931\n",
      "train loss:0.030421258016350743\n",
      "train loss:0.030661673078064586\n",
      "train loss:0.06053480071770084\n",
      "train loss:0.0803881202451442\n",
      "train loss:0.015970879813240576\n",
      "train loss:0.02472450653456812\n",
      "train loss:0.09745260615361306\n",
      "train loss:0.010916786834738486\n",
      "train loss:0.1001668062745559\n",
      "train loss:0.010376947202049278\n",
      "train loss:0.011072956487834202\n",
      "train loss:0.023388758517409527\n",
      "train loss:0.021456269625433816\n",
      "train loss:0.010647380299314226\n",
      "train loss:0.054536691655227204\n",
      "train loss:0.0321513491432171\n",
      "train loss:0.03641330568790073\n",
      "train loss:0.03188396877528868\n",
      "train loss:0.029543581457634947\n",
      "train loss:0.008107504319323245\n",
      "train loss:0.04598014864817314\n",
      "train loss:0.02329322793030489\n",
      "train loss:0.022136263659250213\n",
      "train loss:0.013118134068859463\n",
      "train loss:0.006053626588356207\n",
      "train loss:0.02800885875636824\n",
      "train loss:0.04840730425235191\n",
      "train loss:0.017339945766277752\n",
      "train loss:0.12996828010019845\n",
      "train loss:0.02721701423214727\n",
      "train loss:0.060342748990399465\n",
      "train loss:0.05146745410839133\n",
      "train loss:0.06087150759582473\n",
      "train loss:0.027144762538923083\n",
      "train loss:0.029306753856014788\n",
      "train loss:0.041854093944836315\n",
      "train loss:0.049010688587598744\n",
      "train loss:0.03367058611178869\n",
      "train loss:0.013231133077135948\n",
      "train loss:0.04209851560158166\n",
      "train loss:0.03768152592004625\n",
      "train loss:0.024385809953253616\n",
      "train loss:0.011050143338915184\n",
      "train loss:0.022864994113441773\n",
      "train loss:0.014956985791040163\n",
      "train loss:0.06965434720317712\n",
      "train loss:0.026398231203465322\n",
      "train loss:0.009725295078931993\n",
      "train loss:0.01187240489469202\n",
      "train loss:0.013856665234403726\n",
      "train loss:0.04805939286164553\n",
      "train loss:0.019881930283920643\n",
      "train loss:0.017276296306291145\n",
      "train loss:0.005669830555844091\n",
      "train loss:0.01871478663756043\n",
      "train loss:0.0873322374185121\n",
      "train loss:0.012257941749851958\n",
      "train loss:0.08844200613023163\n",
      "train loss:0.047120769011025396\n",
      "train loss:0.07723089011103179\n",
      "train loss:0.08723603569216548\n",
      "train loss:0.026421196741028396\n",
      "train loss:0.018072514580662148\n",
      "train loss:0.10806022885763249\n",
      "train loss:0.010634540296433919\n",
      "train loss:0.0437031135863867\n",
      "train loss:0.043346976841353954\n",
      "train loss:0.03156420070744083\n",
      "train loss:0.03781249347171525\n",
      "train loss:0.040044381034177484\n",
      "train loss:0.028210361322782825\n",
      "train loss:0.010493762464851423\n",
      "train loss:0.08029399532352087\n",
      "train loss:0.010555765839007572\n",
      "train loss:0.04468610670510651\n",
      "train loss:0.011650794177607043\n",
      "train loss:0.1111467709424256\n",
      "train loss:0.02442069738656993\n",
      "train loss:0.015018044437629179\n",
      "train loss:0.03278103186786239\n",
      "train loss:0.015188569935488418\n",
      "train loss:0.009205942285415002\n",
      "train loss:0.02372387376911012\n",
      "train loss:0.04622098051345027\n",
      "train loss:0.03956081102506294\n",
      "train loss:0.057157258152257494\n",
      "train loss:0.02746357414078539\n",
      "train loss:0.007066703644768486\n",
      "train loss:0.015214444496350718\n",
      "train loss:0.006897948771284013\n",
      "train loss:0.003569370507436638\n",
      "train loss:0.046407449862842176\n",
      "train loss:0.019878189549532646\n",
      "train loss:0.030691259068114145\n",
      "train loss:0.00904329650081775\n",
      "train loss:0.014173816230132485\n",
      "train loss:0.012835491639807262\n",
      "train loss:0.07471337191314249\n",
      "train loss:0.01067840065811648\n",
      "train loss:0.041989767004393296\n",
      "train loss:0.01207710498549171\n",
      "train loss:0.015247341654596831\n",
      "train loss:0.007391050474257069\n",
      "train loss:0.046282422093267765\n",
      "train loss:0.029545626811874087\n",
      "train loss:0.025338240265535297\n",
      "train loss:0.020585827024220803\n",
      "train loss:0.09064223195797533\n",
      "train loss:0.029018348007732776\n",
      "train loss:0.05178605308562204\n",
      "train loss:0.005524774372371607\n",
      "train loss:0.009798069662647098\n",
      "train loss:0.05706919026901535\n",
      "train loss:0.07443338541845451\n",
      "train loss:0.06011998722624472\n",
      "train loss:0.011150770471269347\n",
      "train loss:0.04844947715219778\n",
      "train loss:0.024001684558886547\n",
      "train loss:0.01341317729136544\n",
      "train loss:0.011580699869741045\n",
      "train loss:0.057020381683428385\n",
      "train loss:0.0407188624198419\n",
      "train loss:0.01982221030528606\n",
      "train loss:0.043010659365721834\n",
      "train loss:0.014715862354628812\n",
      "train loss:0.039562933436938466\n",
      "train loss:0.030203360868585927\n",
      "train loss:0.019679056900391175\n",
      "train loss:0.058513708567032\n",
      "train loss:0.03413634470803941\n",
      "train loss:0.02300279943558786\n",
      "train loss:0.036110495292588286\n",
      "train loss:0.018962790632306593\n",
      "train loss:0.02290395941059845\n",
      "train loss:0.0520872157058057\n",
      "train loss:0.03243956977646907\n",
      "train loss:0.03496363489043453\n",
      "train loss:0.009496300089382338\n",
      "train loss:0.009686844145141283\n",
      "train loss:0.009758596079748579\n",
      "train loss:0.017618224418693085\n",
      "train loss:0.0133028984771758\n",
      "train loss:0.04972326935461445\n",
      "train loss:0.007859408208646836\n",
      "train loss:0.039927174022951886\n",
      "train loss:0.00885400810644088\n",
      "train loss:0.060006187095363345\n",
      "train loss:0.01428979479784812\n",
      "train loss:0.008569106676615791\n",
      "train loss:0.021272484886215057\n",
      "train loss:0.039459786481301176\n",
      "train loss:0.032869878310222235\n",
      "train loss:0.014276188505578862\n",
      "train loss:0.04271903394655794\n",
      "train loss:0.01219966176252928\n",
      "train loss:0.012805623480533525\n",
      "train loss:0.09032950394068504\n",
      "train loss:0.033845585362483166\n",
      "train loss:0.016014718736870014\n",
      "train loss:0.024542454517580414\n",
      "train loss:0.07234129780230905\n",
      "train loss:0.008193419340164742\n",
      "train loss:0.01917711790080817\n",
      "train loss:0.0028038312834448935\n",
      "train loss:0.051369681810232874\n",
      "train loss:0.008783516794727797\n",
      "train loss:0.10543256592362417\n",
      "train loss:0.017941183688448905\n",
      "train loss:0.024051349799225092\n",
      "train loss:0.021122631881579507\n",
      "train loss:0.06518288121844078\n",
      "train loss:0.015024731309430422\n",
      "train loss:0.021148540259564527\n",
      "train loss:0.08763759162848438\n",
      "train loss:0.08036686241877579\n",
      "train loss:0.011458275420910206\n",
      "train loss:0.03168119728742988\n",
      "train loss:0.017920711485042765\n",
      "train loss:0.028212381349858196\n",
      "train loss:0.09859254142924635\n",
      "train loss:0.011801178198400241\n",
      "train loss:0.051937344490994075\n",
      "train loss:0.041608611790600755\n",
      "train loss:0.03959057012967116\n",
      "train loss:0.056523180964730156\n",
      "train loss:0.01689888783089178\n",
      "train loss:0.027810230610424858\n",
      "train loss:0.01197313760311474\n",
      "train loss:0.02513720187890883\n",
      "train loss:0.010070516186402437\n",
      "train loss:0.02638602824675892\n",
      "train loss:0.1261674349403731\n",
      "train loss:0.028917572760724698\n",
      "train loss:0.06818223501196963\n",
      "train loss:0.005694017497850517\n",
      "train loss:0.05650817128531899\n",
      "train loss:0.02427893474848021\n",
      "train loss:0.022489619924818988\n",
      "train loss:0.1116609140267828\n",
      "train loss:0.014507117511241967\n",
      "train loss:0.021059779664539514\n",
      "train loss:0.04094373435623031\n",
      "train loss:0.00995075196491403\n",
      "train loss:0.02189431981102109\n",
      "train loss:0.01697754959559581\n",
      "train loss:0.045265542315047036\n",
      "train loss:0.06577024508368541\n",
      "train loss:0.011525665946398821\n",
      "train loss:0.04332040004182318\n",
      "train loss:0.037451394499156423\n",
      "train loss:0.018769546064827114\n",
      "train loss:0.04396544798788348\n",
      "train loss:0.005438708940977159\n",
      "train loss:0.039607419979392995\n",
      "train loss:0.022784286935676655\n",
      "train loss:0.006594715475275815\n",
      "train loss:0.015473462535279044\n",
      "train loss:0.007740630577931512\n",
      "train loss:0.060569015549802166\n",
      "train loss:0.05791515109056231\n",
      "train loss:0.029070286621032237\n",
      "train loss:0.021506767035947113\n",
      "train loss:0.015945460869764828\n",
      "train loss:0.011977236045568298\n",
      "train loss:0.05445921910518917\n",
      "train loss:0.03337410901650975\n",
      "train loss:0.016884710807093153\n",
      "train loss:0.008719775295322326\n",
      "train loss:0.021996140031191987\n",
      "train loss:0.035154661810750415\n",
      "train loss:0.07021916240942457\n",
      "train loss:0.01995864834482731\n",
      "train loss:0.03480727366510256\n",
      "train loss:0.017106302620083634\n",
      "train loss:0.01988332657303202\n",
      "train loss:0.047890311922505625\n",
      "train loss:0.12307767514456965\n",
      "train loss:0.011361949242948722\n",
      "train loss:0.019408719156770372\n",
      "train loss:0.07227612882636789\n",
      "train loss:0.013820993171865314\n",
      "train loss:0.010443574791531504\n",
      "train loss:0.05670018585574054\n",
      "train loss:0.010717400097409987\n",
      "train loss:0.009418194809170458\n",
      "train loss:0.07234854711400143\n",
      "train loss:0.010089342229712831\n",
      "train loss:0.012385767935275196\n",
      "train loss:0.03153678308991815\n",
      "train loss:0.02521850638481142\n",
      "train loss:0.012077303765205162\n",
      "train loss:0.005836881051121331\n",
      "=== epoch:5, train acc:0.985, test acc:0.983 ===\n",
      "train loss:0.01792297166830993\n",
      "train loss:0.031020598176096485\n",
      "train loss:0.03032491762699917\n",
      "train loss:0.020885227980740318\n",
      "train loss:0.03815455838766455\n",
      "train loss:0.03843189391110484\n",
      "train loss:0.03250112815964837\n",
      "train loss:0.03387955346577085\n",
      "train loss:0.01398930634297457\n",
      "train loss:0.061620592550229676\n",
      "train loss:0.019038679729737804\n",
      "train loss:0.02255895671279376\n",
      "train loss:0.07233815427933085\n",
      "train loss:0.020939020312330445\n",
      "train loss:0.010727878141337722\n",
      "train loss:0.08491589262793893\n",
      "train loss:0.06245834857474236\n",
      "train loss:0.01058144930686768\n",
      "train loss:0.013703140886232206\n",
      "train loss:0.03753168068488959\n",
      "train loss:0.027451824377220223\n",
      "train loss:0.039623748931951745\n",
      "train loss:0.012877880892956226\n",
      "train loss:0.04386174280561776\n",
      "train loss:0.005402708753789683\n",
      "train loss:0.0044735247974112095\n",
      "train loss:0.06349395924122764\n",
      "train loss:0.020504855344790712\n",
      "train loss:0.013615961274299895\n",
      "train loss:0.026847221605141466\n",
      "train loss:0.039734101970576245\n",
      "train loss:0.054770012801104384\n",
      "train loss:0.01009645973300087\n",
      "train loss:0.05768448940481618\n",
      "train loss:0.009194342722112702\n",
      "train loss:0.01948924066981175\n",
      "train loss:0.0169274136110511\n",
      "train loss:0.04662748201928795\n",
      "train loss:0.03609596758188918\n",
      "train loss:0.03589089917900881\n",
      "train loss:0.038008898173566175\n",
      "train loss:0.04243095336748162\n",
      "train loss:0.048740903416830184\n",
      "train loss:0.026858401342348525\n",
      "train loss:0.03901756693856493\n",
      "train loss:0.04558577656892296\n",
      "train loss:0.04107015405361407\n",
      "train loss:0.014642895734605341\n",
      "train loss:0.0061099400139491035\n",
      "train loss:0.027256232491673537\n",
      "train loss:0.015486135389455466\n",
      "train loss:0.049191330037260575\n",
      "train loss:0.01698642526382756\n",
      "train loss:0.0506309460893946\n",
      "train loss:0.01616928026131753\n",
      "train loss:0.08875858989561351\n",
      "train loss:0.009968388745212286\n",
      "train loss:0.044778747299391276\n",
      "train loss:0.011881789424748738\n",
      "train loss:0.003064701635088044\n",
      "train loss:0.012154520875321572\n",
      "train loss:0.015392131561858582\n",
      "train loss:0.05165029362719101\n",
      "train loss:0.01083054091217967\n",
      "train loss:0.0036997748380849416\n",
      "train loss:0.11701979082693115\n",
      "train loss:0.03752270504628745\n",
      "train loss:0.03816324625967405\n",
      "train loss:0.008089572799018792\n",
      "train loss:0.023618007903975457\n",
      "train loss:0.01800495780353521\n",
      "train loss:0.06147845633435998\n",
      "train loss:0.016170584504539617\n",
      "train loss:0.007773761528770947\n",
      "train loss:0.1509834451194963\n",
      "train loss:0.020019389528660655\n",
      "train loss:0.051252004187453314\n",
      "train loss:0.009729626976869273\n",
      "train loss:0.018359941124568117\n",
      "train loss:0.06375731408552947\n",
      "train loss:0.00780016046837485\n",
      "train loss:0.026162074533803238\n",
      "train loss:0.019578819373343288\n",
      "train loss:0.03209119000932439\n",
      "train loss:0.0299664592431457\n",
      "train loss:0.011558051887930203\n",
      "train loss:0.030609113895395747\n",
      "train loss:0.021520950838164153\n",
      "train loss:0.06754319960951218\n",
      "train loss:0.05742383556523507\n",
      "train loss:0.03227421959303245\n",
      "train loss:0.026337267549114772\n",
      "train loss:0.03559991862608107\n",
      "train loss:0.01245039886410345\n",
      "train loss:0.03942946013961875\n",
      "train loss:0.008764291363798577\n",
      "train loss:0.05855408632289761\n",
      "train loss:0.02914758460522505\n",
      "train loss:0.05444364515771913\n",
      "train loss:0.024683566222019587\n",
      "train loss:0.06829209683806138\n",
      "train loss:0.02081977794500848\n",
      "train loss:0.07306600280020246\n",
      "train loss:0.07316894328843965\n",
      "train loss:0.08037041788465932\n",
      "train loss:0.016307974049433748\n",
      "train loss:0.012386225854041612\n",
      "train loss:0.04135877882873409\n",
      "train loss:0.0499580171987795\n",
      "train loss:0.06798205544232815\n",
      "train loss:0.02166542956677949\n",
      "train loss:0.08840752944429209\n",
      "train loss:0.15929175629575767\n",
      "train loss:0.05055113977148703\n",
      "train loss:0.047892736756728053\n",
      "train loss:0.057661021870522806\n",
      "train loss:0.02282556004597355\n",
      "train loss:0.02940529363537884\n",
      "train loss:0.08928770882654496\n",
      "train loss:0.0300033436374736\n",
      "train loss:0.011234813827970414\n",
      "train loss:0.03232769662079683\n",
      "train loss:0.01521003937120073\n",
      "train loss:0.013903608702649237\n",
      "train loss:0.015385450776552345\n",
      "train loss:0.025629469851758676\n",
      "train loss:0.00764573415750861\n",
      "train loss:0.008583391405689887\n",
      "train loss:0.02746078348069319\n",
      "train loss:0.021103438466129207\n",
      "train loss:0.012693987377523112\n",
      "train loss:0.010776987236641646\n",
      "train loss:0.0036709741801649936\n",
      "train loss:0.032211156214188075\n",
      "train loss:0.01438207690807558\n",
      "train loss:0.040945247098707005\n",
      "train loss:0.016649507422314084\n",
      "train loss:0.0173251781196688\n",
      "train loss:0.046818610648810675\n",
      "train loss:0.03017961841321283\n",
      "train loss:0.05192604415861233\n",
      "train loss:0.028356762237597897\n",
      "train loss:0.047889824484340766\n",
      "train loss:0.02377526358980704\n",
      "train loss:0.05448114810797136\n",
      "train loss:0.037652621407148736\n",
      "train loss:0.0379470897282891\n",
      "train loss:0.015713302822047488\n",
      "train loss:0.023123034183926156\n",
      "train loss:0.009645965377716089\n",
      "train loss:0.018294255656717703\n",
      "train loss:0.016268029199196704\n",
      "train loss:0.015634955150470028\n",
      "train loss:0.05727499357550293\n",
      "train loss:0.017450060837051563\n",
      "train loss:0.007676678937387357\n",
      "train loss:0.05880902927764942\n",
      "train loss:0.0085212401065411\n",
      "train loss:0.01760843477267452\n",
      "train loss:0.011650780841841495\n",
      "train loss:0.06906207861686528\n",
      "train loss:0.018907900933088747\n",
      "train loss:0.023750894391637817\n",
      "train loss:0.01391824708805209\n",
      "train loss:0.014558525543311263\n",
      "train loss:0.012979066636452685\n",
      "train loss:0.054117962574930054\n",
      "train loss:0.008908444521421801\n",
      "train loss:0.006813115989666416\n",
      "train loss:0.01126211844657845\n",
      "train loss:0.026186839883536078\n",
      "train loss:0.042176629305782344\n",
      "train loss:0.010982626795254434\n",
      "train loss:0.012769779596968258\n",
      "train loss:0.010407411408156627\n",
      "train loss:0.02445939554746749\n",
      "train loss:0.006177460186401601\n",
      "train loss:0.015728697270605663\n",
      "train loss:0.023318840087966634\n",
      "train loss:0.017476915413003506\n",
      "train loss:0.004541416665651833\n",
      "train loss:0.01374931629549734\n",
      "train loss:0.01707408220762323\n",
      "train loss:0.0062813725746436256\n",
      "train loss:0.022928118277374544\n",
      "train loss:0.016902401378358266\n",
      "train loss:0.033855110368260825\n",
      "train loss:0.018694665798298257\n",
      "train loss:0.03384002896661982\n",
      "train loss:0.016614215948066155\n",
      "train loss:0.0028863937077633892\n",
      "train loss:0.003703543580066544\n",
      "train loss:0.03211406473170758\n",
      "train loss:0.02607024815808536\n",
      "train loss:0.07974027778651943\n",
      "train loss:0.013007649737438693\n",
      "train loss:0.003967494568499179\n",
      "train loss:0.01594024775685521\n",
      "train loss:0.024043289353350646\n",
      "train loss:0.025531248186877777\n",
      "train loss:0.01329323766612946\n",
      "train loss:0.04938022803865038\n",
      "train loss:0.030888346321718455\n",
      "train loss:0.004049090134926878\n",
      "train loss:0.15530866427410372\n",
      "train loss:0.029892585790803446\n",
      "train loss:0.031904364094328834\n",
      "train loss:0.04410293947944081\n",
      "train loss:0.021436507093963733\n",
      "train loss:0.016274992428707176\n",
      "train loss:0.0412545942510024\n",
      "train loss:0.011064874532012737\n",
      "train loss:0.0129818846696102\n",
      "train loss:0.04024559808445128\n",
      "train loss:0.015490056893983372\n",
      "train loss:0.08914405409368131\n",
      "train loss:0.009545691882413047\n",
      "train loss:0.04179856522296716\n",
      "train loss:0.02615956281337303\n",
      "train loss:0.02291893161945646\n",
      "train loss:0.0872057603792091\n",
      "train loss:0.03854796886769701\n",
      "train loss:0.025430125884637778\n",
      "train loss:0.014989078814698848\n",
      "train loss:0.06968423516027125\n",
      "train loss:0.04581642684015916\n",
      "train loss:0.0448686302970675\n",
      "train loss:0.041305982259495125\n",
      "train loss:0.022682880289378322\n",
      "train loss:0.026726338266398315\n",
      "train loss:0.04226951814423846\n",
      "train loss:0.0151018355446359\n",
      "train loss:0.007891551464895138\n",
      "train loss:0.009186517101321814\n",
      "train loss:0.027224391662137814\n",
      "train loss:0.01696298854630406\n",
      "train loss:0.011459093680689087\n",
      "train loss:0.018395768189469155\n",
      "train loss:0.01246902463217136\n",
      "train loss:0.07332128274844758\n",
      "train loss:0.02117925141876607\n",
      "train loss:0.007495985442509923\n",
      "train loss:0.022953381828694476\n",
      "train loss:0.017666996569034074\n",
      "train loss:0.008561939571012703\n",
      "train loss:0.010286749099773541\n",
      "train loss:0.01089258733613949\n",
      "train loss:0.019558407908649163\n",
      "train loss:0.032709439379971034\n",
      "train loss:0.030713110227365895\n",
      "train loss:0.018542136814152135\n",
      "train loss:0.023936121385100553\n",
      "train loss:0.02952454482819949\n",
      "train loss:0.08659877911652603\n",
      "train loss:0.03432331775703162\n",
      "train loss:0.03220831068379912\n",
      "train loss:0.0032862334370268764\n",
      "train loss:0.010884922401545078\n",
      "train loss:0.020896390015270227\n",
      "train loss:0.04732089811278268\n",
      "train loss:0.03849206878801634\n",
      "train loss:0.030501600950984215\n",
      "train loss:0.002716674237790966\n",
      "train loss:0.024593382402445546\n",
      "train loss:0.042400551031188984\n",
      "train loss:0.06052610159248017\n",
      "train loss:0.013787212916424828\n",
      "train loss:0.04241427515535395\n",
      "train loss:0.012379468780124494\n",
      "train loss:0.09755892347474608\n",
      "train loss:0.028186615878155245\n",
      "train loss:0.00965324528340435\n",
      "train loss:0.0117618060475093\n",
      "train loss:0.02413174345859882\n",
      "train loss:0.017005154084995703\n",
      "train loss:0.01615153668013453\n",
      "train loss:0.02250019565719273\n",
      "train loss:0.029754303271959003\n",
      "train loss:0.017558562476336925\n",
      "train loss:0.009520027436852325\n",
      "train loss:0.0007105809307633551\n",
      "train loss:0.025818089917947883\n",
      "train loss:0.013156127146045596\n",
      "train loss:0.031174675062041236\n",
      "train loss:0.017625161931854115\n",
      "train loss:0.021417981760638704\n",
      "train loss:0.03375268383527661\n",
      "train loss:0.06805354373815699\n",
      "train loss:0.029778907909416442\n",
      "train loss:0.008078634604146163\n",
      "train loss:0.011480517053421645\n",
      "train loss:0.018295780878024772\n",
      "train loss:0.040568112966555804\n",
      "train loss:0.023753604986112266\n",
      "train loss:0.02875592145335894\n",
      "train loss:0.03970522539973137\n",
      "train loss:0.016401293120105558\n",
      "train loss:0.016998156030553033\n",
      "train loss:0.019325707597959752\n",
      "train loss:0.006135738297540553\n",
      "train loss:0.018395720037820747\n",
      "train loss:0.05730919606663178\n",
      "train loss:0.020742834372684094\n",
      "train loss:0.02639500473029672\n",
      "train loss:0.01816472190469275\n",
      "train loss:0.00728827234533959\n",
      "train loss:0.08230740438293425\n",
      "train loss:0.024301225512328762\n",
      "train loss:0.00506254188528996\n",
      "train loss:0.15042964826777022\n",
      "train loss:0.007146702375214106\n",
      "train loss:0.01697674885525151\n",
      "train loss:0.004962585669060282\n",
      "train loss:0.07401353471662808\n",
      "train loss:0.02243212296164101\n",
      "train loss:0.012136980801446954\n",
      "train loss:0.039723538104236206\n",
      "train loss:0.02682773920703574\n",
      "train loss:0.009280422681916705\n",
      "train loss:0.03345765777969534\n",
      "train loss:0.14928188459379502\n",
      "train loss:0.005542552133300095\n",
      "train loss:0.03171000952921086\n",
      "train loss:0.01204201350347464\n",
      "train loss:0.014267207578460984\n",
      "train loss:0.032330595340003286\n",
      "train loss:0.012842198138466283\n",
      "train loss:0.0426371829775941\n",
      "train loss:0.016708129130770997\n",
      "train loss:0.014978351391989943\n",
      "train loss:0.02878477362597752\n",
      "train loss:0.008899604945644053\n",
      "train loss:0.041989198449570865\n",
      "train loss:0.008712799578199835\n",
      "train loss:0.021524865200397127\n",
      "train loss:0.07234756635967315\n",
      "train loss:0.007324928173696058\n",
      "train loss:0.009577910517288898\n",
      "train loss:0.015104358333688282\n",
      "train loss:0.013167328241500392\n",
      "train loss:0.007450473263209114\n",
      "train loss:0.012653847585778462\n",
      "train loss:0.042475240659834344\n",
      "train loss:0.027419540883547314\n",
      "train loss:0.03344079897204273\n",
      "train loss:0.004112050409086014\n",
      "train loss:0.020848987411672383\n",
      "train loss:0.06851362927636193\n",
      "train loss:0.045323660741251014\n",
      "train loss:0.035778859591720835\n",
      "train loss:0.034793436600438946\n",
      "train loss:0.02500609055501997\n",
      "train loss:0.02206625740653148\n",
      "train loss:0.038179466249954094\n",
      "train loss:0.033528092204185095\n",
      "train loss:0.0274992393962689\n",
      "train loss:0.032494416018636646\n",
      "train loss:0.010335757561364716\n",
      "train loss:0.031656724205359366\n",
      "train loss:0.008936306685729898\n",
      "train loss:0.01061328961480428\n",
      "train loss:0.014552236347631026\n",
      "train loss:0.024772115866205122\n",
      "train loss:0.0248480214803872\n",
      "train loss:0.030156265123830215\n",
      "train loss:0.09589921665498308\n",
      "train loss:0.034165490263284824\n",
      "train loss:0.007714066539740953\n",
      "train loss:0.03137081121759798\n",
      "train loss:0.012970659628528605\n",
      "train loss:0.02499360544468576\n",
      "train loss:0.07884346361221656\n",
      "train loss:0.021576988470526137\n",
      "train loss:0.011538407582838507\n",
      "train loss:0.038671250249968876\n",
      "train loss:0.02673667766008554\n",
      "train loss:0.04375356831912822\n",
      "train loss:0.008927924346615719\n",
      "train loss:0.02576128118410172\n",
      "train loss:0.008138915371605488\n",
      "train loss:0.048289054545992\n",
      "train loss:0.006615812011340756\n",
      "train loss:0.03331076047492807\n",
      "train loss:0.050965255198072545\n",
      "train loss:0.00719709635319268\n",
      "train loss:0.009699708532250691\n",
      "train loss:0.00607024959642972\n",
      "train loss:0.06313270592608762\n",
      "train loss:0.004504469642754104\n",
      "train loss:0.056441081257276746\n",
      "train loss:0.04847486232828947\n",
      "train loss:0.008713648423513642\n",
      "train loss:0.019553661144636893\n",
      "train loss:0.050800438441286164\n",
      "train loss:0.015457875203649685\n",
      "train loss:0.01314759628676859\n",
      "train loss:0.037096484679361004\n",
      "train loss:0.019686053655407453\n",
      "train loss:0.07427803219168284\n",
      "train loss:0.008731434275497572\n",
      "train loss:0.02600310686010981\n",
      "train loss:0.012166053516089972\n",
      "train loss:0.00744575702570179\n",
      "train loss:0.1272059891651408\n",
      "train loss:0.008485983422133499\n",
      "train loss:0.004886779691746098\n",
      "train loss:0.026307332555694146\n",
      "train loss:0.01563113546881708\n",
      "train loss:0.06299452448759685\n",
      "train loss:0.041892802447068285\n",
      "train loss:0.04987154691243254\n",
      "train loss:0.02064113222400691\n",
      "train loss:0.02442183773040639\n",
      "train loss:0.01377094846530375\n",
      "train loss:0.09035225413045188\n",
      "train loss:0.006615961517206004\n",
      "train loss:0.01899506254651521\n",
      "train loss:0.00486669655741288\n",
      "train loss:0.08583173252537919\n",
      "train loss:0.023818407647506604\n",
      "train loss:0.032626786531225256\n",
      "train loss:0.010009354898143814\n",
      "train loss:0.006173891066675057\n",
      "train loss:0.0037611370607055533\n",
      "train loss:0.0461581872325531\n",
      "train loss:0.00451702512987023\n",
      "train loss:0.01822653675726779\n",
      "train loss:0.017429288885782327\n",
      "train loss:0.02204100144543004\n",
      "train loss:0.023030105613834326\n",
      "train loss:0.009492391524148472\n",
      "train loss:0.011795606334171092\n",
      "train loss:0.009255685891029456\n",
      "train loss:0.04390931400184291\n",
      "train loss:0.0643914830564046\n",
      "train loss:0.05147966819926764\n",
      "train loss:0.027272159730287102\n",
      "train loss:0.025036079367808814\n",
      "train loss:0.005060708023730574\n",
      "train loss:0.014544872144990631\n",
      "train loss:0.027376991723203608\n",
      "train loss:0.02431429443258372\n",
      "train loss:0.015141446447599613\n",
      "train loss:0.01866456879189649\n",
      "train loss:0.02917253072935355\n",
      "train loss:0.015143344857179341\n",
      "train loss:0.07383292556673585\n",
      "train loss:0.014012122141148074\n",
      "train loss:0.004194398451528373\n",
      "train loss:0.011743473618339295\n",
      "train loss:0.027636309046318273\n",
      "train loss:0.0026596771430941165\n",
      "train loss:0.004157022497652168\n",
      "train loss:0.027401621661674386\n",
      "train loss:0.012478201702050993\n",
      "train loss:0.0323037658703931\n",
      "train loss:0.05321212185750778\n",
      "train loss:0.0980120266145085\n",
      "train loss:0.002728872306761314\n",
      "train loss:0.006958079289677705\n",
      "train loss:0.03084502942041454\n",
      "train loss:0.04004267189355163\n",
      "train loss:0.03697252190182215\n",
      "train loss:0.052494243519550635\n",
      "train loss:0.006986560282647768\n",
      "train loss:0.012860158741973009\n",
      "train loss:0.006708269861793656\n",
      "train loss:0.029763423925860847\n",
      "train loss:0.008997458568442892\n",
      "train loss:0.03093054054008433\n",
      "train loss:0.01298137363572127\n",
      "train loss:0.007911622215732843\n",
      "train loss:0.014947802738171185\n",
      "train loss:0.02332344533434286\n",
      "train loss:0.07636365301303344\n",
      "train loss:0.012699875692254965\n",
      "train loss:0.01849495078023183\n",
      "train loss:0.01766759185591947\n",
      "train loss:0.006855374936694126\n",
      "train loss:0.05467945730506219\n",
      "train loss:0.06536420415865495\n",
      "train loss:0.04157943669085926\n",
      "train loss:0.008566789761850328\n",
      "train loss:0.02431389148167334\n",
      "train loss:0.004332467940948044\n",
      "train loss:0.023351332596643477\n",
      "train loss:0.05105997914094305\n",
      "train loss:0.012192388042238475\n",
      "train loss:0.017325379707247096\n",
      "train loss:0.015466215697400055\n",
      "train loss:0.007183726681800753\n",
      "train loss:0.004694055477148116\n",
      "train loss:0.03854691629073354\n",
      "train loss:0.028099786340378078\n",
      "train loss:0.011172910119545259\n",
      "train loss:0.01932940937468404\n",
      "train loss:0.011429324545756214\n",
      "train loss:0.038802943688435336\n",
      "train loss:0.06582998576921167\n",
      "train loss:0.009244616733950406\n",
      "train loss:0.004324493010142779\n",
      "train loss:0.037236468235809256\n",
      "train loss:0.0013169728654458058\n",
      "train loss:0.011121216901404597\n",
      "train loss:0.027871219360464994\n",
      "train loss:0.023230998006066504\n",
      "train loss:0.029094410965629657\n",
      "train loss:0.005952146110181301\n",
      "train loss:0.008496996330981187\n",
      "train loss:0.015846266161588324\n",
      "train loss:0.039878278258600824\n",
      "train loss:0.005845449635413656\n",
      "train loss:0.029017345396042424\n",
      "train loss:0.02129416812342464\n",
      "train loss:0.02264488404500739\n",
      "train loss:0.006403223327697052\n",
      "train loss:0.025378288923282612\n",
      "train loss:0.009478472332404609\n",
      "train loss:0.0846391691206686\n",
      "train loss:0.05581059742544868\n",
      "train loss:0.018563167058200606\n",
      "train loss:0.005110701751926181\n",
      "train loss:0.005753173177925905\n",
      "train loss:0.008466128241541691\n",
      "train loss:0.017280779923273187\n",
      "train loss:0.012749705445336146\n",
      "train loss:0.023722339810367688\n",
      "train loss:0.009915307683242646\n",
      "train loss:0.03294792822422222\n",
      "train loss:0.021177778531419094\n",
      "train loss:0.0312895731894318\n",
      "train loss:0.017234843686820985\n",
      "train loss:0.0428285599247637\n",
      "train loss:0.01920721076093516\n",
      "train loss:0.08084122102686879\n",
      "train loss:0.040259669895583994\n",
      "train loss:0.007907347883248202\n",
      "train loss:0.010610594721256701\n",
      "train loss:0.020717281670422815\n",
      "train loss:0.011250697130133758\n",
      "train loss:0.01129299580362225\n",
      "train loss:0.0056339063182479585\n",
      "train loss:0.005321832184990941\n",
      "train loss:0.006144946794370664\n",
      "train loss:0.02358847127818518\n",
      "train loss:0.0025708979399379998\n",
      "train loss:0.02541799904270153\n",
      "train loss:0.0284238800474159\n",
      "train loss:0.01859438395653948\n",
      "train loss:0.040359009819589675\n",
      "train loss:0.008326071880171527\n",
      "train loss:0.004576065208298619\n",
      "train loss:0.0174582168606852\n",
      "train loss:0.015426859475522067\n",
      "train loss:0.014406072619333748\n",
      "train loss:0.01901190678670373\n",
      "train loss:0.007846631804070407\n",
      "train loss:0.02492078351908741\n",
      "train loss:0.0290674521869289\n",
      "train loss:0.03415313929018057\n",
      "train loss:0.027310066729774693\n",
      "train loss:0.01527797795430929\n",
      "train loss:0.007335712001400887\n",
      "train loss:0.009440087214647876\n",
      "train loss:0.034519469046415764\n",
      "train loss:0.0019710015462248483\n",
      "train loss:0.028158154548541106\n",
      "train loss:0.018048136149584337\n",
      "train loss:0.004914430979084457\n",
      "train loss:0.004313847047543436\n",
      "train loss:0.024763351808696057\n",
      "train loss:0.018812005404539076\n",
      "train loss:0.03966851534867533\n",
      "train loss:0.023430320422931344\n",
      "train loss:0.02063157788936114\n",
      "train loss:0.006193396366600318\n",
      "train loss:0.018343861020446096\n",
      "train loss:0.013037000701228587\n",
      "train loss:0.013592618935648082\n",
      "train loss:0.01919945511248543\n",
      "train loss:0.01253782044826921\n",
      "train loss:0.012482110830375233\n",
      "train loss:0.006385788775904306\n",
      "train loss:0.021222914416819116\n",
      "train loss:0.009172181831141277\n",
      "train loss:0.02303236939920293\n",
      "train loss:0.09429135730621148\n",
      "train loss:0.025877576538072482\n",
      "train loss:0.010116950673777445\n",
      "train loss:0.05033913866759655\n",
      "train loss:0.020532497530757584\n",
      "train loss:0.016436145357807557\n",
      "train loss:0.009234084584035008\n",
      "train loss:0.03885711427801252\n",
      "train loss:0.0335594134081319\n",
      "train loss:0.04406398849004752\n",
      "train loss:0.018651131885527243\n",
      "train loss:0.00613075703463143\n",
      "train loss:0.008764332750378108\n",
      "train loss:0.007543829656754145\n",
      "=== epoch:6, train acc:0.989, test acc:0.989 ===\n",
      "train loss:0.019636906762822162\n",
      "train loss:0.008871302593423\n",
      "train loss:0.02058253776322779\n",
      "train loss:0.06588051214701003\n",
      "train loss:0.0350614587645493\n",
      "train loss:0.013463684748496298\n",
      "train loss:0.00868830805924854\n",
      "train loss:0.056664139568311095\n",
      "train loss:0.011872092411615423\n",
      "train loss:0.01729075036167569\n",
      "train loss:0.019555955698031532\n",
      "train loss:0.01599007613822107\n",
      "train loss:0.002722286622994249\n",
      "train loss:0.006928834787326235\n",
      "train loss:0.004352492143256049\n",
      "train loss:0.01621982814853388\n",
      "train loss:0.023759755823960046\n",
      "train loss:0.03264145228572215\n",
      "train loss:0.021175778310974887\n",
      "train loss:0.06471642269749192\n",
      "train loss:0.005274771311920264\n",
      "train loss:0.004342651275383467\n",
      "train loss:0.047747352809425415\n",
      "train loss:0.031497792554862444\n",
      "train loss:0.018220546320897427\n",
      "train loss:0.02175063937881148\n",
      "train loss:0.016661492751955432\n",
      "train loss:0.004026612315149937\n",
      "train loss:0.015160606562381933\n",
      "train loss:0.05055845546069948\n",
      "train loss:0.06543191820272186\n",
      "train loss:0.006471463597304479\n",
      "train loss:0.005242093247479167\n",
      "train loss:0.01692454020935287\n",
      "train loss:0.048085187932081025\n",
      "train loss:0.004160857977715916\n",
      "train loss:0.01962467739390322\n",
      "train loss:0.03383508311308279\n",
      "train loss:0.006238191714701337\n",
      "train loss:0.011119855698571785\n",
      "train loss:0.028232826661586587\n",
      "train loss:0.01098125288003818\n",
      "train loss:0.05356586297487708\n",
      "train loss:0.030757245733756156\n",
      "train loss:0.01705424709100728\n",
      "train loss:0.0059283689899433\n",
      "train loss:0.009200579718976403\n",
      "train loss:0.04745105976856151\n",
      "train loss:0.04508350815977577\n",
      "train loss:0.02382357007946565\n",
      "train loss:0.007378987631543445\n",
      "train loss:0.051016924245041714\n",
      "train loss:0.008239689417340983\n",
      "train loss:0.005228354561338069\n",
      "train loss:0.01974977636984704\n",
      "train loss:0.038321080766091836\n",
      "train loss:0.010260921844738518\n",
      "train loss:0.039483494197656406\n",
      "train loss:0.006017813699720289\n",
      "train loss:0.005229004978185886\n",
      "train loss:0.012069663549472252\n",
      "train loss:0.00867483898962234\n",
      "train loss:0.058591628671519506\n",
      "train loss:0.03179087289848931\n",
      "train loss:0.012706683988490406\n",
      "train loss:0.018997817852043353\n",
      "train loss:0.0018246495326321732\n",
      "train loss:0.07297668638821996\n",
      "train loss:0.006383901730999148\n",
      "train loss:0.022542768258517802\n",
      "train loss:0.010418635914983577\n",
      "train loss:0.010761957099660446\n",
      "train loss:0.02486510513123086\n",
      "train loss:0.011913727437801185\n",
      "train loss:0.08641362526084338\n",
      "train loss:0.03953029211625534\n",
      "train loss:0.04800243304829764\n",
      "train loss:0.021146248259095713\n",
      "train loss:0.01935372485469648\n",
      "train loss:0.009698670516346809\n",
      "train loss:0.022367655668991827\n",
      "train loss:0.02205172212441831\n",
      "train loss:0.009019024798520925\n",
      "train loss:0.006946658195635305\n",
      "train loss:0.010527891507949149\n",
      "train loss:0.0188879581733195\n",
      "train loss:0.05804349355479754\n",
      "train loss:0.01113254492199777\n",
      "train loss:0.00840568352854486\n",
      "train loss:0.005172392033777038\n",
      "train loss:0.006501686562715757\n",
      "train loss:0.029784695320194645\n",
      "train loss:0.018440486432157673\n",
      "train loss:0.009911059863775563\n",
      "train loss:0.05409324355831874\n",
      "train loss:0.047029972026482056\n",
      "train loss:0.016104364631918076\n",
      "train loss:0.010179375001479146\n",
      "train loss:0.0062773995183254\n",
      "train loss:0.06707035139114997\n",
      "train loss:0.010180980175699756\n",
      "train loss:0.016386258549664683\n",
      "train loss:0.016388136009614626\n",
      "train loss:0.021285716967283617\n",
      "train loss:0.020046782319754004\n",
      "train loss:0.006788734992268762\n",
      "train loss:0.050137342815904956\n",
      "train loss:0.015681782283242816\n",
      "train loss:0.10797040479290242\n",
      "train loss:0.007454622724256133\n",
      "train loss:0.08454231659295391\n",
      "train loss:0.008240103606988273\n",
      "train loss:0.07439658405091044\n",
      "train loss:0.00705053154816811\n",
      "train loss:0.04720805761054682\n",
      "train loss:0.02493512196419345\n",
      "train loss:0.011545616221510091\n",
      "train loss:0.06156996188650185\n",
      "train loss:0.03433808923272187\n",
      "train loss:0.023489519427790078\n",
      "train loss:0.00573156363858522\n",
      "train loss:0.012976349226691344\n",
      "train loss:0.03293580389812373\n",
      "train loss:0.050174125971830054\n",
      "train loss:0.019357215273284732\n",
      "train loss:0.09278238888912477\n",
      "train loss:0.005603524302827805\n",
      "train loss:0.05117201675102928\n",
      "train loss:0.034421003955043224\n",
      "train loss:0.00603109216999151\n",
      "train loss:0.004245256243181197\n",
      "train loss:0.0313321133307136\n",
      "train loss:0.014430248176826524\n",
      "train loss:0.02546567598247335\n",
      "train loss:0.016167190797508453\n",
      "train loss:0.05173997513932212\n",
      "train loss:0.017932912619009932\n",
      "train loss:0.021162305626761836\n",
      "train loss:0.059109315119292904\n",
      "train loss:0.009866261753056901\n",
      "train loss:0.07117468197283397\n",
      "train loss:0.01404861062845133\n",
      "train loss:0.008298437533685756\n",
      "train loss:0.022578220923881825\n",
      "train loss:0.022816717714221238\n",
      "train loss:0.024739726223261163\n",
      "train loss:0.02658002838992701\n",
      "train loss:0.013731546012781745\n",
      "train loss:0.019375119267958384\n",
      "train loss:0.009316885749756363\n",
      "train loss:0.005374380753093414\n",
      "train loss:0.018580470407370667\n",
      "train loss:0.010268793720683244\n",
      "train loss:0.007553901404018138\n",
      "train loss:0.019682595067432224\n",
      "train loss:0.01846430537921218\n",
      "train loss:0.008864388094383054\n",
      "train loss:0.05899306117341208\n",
      "train loss:0.05771787450155737\n",
      "train loss:0.023184458952155897\n",
      "train loss:0.07123060189617982\n",
      "train loss:0.06329818996856176\n",
      "train loss:0.039352118401825266\n",
      "train loss:0.013663150581108825\n",
      "train loss:0.009489335933454383\n",
      "train loss:0.010243569530763304\n",
      "train loss:0.006566092055960613\n",
      "train loss:0.012846215310344489\n",
      "train loss:0.020670845367001468\n",
      "train loss:0.006572232040721753\n",
      "train loss:0.023554873777101526\n",
      "train loss:0.007001908915472528\n",
      "train loss:0.05932115614260024\n",
      "train loss:0.06390858587693171\n",
      "train loss:0.034505068985765344\n",
      "train loss:0.025978530962619553\n",
      "train loss:0.028909978238268665\n",
      "train loss:0.011900567351334159\n",
      "train loss:0.028527445784912852\n",
      "train loss:0.022969551258294\n",
      "train loss:0.0364456847245425\n",
      "train loss:0.010244711374966988\n",
      "train loss:0.01572287059081374\n",
      "train loss:0.03374707965826734\n",
      "train loss:0.007071086953436272\n",
      "train loss:0.02444746354010831\n",
      "train loss:0.009442596551298447\n",
      "train loss:0.10558925380641987\n",
      "train loss:0.03723663352836915\n",
      "train loss:0.016268284794873574\n",
      "train loss:0.004439504730178361\n",
      "train loss:0.01548645176928128\n",
      "train loss:0.04644355043909185\n",
      "train loss:0.009179408550232245\n",
      "train loss:0.03142278070939127\n",
      "train loss:0.009235722134789667\n",
      "train loss:0.005421179546302275\n",
      "train loss:0.018087546264157015\n",
      "train loss:0.008128768678250775\n",
      "train loss:0.01703336064995761\n",
      "train loss:0.01338084127306925\n",
      "train loss:0.012612496263213799\n",
      "train loss:0.047435387364197695\n",
      "train loss:0.03558419642727845\n",
      "train loss:0.02859730440135585\n",
      "train loss:0.02681641649202583\n",
      "train loss:0.010530183082023328\n",
      "train loss:0.03954441715608961\n",
      "train loss:0.010637187004992181\n",
      "train loss:0.05720007722943008\n",
      "train loss:0.010468542457032411\n",
      "train loss:0.012871469905726837\n",
      "train loss:0.01937030926228716\n",
      "train loss:0.00715964998088202\n",
      "train loss:0.02397982283652395\n",
      "train loss:0.008894739283702071\n",
      "train loss:0.01482421030866325\n",
      "train loss:0.017692216939415573\n",
      "train loss:0.024353849023960678\n",
      "train loss:0.056842811552332846\n",
      "train loss:0.046731423987884\n",
      "train loss:0.006422090546191247\n",
      "train loss:0.02722461388246391\n",
      "train loss:0.011611127929578397\n",
      "train loss:0.034327804702309214\n",
      "train loss:0.025862109709914684\n",
      "train loss:0.014512291032143505\n",
      "train loss:0.021662142730027038\n",
      "train loss:0.00900185757313348\n",
      "train loss:0.018195589887430287\n",
      "train loss:0.012719184007516204\n",
      "train loss:0.033576315349943105\n",
      "train loss:0.013152665623565856\n",
      "train loss:0.021910132996500565\n",
      "train loss:0.00347129176479828\n",
      "train loss:0.022043485392955198\n",
      "train loss:0.010451814166419122\n",
      "train loss:0.007841932199637702\n",
      "train loss:0.029047538350617554\n",
      "train loss:0.014215154050823877\n",
      "train loss:0.013093432261373033\n",
      "train loss:0.0017439634169384049\n",
      "train loss:0.02649365681756249\n",
      "train loss:0.033462862781794556\n",
      "train loss:0.026353083772395737\n",
      "train loss:0.015518297675287531\n",
      "train loss:0.014878564692876198\n",
      "train loss:0.01976307299376215\n",
      "train loss:0.03451083046172593\n",
      "train loss:0.008465317989119758\n",
      "train loss:0.00871580713496892\n",
      "train loss:0.08659353651368497\n",
      "train loss:0.03372379017541133\n",
      "train loss:0.004550071524199107\n",
      "train loss:0.044770929507710874\n",
      "train loss:0.011393442262869417\n",
      "train loss:0.012108580351998799\n",
      "train loss:0.004554201654079046\n",
      "train loss:0.013445810578939845\n",
      "train loss:0.09026281657587878\n",
      "train loss:0.04918923405215892\n",
      "train loss:0.02269250626453616\n",
      "train loss:0.006539548445401832\n",
      "train loss:0.04571535732632186\n",
      "train loss:0.012004039531172848\n",
      "train loss:0.005585996059019588\n",
      "train loss:0.024284440113516128\n",
      "train loss:0.004446334767801599\n",
      "train loss:0.022480960478664654\n",
      "train loss:0.004436419987090965\n",
      "train loss:0.0029658227341735357\n",
      "train loss:0.027955814673556012\n",
      "train loss:0.01718604852248837\n",
      "train loss:0.03326281743923315\n",
      "train loss:0.03789971192602191\n",
      "train loss:0.014792776131501957\n",
      "train loss:0.005859014981432329\n",
      "train loss:0.017711160716516582\n",
      "train loss:0.00888219003922183\n",
      "train loss:0.020642074521055934\n",
      "train loss:0.017608903326602348\n",
      "train loss:0.013826864445119881\n",
      "train loss:0.017514964033581777\n",
      "train loss:0.03836574459380196\n",
      "train loss:0.005419190507805574\n",
      "train loss:0.03149433832295048\n",
      "train loss:0.005039092903659805\n",
      "train loss:0.010613691691870047\n",
      "train loss:0.03266599497826936\n",
      "train loss:0.022156596515535337\n",
      "train loss:0.036662882275466\n",
      "train loss:0.035771973459046195\n",
      "train loss:0.12927704481899718\n",
      "train loss:0.016663523998718836\n",
      "train loss:0.02708016284109144\n",
      "train loss:0.015094242111360965\n",
      "train loss:0.01607835627427802\n",
      "train loss:0.06438667614611633\n",
      "train loss:0.013631476411790357\n",
      "train loss:0.026809481198377057\n",
      "train loss:0.020014041327344797\n",
      "train loss:0.038613430415527984\n",
      "train loss:0.006709228412878608\n",
      "train loss:0.08476681389004975\n",
      "train loss:0.0049165264406516845\n",
      "train loss:0.0049030082476688545\n",
      "train loss:0.02767729759416663\n",
      "train loss:0.0061282092149576904\n",
      "train loss:0.024446776235492518\n",
      "train loss:0.033258621905661416\n",
      "train loss:0.02656363713856643\n",
      "train loss:0.021511297496147913\n",
      "train loss:0.08916685520974392\n",
      "train loss:0.021508066417913224\n",
      "train loss:0.036603046012503124\n",
      "train loss:0.007851147307334529\n",
      "train loss:0.027458787709829166\n",
      "train loss:0.03251796620557524\n",
      "train loss:0.03566139873571259\n",
      "train loss:0.010915904225277783\n",
      "train loss:0.011568071339478155\n",
      "train loss:0.017418995140359005\n",
      "train loss:0.019609847146163192\n",
      "train loss:0.003116331929003765\n",
      "train loss:0.02244106238315432\n",
      "train loss:0.01046770067821968\n",
      "train loss:0.006341177318089075\n",
      "train loss:0.00660739653776735\n",
      "train loss:0.035784712610124864\n",
      "train loss:0.010917101546008934\n",
      "train loss:0.03280369187747007\n",
      "train loss:0.04420859648803327\n",
      "train loss:0.048821960965566076\n",
      "train loss:0.014835780961353306\n",
      "train loss:0.007541043775900461\n",
      "train loss:0.0233915774345186\n",
      "train loss:0.03765005840940487\n",
      "train loss:0.007809899927195719\n",
      "train loss:0.01732188953354773\n",
      "train loss:0.025690029245314758\n",
      "train loss:0.002776639780893126\n",
      "train loss:0.014108425311855195\n",
      "train loss:0.013243027921160359\n",
      "train loss:0.013793135459422687\n",
      "train loss:0.04295436704223572\n",
      "train loss:0.022060327481542\n",
      "train loss:0.040410889495384496\n",
      "train loss:0.06960705035018909\n",
      "train loss:0.002424203414946764\n",
      "train loss:0.005749943804241428\n",
      "train loss:0.026158110449201202\n",
      "train loss:0.03474008353995508\n",
      "train loss:0.01137230533768692\n",
      "train loss:0.005881329670707251\n",
      "train loss:0.01747239560905411\n",
      "train loss:0.013449087834942633\n",
      "train loss:0.05094850211571845\n",
      "train loss:0.027626757792081666\n",
      "train loss:0.015627595454782837\n",
      "train loss:0.015661292407976153\n",
      "train loss:0.03865713218329541\n",
      "train loss:0.05880453163164767\n",
      "train loss:0.039311521162692147\n",
      "train loss:0.006684994344421959\n",
      "train loss:0.005879664858406814\n",
      "train loss:0.014914281727566835\n",
      "train loss:0.00756535015000935\n",
      "train loss:0.053988919898845696\n",
      "train loss:0.015451608290926953\n",
      "train loss:0.009038102151134361\n",
      "train loss:0.01158074907241214\n",
      "train loss:0.033959085917490875\n",
      "train loss:0.005349459418091916\n",
      "train loss:0.018884068766882433\n",
      "train loss:0.006187500259773813\n",
      "train loss:0.013771824945348\n",
      "train loss:0.014769065076280181\n",
      "train loss:0.04570316604125095\n",
      "train loss:0.015458294689735171\n",
      "train loss:0.0072739578906576485\n",
      "train loss:0.003874588495190791\n",
      "train loss:0.06639288338427944\n",
      "train loss:0.013175145268900195\n",
      "train loss:0.00763601899642521\n",
      "train loss:0.018930217334993792\n",
      "train loss:0.03662748879698613\n",
      "train loss:0.005448011983924469\n",
      "train loss:0.10644824139878337\n",
      "train loss:0.01830681262427894\n",
      "train loss:0.01780432700461847\n",
      "train loss:0.02881380588303761\n",
      "train loss:0.039431926148440655\n",
      "train loss:0.008440881167867653\n",
      "train loss:0.0432677613882134\n",
      "train loss:0.017432635573818682\n",
      "train loss:0.014164377213871293\n",
      "train loss:0.06591254622449216\n",
      "train loss:0.009409899418072546\n",
      "train loss:0.0038742271507400273\n",
      "train loss:0.023592341326505815\n",
      "train loss:0.0640579982930466\n",
      "train loss:0.015850399828469596\n",
      "train loss:0.024750378767842723\n",
      "train loss:0.006119485160045881\n",
      "train loss:0.026865537040306683\n",
      "train loss:0.0039438477661721885\n",
      "train loss:0.018010034832800868\n",
      "train loss:0.03933849542029484\n",
      "train loss:0.057209894415790954\n",
      "train loss:0.019106560464295062\n",
      "train loss:0.013895791830202377\n",
      "train loss:0.04038127529820199\n",
      "train loss:0.020780117584287466\n",
      "train loss:0.01517490460302837\n",
      "train loss:0.005964118623544341\n",
      "train loss:0.005635861778337104\n",
      "train loss:0.020876611502828178\n",
      "train loss:0.008215892431244065\n",
      "train loss:0.010338736772583665\n",
      "train loss:0.030664681461137874\n",
      "train loss:0.010025580328940999\n",
      "train loss:0.026773198562137016\n",
      "train loss:0.014926495887663718\n",
      "train loss:0.00517899995372026\n",
      "train loss:0.013535398115959673\n",
      "train loss:0.00402144315484842\n",
      "train loss:0.018648866542223922\n",
      "train loss:0.015121936569203253\n",
      "train loss:0.05189603648268238\n",
      "train loss:0.015365663461276418\n",
      "train loss:0.07596571300575979\n",
      "train loss:0.01674317462972175\n",
      "train loss:0.011888537001568283\n",
      "train loss:0.014077539116782752\n",
      "train loss:0.020234564374594043\n",
      "train loss:0.031442252545773874\n",
      "train loss:0.02274259090339676\n",
      "train loss:0.009217089092235408\n",
      "train loss:0.016386178948553255\n",
      "train loss:0.002954939784423189\n",
      "train loss:0.02668686249777896\n",
      "train loss:0.013120212409103804\n",
      "train loss:0.03097834922183523\n",
      "train loss:0.029122540309089326\n",
      "train loss:0.03492399680809911\n",
      "train loss:0.003120133526112997\n",
      "train loss:0.011500596126634345\n",
      "train loss:0.011024413085995374\n",
      "train loss:0.010657590407647714\n",
      "train loss:0.004690382867778557\n",
      "train loss:0.07734188760338688\n",
      "train loss:0.01606863343128588\n",
      "train loss:0.04228961975261592\n",
      "train loss:0.01756155869670561\n",
      "train loss:0.03646562312460015\n",
      "train loss:0.03458987378270513\n",
      "train loss:0.011951901969350412\n",
      "train loss:0.022000144950013047\n",
      "train loss:0.012065114744939235\n",
      "train loss:0.02626446963096202\n",
      "train loss:0.029506635892136077\n",
      "train loss:0.02911912266780188\n",
      "train loss:0.05217872647701486\n",
      "train loss:0.013610670705579366\n",
      "train loss:0.012155747752264303\n",
      "train loss:0.02067337135074656\n",
      "train loss:0.011320117154205413\n",
      "train loss:0.010757498703341568\n",
      "train loss:0.03411115494076367\n",
      "train loss:0.010077541018952639\n",
      "train loss:0.002106215838246309\n",
      "train loss:0.04665967442178601\n",
      "train loss:0.08640997942072705\n",
      "train loss:0.009111834192398959\n",
      "train loss:0.02000384840628768\n",
      "train loss:0.031438030941620704\n",
      "train loss:0.061632352265290555\n",
      "train loss:0.047361082609293455\n",
      "train loss:0.03784598755790665\n",
      "train loss:0.021732511535607468\n",
      "train loss:0.07197119607615517\n",
      "train loss:0.032845065322356336\n",
      "train loss:0.04531255508999914\n",
      "train loss:0.02060170896192011\n",
      "train loss:0.018539772592896804\n",
      "train loss:0.019057747678135766\n",
      "train loss:0.056419509466377187\n",
      "train loss:0.02243985257402237\n",
      "train loss:0.02633161652883769\n",
      "train loss:0.06892063107089566\n",
      "train loss:0.06499367318024492\n",
      "train loss:0.010062830690855475\n",
      "train loss:0.06612038176808671\n",
      "train loss:0.012915738890777035\n",
      "train loss:0.009480645458896127\n",
      "train loss:0.03698938190486956\n",
      "train loss:0.03662433317439055\n",
      "train loss:0.01768136140920952\n",
      "train loss:0.14106004796464935\n",
      "train loss:0.010637499065851567\n",
      "train loss:0.0166926467260205\n",
      "train loss:0.01871601767666268\n",
      "train loss:0.005666152664119402\n",
      "train loss:0.021677948421865118\n",
      "train loss:0.01818202214907128\n",
      "train loss:0.016234228924891812\n",
      "train loss:0.01490601706986473\n",
      "train loss:0.009752691275311058\n",
      "train loss:0.005450468131283071\n",
      "train loss:0.018741775306676957\n",
      "train loss:0.025896665775116824\n",
      "train loss:0.030617653641654804\n",
      "train loss:0.034643635174894966\n",
      "train loss:0.006948571892375133\n",
      "train loss:0.010338138112803576\n",
      "train loss:0.03554293859454677\n",
      "train loss:0.03698448296241323\n",
      "train loss:0.0397655725561309\n",
      "train loss:0.011921937145201828\n",
      "train loss:0.00817070006280319\n",
      "train loss:0.04443726942618154\n",
      "train loss:0.011743543248816457\n",
      "train loss:0.11834372241986206\n",
      "train loss:0.06128552834187003\n",
      "train loss:0.006925434074562703\n",
      "train loss:0.009917301838417514\n",
      "train loss:0.05151828632654097\n",
      "train loss:0.0021169274205586996\n",
      "train loss:0.001738584835800994\n",
      "train loss:0.011724443092649639\n",
      "train loss:0.018355657229370718\n",
      "train loss:0.021462715966071525\n",
      "train loss:0.024743740039200462\n",
      "train loss:0.024951461993229857\n",
      "train loss:0.030718948531757998\n",
      "train loss:0.044607805126738356\n",
      "train loss:0.012733544174681302\n",
      "train loss:0.02763835649512038\n",
      "train loss:0.0057903305410699525\n",
      "train loss:0.02272409300909832\n",
      "train loss:0.005956090094439263\n",
      "train loss:0.023454817480779187\n",
      "train loss:0.007740689054392113\n",
      "train loss:0.014161840847254015\n",
      "train loss:0.0212101386352644\n",
      "train loss:0.023623647487913232\n",
      "train loss:0.010250496244150135\n",
      "train loss:0.0480215505231429\n",
      "train loss:0.02133990011755008\n",
      "train loss:0.015829527837728143\n",
      "train loss:0.008951321998227438\n",
      "train loss:0.012594733148029396\n",
      "train loss:0.04366159941051699\n",
      "train loss:0.026318702652795328\n",
      "train loss:0.03762474622723946\n",
      "train loss:0.04489284972576346\n",
      "train loss:0.03557698045967442\n",
      "train loss:0.008230055245657958\n",
      "train loss:0.05302611523836406\n",
      "train loss:0.014241229315533075\n",
      "train loss:0.00448976961791211\n",
      "train loss:0.01799462960214768\n",
      "train loss:0.011560264513450156\n",
      "train loss:0.013228501582745378\n",
      "train loss:0.007933932659278466\n",
      "train loss:0.009705331977895715\n",
      "train loss:0.010261138232681443\n",
      "train loss:0.05663388503945265\n",
      "train loss:0.007570668400041097\n",
      "train loss:0.012256296701032551\n",
      "train loss:0.013980642535696504\n",
      "train loss:0.036796764159385216\n",
      "train loss:0.011370334277040966\n",
      "train loss:0.015016151499976047\n",
      "train loss:0.023118669805985315\n",
      "train loss:0.009402153017801437\n",
      "train loss:0.007667639482320168\n",
      "train loss:0.012772955357464574\n",
      "train loss:0.004638567789475412\n",
      "train loss:0.005030684799682497\n",
      "train loss:0.009820304543867257\n",
      "train loss:0.0070211521993603565\n",
      "train loss:0.029488768866311264\n",
      "train loss:0.015779259646989997\n",
      "train loss:0.007462667915035005\n",
      "train loss:0.006444884889846014\n",
      "train loss:0.004422357204915191\n",
      "train loss:0.01270310754437013\n",
      "train loss:0.004765691806453606\n",
      "train loss:0.01362073840042258\n",
      "train loss:0.004248810712029061\n",
      "train loss:0.05227712409135909\n",
      "train loss:0.007824756175306605\n",
      "train loss:0.009086899805974756\n",
      "train loss:0.004555467522586989\n",
      "train loss:0.006560476992830994\n",
      "train loss:0.011011864881814997\n",
      "train loss:0.0028072792119117956\n",
      "train loss:0.014605640841256548\n",
      "train loss:0.005919205594523721\n",
      "=== epoch:7, train acc:0.989, test acc:0.985 ===\n",
      "train loss:0.00989931953421037\n",
      "train loss:0.015025625938182734\n",
      "train loss:0.0356357729371797\n",
      "train loss:0.010505353674350031\n",
      "train loss:0.11415656024843499\n",
      "train loss:0.027960620707692545\n",
      "train loss:0.017345746044259303\n",
      "train loss:0.006241090785307899\n",
      "train loss:0.004396307563419627\n",
      "train loss:0.010635351366227239\n",
      "train loss:0.0014583788627997992\n",
      "train loss:0.0069441882774585765\n",
      "train loss:0.010181647997505284\n",
      "train loss:0.009702074914640977\n",
      "train loss:0.015764757241630335\n",
      "train loss:0.013254625020950938\n",
      "train loss:0.006462573706754675\n",
      "train loss:0.04194766976233175\n",
      "train loss:0.022829140024089218\n",
      "train loss:0.014815207478947117\n",
      "train loss:0.008120571653266757\n",
      "train loss:0.009803347507598792\n",
      "train loss:0.028161723908241607\n",
      "train loss:0.005932527218561191\n",
      "train loss:0.013311475311570222\n",
      "train loss:0.015390371672280767\n",
      "train loss:0.011876359727623218\n",
      "train loss:0.011364556054177683\n",
      "train loss:0.015452166566772386\n",
      "train loss:0.012750254111641839\n",
      "train loss:0.019701984323082525\n",
      "train loss:0.01365320546487269\n",
      "train loss:0.013476962125050508\n",
      "train loss:0.05127332074796263\n",
      "train loss:0.017446518139499118\n",
      "train loss:0.004365373299636219\n",
      "train loss:0.0119552709882496\n",
      "train loss:0.004215702486727573\n",
      "train loss:0.005377723012943083\n",
      "train loss:0.016609659197522546\n",
      "train loss:0.019293628335562644\n",
      "train loss:0.032501509579361924\n",
      "train loss:0.004467291570452076\n",
      "train loss:0.004724142820567248\n",
      "train loss:0.015063695458470876\n",
      "train loss:0.015118225126731223\n",
      "train loss:0.005467363857858015\n",
      "train loss:0.005448085786486558\n",
      "train loss:0.012518211814006757\n",
      "train loss:0.014981445869599354\n",
      "train loss:0.007636102845654747\n",
      "train loss:0.051037278392296714\n",
      "train loss:0.02219572356200169\n",
      "train loss:0.00922330899902542\n",
      "train loss:0.0016234330227044849\n",
      "train loss:0.027826556003065325\n",
      "train loss:0.00438208769636314\n",
      "train loss:0.008874476171863618\n",
      "train loss:0.008300797651767996\n",
      "train loss:0.030049486646263283\n",
      "train loss:0.006762871218838939\n",
      "train loss:0.03428210813902722\n",
      "train loss:0.011024180013610697\n",
      "train loss:0.007495968408698304\n",
      "train loss:0.029383592111632013\n",
      "train loss:0.012077016154130775\n",
      "train loss:0.0672668794482202\n",
      "train loss:0.009728608593284666\n",
      "train loss:0.009270284908025868\n",
      "train loss:0.012911126190030535\n",
      "train loss:0.009506719428283099\n",
      "train loss:0.0025278582932924346\n",
      "train loss:0.0026393062236374997\n",
      "train loss:0.038005324343132724\n",
      "train loss:0.02939367436199358\n",
      "train loss:0.012590246438556689\n",
      "train loss:0.014314742890979015\n",
      "train loss:0.01820814004268935\n",
      "train loss:0.02248201700755349\n",
      "train loss:0.023613166363159067\n",
      "train loss:0.014092455265443681\n",
      "train loss:0.011489286320129547\n",
      "train loss:0.011457120473019554\n",
      "train loss:0.07423919836730491\n",
      "train loss:0.009448876746219735\n",
      "train loss:0.012426893388126654\n",
      "train loss:0.011436203153861015\n",
      "train loss:0.022156919055375125\n",
      "train loss:0.01167181442268082\n",
      "train loss:0.003788141211155631\n",
      "train loss:0.024814221273510975\n",
      "train loss:0.07756258302677263\n",
      "train loss:0.02102719271406636\n",
      "train loss:0.005812075476816132\n",
      "train loss:0.036303785323667424\n",
      "train loss:0.004478767645371278\n",
      "train loss:0.03331620806543818\n",
      "train loss:0.0016538782272663914\n",
      "train loss:0.006588173789587314\n",
      "train loss:0.004221219900899365\n",
      "train loss:0.02125409817535131\n",
      "train loss:0.01533431368788774\n",
      "train loss:0.003940172548545786\n",
      "train loss:0.0057192401350300995\n",
      "train loss:0.04198416386653656\n",
      "train loss:0.0028460086627542527\n",
      "train loss:0.003152312993981018\n",
      "train loss:0.009201841379283461\n",
      "train loss:0.004443435085958421\n",
      "train loss:0.0248192316534647\n",
      "train loss:0.003019912903456063\n",
      "train loss:0.009322113905870571\n",
      "train loss:0.0356592062265089\n",
      "train loss:0.017779906430245435\n",
      "train loss:0.07322003331176011\n",
      "train loss:0.008337347416673888\n",
      "train loss:0.15140633826870115\n",
      "train loss:0.026837459020157963\n",
      "train loss:0.007589311916686556\n",
      "train loss:0.009951063191946668\n",
      "train loss:0.005162811585894421\n",
      "train loss:0.009045494848809298\n",
      "train loss:0.007441810295757083\n",
      "train loss:0.022050112085357242\n",
      "train loss:0.030050080676041976\n",
      "train loss:0.02476664043610423\n",
      "train loss:0.014465563734388382\n",
      "train loss:0.009574293417437313\n",
      "train loss:0.013284315088588993\n",
      "train loss:0.010481178924623094\n",
      "train loss:0.010171191089232073\n",
      "train loss:0.03803750894040384\n",
      "train loss:0.008556026786031012\n",
      "train loss:0.01870923357453502\n",
      "train loss:0.010854005675240322\n",
      "train loss:0.03633730415549155\n",
      "train loss:0.03285082701016686\n",
      "train loss:0.01464900187520006\n",
      "train loss:0.004074541819116513\n",
      "train loss:0.002112145850481974\n",
      "train loss:0.03218260225388441\n",
      "train loss:0.01362554353308304\n",
      "train loss:0.005523416177356593\n",
      "train loss:0.004319336714148487\n",
      "train loss:0.010880329235092105\n",
      "train loss:0.01096110677222187\n",
      "train loss:0.0025914682552870684\n",
      "train loss:0.014612126825016121\n",
      "train loss:0.013720984460372074\n",
      "train loss:0.011261754117651594\n",
      "train loss:0.014889924977989527\n",
      "train loss:0.04001525829951525\n",
      "train loss:0.009801606137643621\n",
      "train loss:0.05223586135841662\n",
      "train loss:0.009154051267087319\n",
      "train loss:0.00451792831413531\n",
      "train loss:0.11540467437373456\n",
      "train loss:0.009576947487355832\n",
      "train loss:0.006450262314737493\n",
      "train loss:0.007027767491569619\n",
      "train loss:0.009114518926728703\n",
      "train loss:0.015290017438569283\n",
      "train loss:0.00326051965323596\n",
      "train loss:0.01649947460841053\n",
      "train loss:0.01888469333051805\n",
      "train loss:0.0431098207227547\n",
      "train loss:0.06832815650505983\n",
      "train loss:0.011087444447356994\n",
      "train loss:0.058696133513048795\n",
      "train loss:0.010969860463862401\n",
      "train loss:0.00825379905231715\n",
      "train loss:0.02814495192494461\n",
      "train loss:0.011873135328753486\n",
      "train loss:0.009437345317209779\n",
      "train loss:0.012292757392127467\n",
      "train loss:0.0038545689794916805\n",
      "train loss:0.03842377979803613\n",
      "train loss:0.003569038775052537\n",
      "train loss:0.012771669199376001\n",
      "train loss:0.004824707367183333\n",
      "train loss:0.00820096646862548\n",
      "train loss:0.0010815626385676\n",
      "train loss:0.009446997496667716\n",
      "train loss:0.023326047468413186\n",
      "train loss:0.0331369965098091\n",
      "train loss:0.008816535010486272\n",
      "train loss:0.0035416814680975794\n",
      "train loss:0.029248598733995778\n",
      "train loss:0.004357053494656787\n",
      "train loss:0.003818874829841493\n",
      "train loss:0.01268091477482969\n",
      "train loss:0.03566120249263002\n",
      "train loss:0.012262576477346708\n",
      "train loss:0.01002593549789741\n",
      "train loss:0.005001533051530468\n",
      "train loss:0.009292460039370058\n",
      "train loss:0.040255976579721506\n",
      "train loss:0.004590763233703775\n",
      "train loss:0.0834960222024767\n",
      "train loss:0.007437390232110466\n",
      "train loss:0.06082548642077604\n",
      "train loss:0.029367573678426816\n",
      "train loss:0.030224108530382456\n",
      "train loss:0.01929694309501073\n",
      "train loss:0.0020017747981024847\n",
      "train loss:0.03032516058770456\n",
      "train loss:0.011957471109529437\n",
      "train loss:0.051148784095547385\n",
      "train loss:0.015271168555599925\n",
      "train loss:0.01594902675293913\n",
      "train loss:0.02016214146292282\n",
      "train loss:0.014807570079325729\n",
      "train loss:0.019789571303789926\n",
      "train loss:0.011815760123477624\n",
      "train loss:0.004958517582133852\n",
      "train loss:0.0066832506553720465\n",
      "train loss:0.007435507577451167\n",
      "train loss:0.00413826288759343\n",
      "train loss:0.009386151050949917\n",
      "train loss:0.016745080819578363\n",
      "train loss:0.06550398143384349\n",
      "train loss:0.011292446964563347\n",
      "train loss:0.008948808122082343\n",
      "train loss:0.03994987763884275\n",
      "train loss:0.0253489963979095\n",
      "train loss:0.021741410583665702\n",
      "train loss:0.014611846032115768\n",
      "train loss:0.01685267484726257\n",
      "train loss:0.018507348277644223\n",
      "train loss:0.007341532677473675\n",
      "train loss:0.018900353310171238\n",
      "train loss:0.01233064159290083\n",
      "train loss:0.0027373981000730113\n",
      "train loss:0.026812479618949166\n",
      "train loss:0.01365573763630094\n",
      "train loss:0.011963168441583492\n",
      "train loss:0.0072605275706363916\n",
      "train loss:0.026374004741929396\n",
      "train loss:0.013576486022100742\n",
      "train loss:0.018136236152157103\n",
      "train loss:0.09518082309471593\n",
      "train loss:0.04219414283461127\n",
      "train loss:0.009852512189593423\n",
      "train loss:0.00443967716585755\n",
      "train loss:0.010631933403218035\n",
      "train loss:0.027288924621380096\n",
      "train loss:0.05062724797215688\n",
      "train loss:0.005902886331652829\n",
      "train loss:0.015035668228591634\n",
      "train loss:0.00908023145515773\n",
      "train loss:0.003302609410831098\n",
      "train loss:0.07297682687678918\n",
      "train loss:0.004074497402034574\n",
      "train loss:0.016205824686014333\n",
      "train loss:0.029840961125677433\n",
      "train loss:0.047980359109393335\n",
      "train loss:0.005478059338053977\n",
      "train loss:0.021955378518684485\n",
      "train loss:0.010284016955718577\n",
      "train loss:0.025372111942004383\n",
      "train loss:0.03877669808489939\n",
      "train loss:0.01654612889295085\n",
      "train loss:0.04352092696956329\n",
      "train loss:0.014675490513571731\n",
      "train loss:0.015438451314535672\n",
      "train loss:0.013816449170477543\n",
      "train loss:0.015100039162659897\n",
      "train loss:0.06354262360859127\n",
      "train loss:0.006955612419834984\n",
      "train loss:0.012891338115232954\n",
      "train loss:0.007895862520627822\n",
      "train loss:0.0029715460082312566\n",
      "train loss:0.0013788887282181821\n",
      "train loss:0.012447598801571513\n",
      "train loss:0.02015055816836373\n",
      "train loss:0.02145768490829323\n",
      "train loss:0.017262543585130487\n",
      "train loss:0.005935419355148766\n",
      "train loss:0.012034196176979954\n",
      "train loss:0.017235044320492356\n",
      "train loss:0.028199139130854377\n",
      "train loss:0.004942620788713141\n",
      "train loss:0.01221313370102179\n",
      "train loss:0.031364589320689064\n",
      "train loss:0.02224669536674343\n",
      "train loss:0.007765943822359726\n",
      "train loss:0.00879482061518875\n",
      "train loss:0.00529622357697389\n",
      "train loss:0.010758967684384139\n",
      "train loss:0.010538832818368432\n",
      "train loss:0.006739444955005911\n",
      "train loss:0.010510608389545676\n",
      "train loss:0.06030662334655752\n",
      "train loss:0.010398288679409076\n",
      "train loss:0.06192695080243669\n",
      "train loss:0.01966879867414079\n",
      "train loss:0.047520413162576214\n",
      "train loss:0.0015254230176539748\n",
      "train loss:0.00560169509811922\n",
      "train loss:0.0019182169765577534\n",
      "train loss:0.0378752524625903\n",
      "train loss:0.0035543485258261575\n",
      "train loss:0.019275346157247585\n",
      "train loss:0.05478851738208047\n",
      "train loss:0.000811045917092266\n",
      "train loss:0.007104863582445389\n",
      "train loss:0.005778316813763965\n",
      "train loss:0.011625539413255623\n",
      "train loss:0.0031011396954982178\n",
      "train loss:0.013170752730769723\n",
      "train loss:0.014773404089518138\n",
      "train loss:0.014779465332830637\n",
      "train loss:0.017978911886861404\n",
      "train loss:0.0016411057809145257\n",
      "train loss:0.0031543389055410428\n",
      "train loss:0.0014515988414727007\n",
      "train loss:0.009516919386670146\n",
      "train loss:0.005550790900994343\n",
      "train loss:0.005307602091836696\n",
      "train loss:0.005255033196997997\n",
      "train loss:0.005084125635000539\n",
      "train loss:0.015608542929573723\n",
      "train loss:0.004360123364206973\n",
      "train loss:0.009068807706990305\n",
      "train loss:0.057494029265108354\n",
      "train loss:0.04554751477853193\n",
      "train loss:0.01361412823671909\n",
      "train loss:0.007191744257917972\n",
      "train loss:0.006116435274757756\n",
      "train loss:0.005064810882633181\n",
      "train loss:0.011105783657131963\n",
      "train loss:0.044573970094417575\n",
      "train loss:0.010226834414809729\n",
      "train loss:0.00454429271314198\n",
      "train loss:0.0088890700636126\n",
      "train loss:0.008493720639566861\n",
      "train loss:0.01232364871897674\n",
      "train loss:0.037266901566411034\n",
      "train loss:0.01093237221611171\n",
      "train loss:0.008165063948516084\n",
      "train loss:0.010012027740122432\n",
      "train loss:0.024582188051320366\n",
      "train loss:0.014200052700756513\n",
      "train loss:0.0038099732018210147\n",
      "train loss:0.00642397420291722\n",
      "train loss:0.011825231398261592\n",
      "train loss:0.010736806084559944\n",
      "train loss:0.029809234576411995\n",
      "train loss:0.0016050809310107028\n",
      "train loss:0.01419580674324419\n",
      "train loss:0.010794289387670755\n",
      "train loss:0.02035347680872628\n",
      "train loss:0.002031516037853755\n",
      "train loss:0.005953003519243818\n",
      "train loss:0.014428965375089885\n",
      "train loss:0.0035829603179089336\n",
      "train loss:0.0055628534760319625\n",
      "train loss:0.011978699484629752\n",
      "train loss:0.009147399722134554\n",
      "train loss:0.010644757281444182\n",
      "train loss:0.033925109650697546\n",
      "train loss:0.00346006019315245\n",
      "train loss:0.018041133825721156\n",
      "train loss:0.008902085851333843\n",
      "train loss:0.08132939608642015\n",
      "train loss:0.014667127565924549\n",
      "train loss:0.022257445168437005\n",
      "train loss:0.006767188452612173\n",
      "train loss:0.008519555218033374\n",
      "train loss:0.016532986145938976\n",
      "train loss:0.0018629747618465278\n",
      "train loss:0.003930115568814669\n",
      "train loss:0.011050658788209669\n",
      "train loss:0.01454349418562732\n",
      "train loss:0.0020114560498903818\n",
      "train loss:0.009397951023364553\n",
      "train loss:0.01285080799225125\n",
      "train loss:0.03532149524768389\n",
      "train loss:0.0027886257642779954\n",
      "train loss:0.005569949752483101\n",
      "train loss:0.002450518342300157\n",
      "train loss:0.021786594391551884\n",
      "train loss:0.015829470671693893\n",
      "train loss:0.0044483592958093655\n",
      "train loss:0.003388642562093555\n",
      "train loss:0.0030111153847247963\n",
      "train loss:0.01284869269414013\n",
      "train loss:0.0032943288218835404\n",
      "train loss:0.008258344795743779\n",
      "train loss:0.004325265448223805\n",
      "train loss:0.009497794274527752\n",
      "train loss:0.006872041010898545\n",
      "train loss:0.020760123631177234\n",
      "train loss:0.02373989856968698\n",
      "train loss:0.051791773586798845\n",
      "train loss:0.011096259344241504\n",
      "train loss:0.002893368376421298\n",
      "train loss:0.00427589269826324\n",
      "train loss:0.018332216491213825\n",
      "train loss:0.0078204791349599\n",
      "train loss:0.0187147850096889\n",
      "train loss:0.003139939393429518\n",
      "train loss:0.016985024932515868\n",
      "train loss:0.034863807161427764\n",
      "train loss:0.004514489273819751\n",
      "train loss:0.01125385117089179\n",
      "train loss:0.019573378317203464\n",
      "train loss:0.00595464322398348\n",
      "train loss:0.0006935136594060537\n",
      "train loss:0.009886799667525582\n",
      "train loss:0.024257458359680007\n",
      "train loss:0.007368378838912512\n",
      "train loss:0.008181216936875\n",
      "train loss:0.010462819628428679\n",
      "train loss:0.0070141439874511925\n",
      "train loss:0.009025999800037928\n",
      "train loss:0.02005201005940964\n",
      "train loss:0.012214234326443196\n",
      "train loss:0.11768023448376963\n",
      "train loss:0.003213082653261341\n",
      "train loss:0.006670328801755859\n",
      "train loss:0.0023643538987307483\n",
      "train loss:0.04475987359720312\n",
      "train loss:0.02518510918517777\n",
      "train loss:0.013833873735485738\n",
      "train loss:0.007432393707971121\n",
      "train loss:0.014025146565049449\n",
      "train loss:0.053705600791163405\n",
      "train loss:0.029685382049163152\n",
      "train loss:0.009644847282862296\n",
      "train loss:0.002263268169585329\n",
      "train loss:0.006893680734649629\n",
      "train loss:0.014552121503265339\n",
      "train loss:0.061442222930389166\n",
      "train loss:0.0397590091766886\n",
      "train loss:0.02733053149085962\n",
      "train loss:0.009165121182347032\n",
      "train loss:0.00507823543459828\n",
      "train loss:0.025042228604606132\n",
      "train loss:0.05755580458412212\n",
      "train loss:0.011125035257761495\n",
      "train loss:0.007561776320611949\n",
      "train loss:0.02768341756706234\n",
      "train loss:0.010967180255720887\n",
      "train loss:0.01946102417149864\n",
      "train loss:0.0031060389729151367\n",
      "train loss:0.019299824226230458\n",
      "train loss:0.005960349912204527\n",
      "train loss:0.016569784662123697\n",
      "train loss:0.02261159915999092\n",
      "train loss:0.02384616556003721\n",
      "train loss:0.0018446399328019556\n",
      "train loss:0.03371768303122703\n",
      "train loss:0.02390633965259311\n",
      "train loss:0.004639541592331626\n",
      "train loss:0.011128339420729126\n",
      "train loss:0.009393258665450486\n",
      "train loss:0.10518619479275745\n",
      "train loss:0.005120487113272096\n",
      "train loss:0.006969631067101891\n",
      "train loss:0.01493874192135865\n",
      "train loss:0.005049851923176588\n",
      "train loss:0.009810987020834805\n",
      "train loss:0.0025873566476950017\n",
      "train loss:0.007842388622550208\n",
      "train loss:0.07313229247046288\n",
      "train loss:0.003912396224078698\n",
      "train loss:0.011148971528181017\n",
      "train loss:0.008942599912715583\n",
      "train loss:0.0018419092058746362\n",
      "train loss:0.005322756427892925\n",
      "train loss:0.0054808903664469956\n",
      "train loss:0.05082212280680097\n",
      "train loss:0.003815807849330875\n",
      "train loss:0.004718014260388987\n",
      "train loss:0.0061555608929966706\n",
      "train loss:0.019328523568776287\n",
      "train loss:0.017199252565792137\n",
      "train loss:0.011951687101490168\n",
      "train loss:0.016502688364197656\n",
      "train loss:0.009541783805106672\n",
      "train loss:0.004062137370259209\n",
      "train loss:0.00968646006069829\n",
      "train loss:0.012768117920866448\n",
      "train loss:0.014895450485177965\n",
      "train loss:0.004963401943396978\n",
      "train loss:0.030566123546024934\n",
      "train loss:0.019249003953800095\n",
      "train loss:0.009541052448887833\n",
      "train loss:0.019938540605806198\n",
      "train loss:0.007725992974714347\n",
      "train loss:0.005042487885532995\n",
      "train loss:0.007634841031928174\n",
      "train loss:0.00867252740069471\n",
      "train loss:0.0023535704552130497\n",
      "train loss:0.014283723578216294\n",
      "train loss:0.011838193598951443\n",
      "train loss:0.016104397443823998\n",
      "train loss:0.04316368433839028\n",
      "train loss:0.006108856170535597\n",
      "train loss:0.03079841020591085\n",
      "train loss:0.004883567834075241\n",
      "train loss:0.07866127793921726\n",
      "train loss:0.048956696074099935\n",
      "train loss:0.008211171820571647\n",
      "train loss:0.0016948532041889822\n",
      "train loss:0.01187754363621204\n",
      "train loss:0.006670503019475088\n",
      "train loss:0.01646570483613262\n",
      "train loss:0.011070696585133542\n",
      "train loss:0.008579357078368231\n",
      "train loss:0.0052796093110509664\n",
      "train loss:0.012397973954477661\n",
      "train loss:0.01393465278874192\n",
      "train loss:0.016558056382256568\n",
      "train loss:0.038714687729924015\n",
      "train loss:0.0056269294504567345\n",
      "train loss:0.024533724406996395\n",
      "train loss:0.005580464480783179\n",
      "train loss:0.02447204285589701\n",
      "train loss:0.0038669301162934638\n",
      "train loss:0.005263953904890568\n",
      "train loss:0.012559649635692999\n",
      "train loss:0.011885099476557137\n",
      "train loss:0.011912533531042525\n",
      "train loss:0.0068233094951970965\n",
      "train loss:0.023177553708680923\n",
      "train loss:0.007280460258032082\n",
      "train loss:0.06264754379331063\n",
      "train loss:0.005776607504882928\n",
      "train loss:0.00770209940283041\n",
      "train loss:0.023541197496721303\n",
      "train loss:0.013447967146950245\n",
      "train loss:0.04134883001636472\n",
      "train loss:0.010817531889065124\n",
      "train loss:0.0051088166640088535\n",
      "train loss:0.01944051341509418\n",
      "train loss:0.005345589201463704\n",
      "train loss:0.023205826130780597\n",
      "train loss:0.01431799814195309\n",
      "train loss:0.007798721217972463\n",
      "train loss:0.018596442479083305\n",
      "train loss:0.017762564411940126\n",
      "train loss:0.0526388966354837\n",
      "train loss:0.0048051587272818245\n",
      "train loss:0.016751155382399092\n",
      "train loss:0.011614718137266023\n",
      "train loss:0.02034638267201335\n",
      "train loss:0.015048331263605205\n",
      "train loss:0.013748653527276335\n",
      "train loss:0.012560518764250605\n",
      "train loss:0.08108296307724353\n",
      "train loss:0.020710055432496263\n",
      "train loss:0.029234222279447105\n",
      "train loss:0.02902818737027968\n",
      "train loss:0.022704995574517137\n",
      "train loss:0.008193132715562344\n",
      "train loss:0.009633714505555717\n",
      "train loss:0.041324513480921594\n",
      "train loss:0.007779921506061297\n",
      "train loss:0.025327147926172317\n",
      "train loss:0.002772001397811882\n",
      "train loss:0.0062153576970306705\n",
      "train loss:0.010940275122248967\n",
      "train loss:0.015781694025058784\n",
      "train loss:0.004576503544944346\n",
      "train loss:0.08203816492599524\n",
      "train loss:0.004154284645053315\n",
      "train loss:0.0040690399913974\n",
      "train loss:0.021453726830751245\n",
      "train loss:0.0021046050336938978\n",
      "train loss:0.011837660772664106\n",
      "train loss:0.05217071281965917\n",
      "train loss:0.029341115921329215\n",
      "train loss:0.015494553988451606\n",
      "train loss:0.0076658088537253114\n",
      "train loss:0.012193951336668825\n",
      "train loss:0.08661030269861257\n",
      "train loss:0.031335156258193025\n",
      "train loss:0.01707090753911942\n",
      "train loss:0.019956754943553624\n",
      "train loss:0.013019051554660871\n",
      "train loss:0.0054542123269402335\n",
      "train loss:0.0011329436154479807\n",
      "train loss:0.013319956733051348\n",
      "train loss:0.007307066326494065\n",
      "train loss:0.023020075859494685\n",
      "train loss:0.07279817273172301\n",
      "train loss:0.004874271028705645\n",
      "train loss:0.00967594708973478\n",
      "train loss:0.0264795921582739\n",
      "train loss:0.02402951355623264\n",
      "train loss:0.002698386541703935\n",
      "train loss:0.007017532375008332\n",
      "train loss:0.001930995859846126\n",
      "train loss:0.00549401027749819\n",
      "train loss:0.00880865430761106\n",
      "train loss:0.011386294549700339\n",
      "train loss:0.00325953186582372\n",
      "train loss:0.045629458307680996\n",
      "=== epoch:8, train acc:0.992, test acc:0.985 ===\n",
      "train loss:0.0021193212761316136\n",
      "train loss:0.008026148124646805\n",
      "train loss:0.01032559876164928\n",
      "train loss:0.004876414230636245\n",
      "train loss:0.013245292735197445\n",
      "train loss:0.059091290553889037\n",
      "train loss:0.010782912715881976\n",
      "train loss:0.01729674142437549\n",
      "train loss:0.010579103029448235\n",
      "train loss:0.008970058185089473\n",
      "train loss:0.008441026220479663\n",
      "train loss:0.015281039357630139\n",
      "train loss:0.005817259373841033\n",
      "train loss:0.03473675724471039\n",
      "train loss:0.017563329766605188\n",
      "train loss:0.03236644186658822\n",
      "train loss:0.011118813410063095\n",
      "train loss:0.00865703576274824\n",
      "train loss:0.009799055015203516\n",
      "train loss:0.006058629809522285\n",
      "train loss:0.00497180419585711\n",
      "train loss:0.002509247440188195\n",
      "train loss:0.018662832478964143\n",
      "train loss:0.0232660521514751\n",
      "train loss:0.004761919027174108\n",
      "train loss:0.00868194360411168\n",
      "train loss:0.011921134947882961\n",
      "train loss:0.004833849371122755\n",
      "train loss:0.04032196969429703\n",
      "train loss:0.010240215413760093\n",
      "train loss:0.010044413324783062\n",
      "train loss:0.021489065158151553\n",
      "train loss:0.0036050257481536974\n",
      "train loss:0.006831372838947934\n",
      "train loss:0.028438985878163067\n",
      "train loss:0.0024776903579246815\n",
      "train loss:0.008174648659881408\n",
      "train loss:0.0036653681395346703\n",
      "train loss:0.013656178276850816\n",
      "train loss:0.008403606495207494\n",
      "train loss:0.05389442617977529\n",
      "train loss:0.006300900535350929\n",
      "train loss:0.002133185366394319\n",
      "train loss:0.021718191175958253\n",
      "train loss:0.004784342160276418\n",
      "train loss:0.007743335319233381\n",
      "train loss:0.031154777619307808\n",
      "train loss:0.0027202645912850227\n",
      "train loss:0.004259837378110604\n",
      "train loss:0.019696144155963557\n",
      "train loss:0.012767494803045936\n",
      "train loss:0.006991128493727116\n",
      "train loss:0.01421289137166752\n",
      "train loss:0.00582697603589134\n",
      "train loss:0.010077503465447987\n",
      "train loss:0.0065262920198110655\n",
      "train loss:0.03061049903563619\n",
      "train loss:0.03793216982719454\n",
      "train loss:0.006221906532250684\n",
      "train loss:0.03772633781702853\n",
      "train loss:0.009403771359540524\n",
      "train loss:0.0094863389964956\n",
      "train loss:0.0020240418892513976\n",
      "train loss:0.0037244002174221876\n",
      "train loss:0.00269435088219751\n",
      "train loss:0.02427449465976209\n",
      "train loss:0.052404787401428184\n",
      "train loss:0.13214120192396295\n",
      "train loss:0.017782235667933566\n",
      "train loss:0.03156136823605908\n",
      "train loss:0.013890160782851303\n",
      "train loss:0.009893088225229236\n",
      "train loss:0.027379971992157855\n",
      "train loss:0.021883314372677088\n",
      "train loss:0.007719733468588455\n",
      "train loss:0.03235976091162119\n",
      "train loss:0.04065618390000617\n",
      "train loss:0.005053505274367135\n",
      "train loss:0.06383878830134586\n",
      "train loss:0.05503302139557026\n",
      "train loss:0.0030474280000210617\n",
      "train loss:0.005256222640880331\n",
      "train loss:0.006704073183840478\n",
      "train loss:0.0028765577032746404\n",
      "train loss:0.03848884377509382\n",
      "train loss:0.017144149612303615\n",
      "train loss:0.008754314343875757\n",
      "train loss:0.014192902536700896\n",
      "train loss:0.009149942941976156\n",
      "train loss:0.01008768897030377\n",
      "train loss:0.0056159008133463036\n",
      "train loss:0.05012074801268961\n",
      "train loss:0.013129217023776294\n",
      "train loss:0.011643598473417805\n",
      "train loss:0.010222740562203602\n",
      "train loss:0.0033162433423644384\n",
      "train loss:0.019154775343130707\n",
      "train loss:0.0028055636630301937\n",
      "train loss:0.0029859496065376685\n",
      "train loss:0.02438783080576458\n",
      "train loss:0.005737529884420503\n",
      "train loss:0.006361313869656469\n",
      "train loss:0.002568356789596509\n",
      "train loss:0.01714744262700173\n",
      "train loss:0.027692890682781465\n",
      "train loss:0.00511983199821563\n",
      "train loss:0.08421002828205991\n",
      "train loss:0.0017346769013165913\n",
      "train loss:0.019903216770421606\n",
      "train loss:0.019563233758791244\n",
      "train loss:0.019602503282635346\n",
      "train loss:0.018120257116249646\n",
      "train loss:0.05405005210126381\n",
      "train loss:0.015396810702823324\n",
      "train loss:0.010688216785739293\n",
      "train loss:0.013902728488599265\n",
      "train loss:0.02047604262418951\n",
      "train loss:0.009296070704440357\n",
      "train loss:0.040941982505581315\n",
      "train loss:0.004714909774049023\n",
      "train loss:0.007165090490608352\n",
      "train loss:0.006676110964401941\n",
      "train loss:0.00575431868154777\n",
      "train loss:0.007234201364061773\n",
      "train loss:0.023447127965032552\n",
      "train loss:0.015531953325325396\n",
      "train loss:0.008012225216225974\n",
      "train loss:0.10341271383998882\n",
      "train loss:0.0044610671042179\n",
      "train loss:0.00795324735084865\n",
      "train loss:0.03138317518688702\n",
      "train loss:0.0026972743141828286\n",
      "train loss:0.016285763288317486\n",
      "train loss:0.0267298373106075\n",
      "train loss:0.004637056775994174\n",
      "train loss:0.009740739571713723\n",
      "train loss:0.025117516380646944\n",
      "train loss:0.018073950205113404\n",
      "train loss:0.0022794824939389793\n",
      "train loss:0.003430678024580883\n",
      "train loss:0.03849026781484135\n",
      "train loss:0.006870295936136291\n",
      "train loss:0.005776384296225087\n",
      "train loss:0.005655219896519383\n",
      "train loss:0.012053817973904061\n",
      "train loss:0.012369477087543759\n",
      "train loss:0.020973528458153763\n",
      "train loss:0.009395910467808397\n",
      "train loss:0.025238498538917794\n",
      "train loss:0.019599324886680796\n",
      "train loss:0.06167968512449133\n",
      "train loss:0.009799024419007908\n",
      "train loss:0.007231907454911633\n",
      "train loss:0.09142360173751568\n",
      "train loss:0.0046509144175358825\n",
      "train loss:0.017500711129054717\n",
      "train loss:0.012488449560385417\n",
      "train loss:0.020321431687200683\n",
      "train loss:0.0029094558892879675\n",
      "train loss:0.02000922673331204\n",
      "train loss:0.015825902540535442\n",
      "train loss:0.041299125925332275\n",
      "train loss:0.0035929772907737335\n",
      "train loss:0.023057745080756732\n",
      "train loss:0.08148072106833132\n",
      "train loss:0.0427751156602661\n",
      "train loss:0.010668867520886331\n",
      "train loss:0.021775198537400217\n",
      "train loss:0.0065615597685106716\n",
      "train loss:0.013719720738325255\n",
      "train loss:0.03032433507406228\n",
      "train loss:0.017012912773453813\n",
      "train loss:0.006115414675367843\n",
      "train loss:0.010158731983812336\n",
      "train loss:0.0046717409929510475\n",
      "train loss:0.010482920459446767\n",
      "train loss:0.010128000047539705\n",
      "train loss:0.04072637475327736\n",
      "train loss:0.006605848612978008\n",
      "train loss:0.003661431074505014\n",
      "train loss:0.02481368526916172\n",
      "train loss:0.00654761552887624\n",
      "train loss:0.014574655867946637\n",
      "train loss:0.010233833523754019\n",
      "train loss:0.0032343769913995846\n",
      "train loss:0.028949172217079905\n",
      "train loss:0.024740157531461474\n",
      "train loss:0.0038731874989434635\n",
      "train loss:0.09745123814360002\n",
      "train loss:0.007638499138152558\n",
      "train loss:0.013859803003204371\n",
      "train loss:0.009746156644933293\n",
      "train loss:0.010492850613657418\n",
      "train loss:0.0014993184526575732\n",
      "train loss:0.019082708677758433\n",
      "train loss:0.019279132779270545\n",
      "train loss:0.004615830103802309\n",
      "train loss:0.015549164561706644\n",
      "train loss:0.015347860718608662\n",
      "train loss:0.00557033084224353\n",
      "train loss:0.0028365098845332597\n",
      "train loss:0.03243069685478973\n",
      "train loss:0.03209359751352278\n",
      "train loss:0.003169280153820837\n",
      "train loss:0.016584970269313874\n",
      "train loss:0.0133860835288673\n",
      "train loss:0.005874714428743995\n",
      "train loss:0.010876354951606484\n",
      "train loss:0.002594291987428071\n",
      "train loss:0.005549598437966157\n",
      "train loss:0.012723946738252088\n",
      "train loss:0.008107864162012023\n",
      "train loss:0.02892954673155197\n",
      "train loss:0.018030154303756093\n",
      "train loss:0.009260269276241634\n",
      "train loss:0.0009319956275797704\n",
      "train loss:0.015006932859328222\n",
      "train loss:0.006225934717989207\n",
      "train loss:0.011662461032652893\n",
      "train loss:0.003992585727422648\n",
      "train loss:0.008947393518998002\n",
      "train loss:0.02888263792782799\n",
      "train loss:0.01363284390028225\n",
      "train loss:0.016895807148393137\n",
      "train loss:0.002387096274431814\n",
      "train loss:0.014944647084940215\n",
      "train loss:0.01025700548519005\n",
      "train loss:0.0243491758455213\n",
      "train loss:0.011064712641552921\n",
      "train loss:0.07823409651999552\n",
      "train loss:0.06641061722038824\n",
      "train loss:0.00095880090801212\n",
      "train loss:0.022893847196546063\n",
      "train loss:0.03292941269227536\n",
      "train loss:0.007642156307436112\n",
      "train loss:0.009086420035600036\n",
      "train loss:0.014544684053922684\n",
      "train loss:0.0029830410000800415\n",
      "train loss:0.008774414007125291\n",
      "train loss:0.014685329441093686\n",
      "train loss:0.00886638084519097\n",
      "train loss:0.004077923801279593\n",
      "train loss:0.010676338001677688\n",
      "train loss:0.018686225436345724\n",
      "train loss:0.008439649598920013\n",
      "train loss:0.014060433022700219\n",
      "train loss:0.004903367281887483\n",
      "train loss:0.009273230285763762\n",
      "train loss:0.013925475502195027\n",
      "train loss:0.010613762338128828\n",
      "train loss:0.009893930180972225\n",
      "train loss:0.0038910915704068235\n",
      "train loss:0.02867070205491657\n",
      "train loss:0.008856796509322723\n",
      "train loss:0.03656221642339997\n",
      "train loss:0.002750854767782358\n",
      "train loss:0.017863346132406758\n",
      "train loss:0.040070153868601785\n",
      "train loss:0.012823553699306971\n",
      "train loss:0.015264379832223682\n",
      "train loss:0.007531316247418492\n",
      "train loss:0.009342930065727662\n",
      "train loss:0.013767931874696205\n",
      "train loss:0.012025721776090322\n",
      "train loss:0.12372951170372443\n",
      "train loss:0.0042595451912662615\n",
      "train loss:0.007310322876557234\n",
      "train loss:0.029277531000157772\n",
      "train loss:0.00885434625983981\n",
      "train loss:0.013626079341639136\n",
      "train loss:0.003727445225875937\n",
      "train loss:0.023060440527940332\n",
      "train loss:0.008299172154234232\n",
      "train loss:0.001098335456216816\n",
      "train loss:0.001995207239851741\n",
      "train loss:0.0031682255995501498\n",
      "train loss:0.011004771918227846\n",
      "train loss:0.005073131020657757\n",
      "train loss:0.02679902529659875\n",
      "train loss:0.007103690801181832\n",
      "train loss:0.018581544873016248\n",
      "train loss:0.010085452452388576\n",
      "train loss:0.004468690740694746\n",
      "train loss:0.0019259680641140419\n",
      "train loss:0.00914650033915324\n",
      "train loss:0.1039496930917221\n",
      "train loss:0.008932440367367797\n",
      "train loss:0.00973232066232118\n",
      "train loss:0.006480588417162056\n",
      "train loss:0.02255433559351168\n",
      "train loss:0.039532204101433044\n",
      "train loss:0.003325068756179269\n",
      "train loss:0.002920998878325867\n",
      "train loss:0.010721447943590891\n",
      "train loss:0.013179186772924132\n",
      "train loss:0.003644214360041993\n",
      "train loss:0.007858576890077984\n",
      "train loss:0.011625870518969177\n",
      "train loss:0.008288842139896406\n",
      "train loss:0.0027171552508357248\n",
      "train loss:0.022838524505277874\n",
      "train loss:0.01335930876899167\n",
      "train loss:0.01043072981855772\n",
      "train loss:0.004776002082818762\n",
      "train loss:0.016140445751474213\n",
      "train loss:0.011321769196598967\n",
      "train loss:0.010499808998041349\n",
      "train loss:0.023905026386894977\n",
      "train loss:0.003950041817094384\n",
      "train loss:0.0024896078225724264\n",
      "train loss:0.015217002137282937\n",
      "train loss:0.002300710763868245\n",
      "train loss:0.0025270041088527925\n",
      "train loss:0.02403666237798706\n",
      "train loss:0.006968533820623228\n",
      "train loss:0.03138488623830653\n",
      "train loss:0.0059453897531862145\n",
      "train loss:0.011715786712986238\n",
      "train loss:0.012757911413291101\n",
      "train loss:0.017235966113683512\n",
      "train loss:0.07246834979015616\n",
      "train loss:0.004482317625149877\n",
      "train loss:0.005673512266203069\n",
      "train loss:0.010122106683543854\n",
      "train loss:0.004968921369371999\n",
      "train loss:0.0038363800769223226\n",
      "train loss:0.021175322089811793\n",
      "train loss:0.008810658679531291\n",
      "train loss:0.02313804798919369\n",
      "train loss:0.018948361981937195\n",
      "train loss:0.009584941737956777\n",
      "train loss:0.005349533638975582\n",
      "train loss:0.0017573944043673545\n",
      "train loss:0.01290134782467755\n",
      "train loss:0.0035664156541996635\n",
      "train loss:0.018498367618271082\n",
      "train loss:0.02624519575069092\n",
      "train loss:0.005796692550184312\n",
      "train loss:0.010668908447039832\n",
      "train loss:0.003437210017491255\n",
      "train loss:0.008742616844732485\n",
      "train loss:0.011772315965115781\n",
      "train loss:0.0022575757708809584\n",
      "train loss:0.005935320646008884\n",
      "train loss:0.0056301554053011325\n",
      "train loss:0.02635697418247178\n",
      "train loss:0.01703175290217723\n",
      "train loss:0.0009518692209914463\n",
      "train loss:0.0008027151764122052\n",
      "train loss:0.002847086291561961\n",
      "train loss:0.022276902925598314\n",
      "train loss:0.0027542712826476713\n",
      "train loss:0.02756945308536407\n",
      "train loss:0.024966928656492086\n",
      "train loss:0.006658582771910574\n",
      "train loss:0.0024414095908566824\n",
      "train loss:0.08374595982508135\n",
      "train loss:0.0031938472280540558\n",
      "train loss:0.01675127822543804\n",
      "train loss:0.020592260227426985\n",
      "train loss:0.02134122978426897\n",
      "train loss:0.041394559306002804\n",
      "train loss:0.01403406415549448\n",
      "train loss:0.004603635794173274\n",
      "train loss:0.0037411217706694953\n",
      "train loss:0.0016726203203311044\n",
      "train loss:0.001327154389233838\n",
      "train loss:0.0021705419450615895\n",
      "train loss:0.0031731113494919915\n",
      "train loss:0.00697672658296703\n",
      "train loss:0.0013598136354249738\n",
      "train loss:0.003545303465604474\n",
      "train loss:0.00508104835819524\n",
      "train loss:0.05870592044594426\n",
      "train loss:0.01215312580176312\n",
      "train loss:0.004446562717441906\n",
      "train loss:0.0025016124707589998\n",
      "train loss:0.005285824607212493\n",
      "train loss:0.039908620402352615\n",
      "train loss:0.003914012660242043\n",
      "train loss:0.002332510100860252\n",
      "train loss:0.03257170190236726\n",
      "train loss:0.0420601595604995\n",
      "train loss:0.002648495462355107\n",
      "train loss:0.008066196603598287\n",
      "train loss:0.0037036274412020416\n",
      "train loss:0.0013453303519846882\n",
      "train loss:0.024543512762313854\n",
      "train loss:0.009491210670805049\n",
      "train loss:0.013059063641678994\n",
      "train loss:0.007218934282658452\n",
      "train loss:0.01894769306041856\n",
      "train loss:0.009627152719580994\n",
      "train loss:0.05594210039050802\n",
      "train loss:0.008095737704834335\n",
      "train loss:0.029043079444621554\n",
      "train loss:0.00833984694514831\n",
      "train loss:0.00165029547830818\n",
      "train loss:0.09253087439333038\n",
      "train loss:0.0030863190316526633\n",
      "train loss:0.023420781087097785\n",
      "train loss:0.005697134310301988\n",
      "train loss:0.0026704759578102728\n",
      "train loss:0.010725436725538136\n",
      "train loss:0.003521214321802032\n",
      "train loss:0.030491139041163004\n",
      "train loss:0.007648519841203638\n",
      "train loss:0.007714549634256487\n",
      "train loss:0.0013282301633516889\n",
      "train loss:0.0036391403494825465\n",
      "train loss:0.031057373067156124\n",
      "train loss:0.004731415795083787\n",
      "train loss:0.021281995829730605\n",
      "train loss:0.014853637648241873\n",
      "train loss:0.01096767302868494\n",
      "train loss:0.037446755063877055\n",
      "train loss:0.006688305241847302\n",
      "train loss:0.0089487169453191\n",
      "train loss:0.014854229531949726\n",
      "train loss:0.00303845966092213\n",
      "train loss:0.010088034932197414\n",
      "train loss:0.0046849653187900405\n",
      "train loss:0.026045948251949728\n",
      "train loss:0.0013178423854487893\n",
      "train loss:0.011275692835945364\n",
      "train loss:0.005880873705054762\n",
      "train loss:0.015704836861609447\n",
      "train loss:0.013598983149726233\n",
      "train loss:0.004596361531499505\n",
      "train loss:0.009732894366591525\n",
      "train loss:0.00621952395791721\n",
      "train loss:0.011687051445154594\n",
      "train loss:0.0027775754884492125\n",
      "train loss:0.005736444041089697\n",
      "train loss:0.059361819943890476\n",
      "train loss:0.0036392321194791883\n",
      "train loss:0.01166067706967028\n",
      "train loss:0.02811228407446524\n",
      "train loss:0.005111477689707984\n",
      "train loss:0.0060855169365233745\n",
      "train loss:0.005960655732663015\n",
      "train loss:0.01924801958254201\n",
      "train loss:0.023703425079457708\n",
      "train loss:0.09486526827833702\n",
      "train loss:0.008849063255028943\n",
      "train loss:0.003193776754732246\n",
      "train loss:0.015549249554518202\n",
      "train loss:0.010508556736997108\n",
      "train loss:0.005309477016181004\n",
      "train loss:0.00901692921951352\n",
      "train loss:0.00983913986341132\n",
      "train loss:0.021381720766292456\n",
      "train loss:0.007745948757021284\n",
      "train loss:0.003239018484067205\n",
      "train loss:0.002230978312846286\n",
      "train loss:0.017786395352439765\n",
      "train loss:0.0017560176647792914\n",
      "train loss:0.012446574150975841\n",
      "train loss:0.01603344309185466\n",
      "train loss:0.04633396385982918\n",
      "train loss:0.01551674861805721\n",
      "train loss:0.024884486236672526\n",
      "train loss:0.0013401462494257214\n",
      "train loss:0.0028090592277150257\n",
      "train loss:0.005490668782730531\n",
      "train loss:0.015498085357715624\n",
      "train loss:0.00926660179128652\n",
      "train loss:0.015956921516284087\n",
      "train loss:0.013946447101104828\n",
      "train loss:0.007319250505355771\n",
      "train loss:0.012543500799763206\n",
      "train loss:0.004030027970547409\n",
      "train loss:0.007738124470249942\n",
      "train loss:0.020207479956157376\n",
      "train loss:0.006599296666083966\n",
      "train loss:0.0008821272022625472\n",
      "train loss:0.0032252923914990635\n",
      "train loss:0.017606579819758737\n",
      "train loss:0.0051829784265538135\n",
      "train loss:0.006196056308438487\n",
      "train loss:0.013872420361532136\n",
      "train loss:0.002680450519061098\n",
      "train loss:0.01940889673081848\n",
      "train loss:0.0014052423259902213\n",
      "train loss:0.008441931631653406\n",
      "train loss:0.005015533869694354\n",
      "train loss:0.0018535901644265692\n",
      "train loss:0.00932826425851032\n",
      "train loss:0.010961843600649357\n",
      "train loss:0.06141012622673147\n",
      "train loss:0.09408309353652138\n",
      "train loss:0.01008690557706853\n",
      "train loss:0.019065733531650107\n",
      "train loss:0.016168114969430405\n",
      "train loss:0.03584142610760657\n",
      "train loss:0.0014113469166689477\n",
      "train loss:0.0013972325245630874\n",
      "train loss:0.004214579807870926\n",
      "train loss:0.01599781338405914\n",
      "train loss:0.0027612669931070515\n",
      "train loss:0.004343484855530399\n",
      "train loss:0.0035336579420846698\n",
      "train loss:0.008092617403459311\n",
      "train loss:0.005001394436736543\n",
      "train loss:0.0031294144133323315\n",
      "train loss:0.0058056114811793295\n",
      "train loss:0.004968055751804543\n",
      "train loss:0.01973919860261806\n",
      "train loss:0.0068717362883575096\n",
      "train loss:0.015110523480964408\n",
      "train loss:0.004855625861025726\n",
      "train loss:0.0006550118187793351\n",
      "train loss:0.021149100093812167\n",
      "train loss:0.006707046237914995\n",
      "train loss:0.002719044659954829\n",
      "train loss:0.01251770222571247\n",
      "train loss:0.0038955816071381054\n",
      "train loss:0.0038010941826920968\n",
      "train loss:0.10652746062939192\n",
      "train loss:0.008038587895199747\n",
      "train loss:0.013918136596656782\n",
      "train loss:0.008182152315920063\n",
      "train loss:0.017075252593212412\n",
      "train loss:0.004564430788388532\n",
      "train loss:0.0011005472019142571\n",
      "train loss:0.0013321786285691208\n",
      "train loss:0.006282720014958196\n",
      "train loss:0.024160513607283725\n",
      "train loss:0.006994301385688153\n",
      "train loss:0.005397413709093188\n",
      "train loss:0.005106196216567962\n",
      "train loss:0.007314537632466248\n",
      "train loss:0.027829170879358513\n",
      "train loss:0.0015177919531850356\n",
      "train loss:0.02421114659064064\n",
      "train loss:0.010777922130969494\n",
      "train loss:0.011541188304354931\n",
      "train loss:0.005917428598863077\n",
      "train loss:0.02249683702157644\n",
      "train loss:0.02108077350163474\n",
      "train loss:0.004036051136951026\n",
      "train loss:0.00340598963671553\n",
      "train loss:0.02481774229543082\n",
      "train loss:0.005271094862377063\n",
      "train loss:0.007603657703312798\n",
      "train loss:0.004300591848917852\n",
      "train loss:0.006053709835662851\n",
      "train loss:0.019667028858076484\n",
      "train loss:0.005341299481753772\n",
      "train loss:0.00776828926062225\n",
      "train loss:0.00943358047492222\n",
      "train loss:0.021321327628186833\n",
      "train loss:0.0528136049535501\n",
      "train loss:0.005319545167409158\n",
      "train loss:0.004874057507939976\n",
      "train loss:0.0013137824423491353\n",
      "train loss:0.0017650166970119702\n",
      "train loss:0.008817625259362992\n",
      "train loss:0.024527499278298958\n",
      "train loss:0.005372924431937253\n",
      "train loss:0.012865228268303941\n",
      "train loss:0.027217193433923773\n",
      "train loss:0.008673164807304934\n",
      "train loss:0.0011591772500603812\n",
      "train loss:0.013643237349927163\n",
      "train loss:0.01580834829696277\n",
      "train loss:0.06299438330509627\n",
      "train loss:0.015457642772758982\n",
      "train loss:0.03475181602491683\n",
      "train loss:0.008393949438902756\n",
      "train loss:0.008292377805266166\n",
      "train loss:0.005942098666589124\n",
      "train loss:0.0072426215011074\n",
      "train loss:0.05298671960812479\n",
      "train loss:0.006960516880230142\n",
      "train loss:0.0024052906645260956\n",
      "train loss:0.006272863742004487\n",
      "train loss:0.010742432274079727\n",
      "train loss:0.011912396566277596\n",
      "train loss:0.011470653993601067\n",
      "train loss:0.004792259087646862\n",
      "train loss:0.02036578919435006\n",
      "train loss:0.003603243250299362\n",
      "train loss:0.010339693398272511\n",
      "train loss:0.013370469305736446\n",
      "train loss:0.004938105481676888\n",
      "train loss:0.003019025534456254\n",
      "train loss:0.03753755753076822\n",
      "train loss:0.00229411685837839\n",
      "train loss:0.008561598963296703\n",
      "train loss:0.011004227247201528\n",
      "train loss:0.007418528755529308\n",
      "train loss:0.003474302939751992\n",
      "train loss:0.014573495630367686\n",
      "train loss:0.0038605654124462176\n",
      "train loss:0.004309138599008457\n",
      "train loss:0.004817878812946675\n",
      "train loss:0.035497987912521865\n",
      "train loss:0.005602817166105757\n",
      "train loss:0.004030144960883321\n",
      "=== epoch:9, train acc:0.996, test acc:0.986 ===\n",
      "train loss:0.0017621267742081348\n",
      "train loss:0.0020731094534296875\n",
      "train loss:0.0011756555360815735\n",
      "train loss:0.010419414000891372\n",
      "train loss:0.019424829442144444\n",
      "train loss:0.0039943661166199716\n",
      "train loss:0.0038489167175517008\n",
      "train loss:0.006206260641851039\n",
      "train loss:0.003860989506619949\n",
      "train loss:0.0367969469797723\n",
      "train loss:0.0018151995646098606\n",
      "train loss:0.0042896607527092975\n",
      "train loss:0.00626242111332417\n",
      "train loss:0.0006383961037359876\n",
      "train loss:0.0016475775812130242\n",
      "train loss:0.004713621084460007\n",
      "train loss:0.00252769338193236\n",
      "train loss:0.003126119319784138\n",
      "train loss:0.0016661465064379125\n",
      "train loss:0.004046815752916753\n",
      "train loss:0.0022397466513224325\n",
      "train loss:0.021056568606996152\n",
      "train loss:0.0010375548084737565\n",
      "train loss:0.0014509513924625557\n",
      "train loss:0.0036387808107623027\n",
      "train loss:0.037428816295602026\n",
      "train loss:0.03807099613556122\n",
      "train loss:0.006634630935992339\n",
      "train loss:0.011024210824927714\n",
      "train loss:0.0017776810831215692\n",
      "train loss:0.0013710896310415103\n",
      "train loss:0.0024863755829816433\n",
      "train loss:0.004183482803915475\n",
      "train loss:0.0036591415911414347\n",
      "train loss:0.002412079489572568\n",
      "train loss:0.0024645708170491663\n",
      "train loss:0.001721975534548742\n",
      "train loss:0.008129876218833293\n",
      "train loss:0.012249885489686663\n",
      "train loss:0.010824762354314059\n",
      "train loss:0.07550556056494967\n",
      "train loss:0.06921283178200988\n",
      "train loss:0.0019351504436613393\n",
      "train loss:0.009571212835645939\n",
      "train loss:0.011702217979466571\n",
      "train loss:0.00133582018823871\n",
      "train loss:0.0051129715773017715\n",
      "train loss:0.008276100302666775\n",
      "train loss:0.019256358954966496\n",
      "train loss:0.021993730590332793\n",
      "train loss:0.002063302126332913\n",
      "train loss:0.007044133242906854\n",
      "train loss:0.0028052300515671136\n",
      "train loss:0.0030331012421270227\n",
      "train loss:0.01919593446997568\n",
      "train loss:0.009242944063944072\n",
      "train loss:0.009716663859437575\n",
      "train loss:0.0006744260483444668\n",
      "train loss:0.008456855604407872\n",
      "train loss:0.02748282536579548\n",
      "train loss:0.010751565905285006\n",
      "train loss:0.009432515793627841\n",
      "train loss:0.006810587979260996\n",
      "train loss:0.0038030346021275074\n",
      "train loss:0.011448012603703577\n",
      "train loss:0.04361810020693409\n",
      "train loss:0.003806289847113261\n",
      "train loss:0.007265785347554434\n",
      "train loss:0.006452583712798516\n",
      "train loss:0.006253882740988512\n",
      "train loss:0.01096902483124276\n",
      "train loss:0.0007932195292296962\n",
      "train loss:0.006446237046769574\n",
      "train loss:0.01675580170111755\n",
      "train loss:0.000942281338316419\n",
      "train loss:0.021691094767734582\n",
      "train loss:0.0029221099176956495\n",
      "train loss:0.012643648542473516\n",
      "train loss:0.0021733111233704725\n",
      "train loss:0.001484035283045004\n",
      "train loss:0.003496760985410951\n",
      "train loss:0.008303142329466707\n",
      "train loss:0.01401754771568518\n",
      "train loss:0.014562679044430234\n",
      "train loss:0.005810404882349289\n",
      "train loss:0.010040086255278335\n",
      "train loss:0.00452818885343234\n",
      "train loss:0.006818066781989315\n",
      "train loss:0.0025372471056113906\n",
      "train loss:0.004750868812432459\n",
      "train loss:0.003186313610508784\n",
      "train loss:0.005693524115556396\n",
      "train loss:0.003261676650577067\n",
      "train loss:0.005828060238907279\n",
      "train loss:0.0062496506335890335\n",
      "train loss:0.003036744476296831\n",
      "train loss:0.0137915892054051\n",
      "train loss:0.004609259815237705\n",
      "train loss:0.0013836797752606158\n",
      "train loss:0.008445902650364035\n",
      "train loss:0.003791379626802976\n",
      "train loss:0.0028535155630360714\n",
      "train loss:0.002066419358937889\n",
      "train loss:0.002832097159276387\n",
      "train loss:0.004482218229312427\n",
      "train loss:0.001906894790285406\n",
      "train loss:0.004290008328516654\n",
      "train loss:0.014198413744203114\n",
      "train loss:0.001960560631680089\n",
      "train loss:0.017701461536711836\n",
      "train loss:0.0043788796486773605\n",
      "train loss:0.024327166980650468\n",
      "train loss:0.019962461679430672\n",
      "train loss:0.004120668963690679\n",
      "train loss:0.0030203564233911707\n",
      "train loss:0.004222628049550003\n",
      "train loss:0.002521794712351844\n",
      "train loss:0.0037591370510799676\n",
      "train loss:0.003076648297125924\n",
      "train loss:0.03729765004690277\n",
      "train loss:0.03193514718186728\n",
      "train loss:0.0021156238625195402\n",
      "train loss:0.01046108924934431\n",
      "train loss:0.014859010415030459\n",
      "train loss:0.008442376393914538\n",
      "train loss:0.006057752447991044\n",
      "train loss:0.009825821341475648\n",
      "train loss:0.01368697390824661\n",
      "train loss:0.005187857646335476\n",
      "train loss:0.0013913422954362823\n",
      "train loss:0.022687923294987974\n",
      "train loss:0.002610130283633065\n",
      "train loss:0.00111354184185291\n",
      "train loss:0.010425668103281485\n",
      "train loss:0.025345664373837798\n",
      "train loss:0.00940217483793184\n",
      "train loss:0.004964847847358033\n",
      "train loss:0.01449965467796755\n",
      "train loss:0.0022793013899797348\n",
      "train loss:0.0013010882029422647\n",
      "train loss:0.0037182549776521227\n",
      "train loss:0.0027567711297885357\n",
      "train loss:0.0016443758997848961\n",
      "train loss:0.0025752421684961508\n",
      "train loss:0.004975314833422168\n",
      "train loss:0.010532533928567596\n",
      "train loss:0.023839149794238627\n",
      "train loss:0.004473181948684642\n",
      "train loss:0.014529740824374481\n",
      "train loss:0.0016290127600039868\n",
      "train loss:0.02476499897640849\n",
      "train loss:0.007691152731557783\n",
      "train loss:0.015246084774264656\n",
      "train loss:0.00916391601847596\n",
      "train loss:0.030991569637743518\n",
      "train loss:0.005111733434237157\n",
      "train loss:0.001797176817871611\n",
      "train loss:0.020863559371967696\n",
      "train loss:0.002688830716063242\n",
      "train loss:0.0033764749917061386\n",
      "train loss:0.0013922150003465123\n",
      "train loss:0.009246638973253247\n",
      "train loss:0.004299027882808511\n",
      "train loss:0.0025827421534395066\n",
      "train loss:0.001995608611290824\n",
      "train loss:0.006928942821339903\n",
      "train loss:0.005633036055035452\n",
      "train loss:0.003929217098758543\n",
      "train loss:0.0025676229492050968\n",
      "train loss:0.0319729136612244\n",
      "train loss:0.006524722612057467\n",
      "train loss:0.008494457207780158\n",
      "train loss:0.09205419814455999\n",
      "train loss:0.0016376474153176764\n",
      "train loss:0.001503477975592493\n",
      "train loss:0.00619722172550893\n",
      "train loss:0.004169306587861692\n",
      "train loss:0.006655334949205809\n",
      "train loss:0.003374819680843978\n",
      "train loss:0.0009850082662818565\n",
      "train loss:0.0032286784657244005\n",
      "train loss:0.0016999830245010355\n",
      "train loss:0.0018177514769047848\n",
      "train loss:0.009602615955347643\n",
      "train loss:0.01065200520619727\n",
      "train loss:0.0020718052665830385\n",
      "train loss:0.024143215749961727\n",
      "train loss:0.0012219893373226678\n",
      "train loss:0.0040737719849007785\n",
      "train loss:0.025256620825868308\n",
      "train loss:0.015015552498422444\n",
      "train loss:0.004203848950988744\n",
      "train loss:0.02483506714461935\n",
      "train loss:0.004394976403386159\n",
      "train loss:0.003665651944620178\n",
      "train loss:0.010807988158145479\n",
      "train loss:0.003277146245070948\n",
      "train loss:0.009262564777890557\n",
      "train loss:0.036141830526565555\n",
      "train loss:0.002339364197991182\n",
      "train loss:0.016984298211725018\n",
      "train loss:0.012214993628626469\n",
      "train loss:0.03903351179074536\n",
      "train loss:0.003528422167871179\n",
      "train loss:0.006127105944613806\n",
      "train loss:0.0034769657376332564\n",
      "train loss:0.06592058344461772\n",
      "train loss:0.002628404154659145\n",
      "train loss:0.009161889699256423\n",
      "train loss:0.003432221874987461\n",
      "train loss:0.0061851730569473275\n",
      "train loss:0.0579047218079529\n",
      "train loss:0.006327771895374202\n",
      "train loss:0.0061326637664055016\n",
      "train loss:0.01747468620640207\n",
      "train loss:0.020099176844726792\n",
      "train loss:0.008088346357152228\n",
      "train loss:0.0033588351855609144\n",
      "train loss:0.014261298268795687\n",
      "train loss:0.05977229651914993\n",
      "train loss:0.008401146577909278\n",
      "train loss:0.004559293243870716\n",
      "train loss:0.003073984025192988\n",
      "train loss:0.009599617661200375\n",
      "train loss:0.01780344616183265\n",
      "train loss:0.012866570731364354\n",
      "train loss:0.002324173149292329\n",
      "train loss:0.006579981355009732\n",
      "train loss:0.006963165342942508\n",
      "train loss:0.0035911978789127086\n",
      "train loss:0.0067460172333794165\n",
      "train loss:0.03741095532047268\n",
      "train loss:0.0012029189655337765\n",
      "train loss:0.002039097322714839\n",
      "train loss:0.012810435465856999\n",
      "train loss:0.00827724986605617\n",
      "train loss:0.011737298870609307\n",
      "train loss:0.031216933717080893\n",
      "train loss:0.10255338021543822\n",
      "train loss:0.006365051489279494\n",
      "train loss:0.01054265948126839\n",
      "train loss:0.0232697826379977\n",
      "train loss:0.010655227272823121\n",
      "train loss:0.025228688622957903\n",
      "train loss:0.021805904771267565\n",
      "train loss:0.004106948378408054\n",
      "train loss:0.00796659963173735\n",
      "train loss:0.020533957717496852\n",
      "train loss:0.019807669289252053\n",
      "train loss:0.000803306815534791\n",
      "train loss:0.005853068390950042\n",
      "train loss:0.004463764299713063\n",
      "train loss:0.005291713044015822\n",
      "train loss:0.018665277445861758\n",
      "train loss:0.0011762426825566192\n",
      "train loss:0.0032934445087745923\n",
      "train loss:0.004753619340457415\n",
      "train loss:0.014304321780756081\n",
      "train loss:0.004183460287488364\n",
      "train loss:0.007214382427900564\n",
      "train loss:0.011603493321648627\n",
      "train loss:0.008765395829895975\n",
      "train loss:0.0007669674495254409\n",
      "train loss:0.005318569462095227\n",
      "train loss:0.01136622288592812\n",
      "train loss:0.04586513633451558\n",
      "train loss:0.0011477806456239826\n",
      "train loss:0.008499632962738148\n",
      "train loss:0.016572129517898673\n",
      "train loss:0.003120727263664046\n",
      "train loss:0.013251489646490375\n",
      "train loss:0.003608760018431049\n",
      "train loss:0.03313422931924725\n",
      "train loss:0.009829143667015991\n",
      "train loss:0.02050523129468402\n",
      "train loss:0.003575266662213763\n",
      "train loss:0.005076741785292913\n",
      "train loss:0.013864869292617106\n",
      "train loss:0.0024131188655650616\n",
      "train loss:0.00862637767659084\n",
      "train loss:0.009449117992790282\n",
      "train loss:0.01917387567109067\n",
      "train loss:0.005930282661402546\n",
      "train loss:0.008802578869625372\n",
      "train loss:0.013660096992857348\n",
      "train loss:0.004859259449849782\n",
      "train loss:0.007808365687949202\n",
      "train loss:0.005593704869919267\n",
      "train loss:0.0020650380507943986\n",
      "train loss:0.001735072533116584\n",
      "train loss:0.0031555287921398597\n",
      "train loss:0.0032466155304439873\n",
      "train loss:0.006965029986578643\n",
      "train loss:0.012168813757085705\n",
      "train loss:0.03166008321365606\n",
      "train loss:0.00566980598840165\n",
      "train loss:0.0036432738654417324\n",
      "train loss:0.007025291820635827\n",
      "train loss:0.004641299930274435\n",
      "train loss:0.008109614191467003\n",
      "train loss:0.00031423706407002704\n",
      "train loss:0.01856822410323037\n",
      "train loss:0.011690139461533983\n",
      "train loss:0.0028345976031622965\n",
      "train loss:0.0013811763841410241\n",
      "train loss:0.0022443200356569554\n",
      "train loss:0.049048706065383675\n",
      "train loss:0.0016470007008775513\n",
      "train loss:0.005763344152626345\n",
      "train loss:0.005046535541730668\n",
      "train loss:0.008144899113748404\n",
      "train loss:0.0024124439107174878\n",
      "train loss:0.0015708274748591016\n",
      "train loss:0.00839044341167146\n",
      "train loss:0.00412544701577328\n",
      "train loss:0.005525886597122225\n",
      "train loss:0.006361680912299117\n",
      "train loss:0.0007355214793756253\n",
      "train loss:0.021922352483377167\n",
      "train loss:0.013163568298869308\n",
      "train loss:0.006727339529477922\n",
      "train loss:0.00356654305550258\n",
      "train loss:0.013260117652923138\n",
      "train loss:0.005996452229017973\n",
      "train loss:0.0046345806315000624\n",
      "train loss:0.02811816701596097\n",
      "train loss:0.016201474896693012\n",
      "train loss:0.009765708508928089\n",
      "train loss:0.0009804267503041675\n",
      "train loss:0.013394213278025185\n",
      "train loss:0.011052776758781661\n",
      "train loss:0.007820842684583586\n",
      "train loss:0.11557529670744261\n",
      "train loss:0.003515914507564411\n",
      "train loss:0.008645134326400253\n",
      "train loss:0.004503452640633299\n",
      "train loss:0.0016103084180455049\n",
      "train loss:0.012767623331954771\n",
      "train loss:0.02176612054548824\n",
      "train loss:0.012904297297987331\n",
      "train loss:0.005706454373968618\n",
      "train loss:0.004104812288593261\n",
      "train loss:0.10029603153053511\n",
      "train loss:0.01517564042140329\n",
      "train loss:0.003552962492413476\n",
      "train loss:0.034242315555629864\n",
      "train loss:0.01157849609088431\n",
      "train loss:0.021606498472320605\n",
      "train loss:0.013081933026477593\n",
      "train loss:0.008694679945166185\n",
      "train loss:0.0016378366871283659\n",
      "train loss:0.01769304835148174\n",
      "train loss:0.030944558136829625\n",
      "train loss:0.005887038772992847\n",
      "train loss:0.0053080039363355555\n",
      "train loss:0.00423416818642111\n",
      "train loss:0.008620828288301812\n",
      "train loss:0.0013320363575231296\n",
      "train loss:0.0029274203034666394\n",
      "train loss:0.003924671480177332\n",
      "train loss:0.004958314920563752\n",
      "train loss:0.0007499614871554247\n",
      "train loss:0.0032889758142226454\n",
      "train loss:0.002695653071973538\n",
      "train loss:0.06732415669173722\n",
      "train loss:0.006097305385534359\n",
      "train loss:0.01145374062857905\n",
      "train loss:0.01636775220716419\n",
      "train loss:0.00927139668511711\n",
      "train loss:0.027893852115571844\n",
      "train loss:0.003995242413325708\n",
      "train loss:0.007273958031980162\n",
      "train loss:0.006342896357757942\n",
      "train loss:0.043551217013697296\n",
      "train loss:0.029932395788218522\n",
      "train loss:0.005751583571163102\n",
      "train loss:0.0013116425718546151\n",
      "train loss:0.005202902103198304\n",
      "train loss:0.006366014497323524\n",
      "train loss:0.03765233163095501\n",
      "train loss:0.0821421655563498\n",
      "train loss:0.00981036597368774\n",
      "train loss:0.002904268353935818\n",
      "train loss:0.003982759949406395\n",
      "train loss:0.01046572260350374\n",
      "train loss:0.01990186586263196\n",
      "train loss:0.0029546406232329597\n",
      "train loss:0.0006196895508273657\n",
      "train loss:0.0054701084377891465\n",
      "train loss:0.004061996867552081\n",
      "train loss:0.0020250927236004604\n",
      "train loss:0.013221939302255997\n",
      "train loss:0.0036074900775760702\n",
      "train loss:0.013876434450624905\n",
      "train loss:0.003324496112678059\n",
      "train loss:0.038042301012951966\n",
      "train loss:0.0015911002309482922\n",
      "train loss:0.0026627151690610906\n",
      "train loss:0.03601700598857482\n",
      "train loss:0.004231073901653118\n",
      "train loss:0.0030960710429721932\n",
      "train loss:0.0034483842574083885\n",
      "train loss:0.009271322975797655\n",
      "train loss:0.012508993203431025\n",
      "train loss:0.0004904069590770074\n",
      "train loss:0.009482917898321268\n",
      "train loss:0.00439491039251122\n",
      "train loss:0.009942764920966567\n",
      "train loss:0.0010479154680661067\n",
      "train loss:0.002691028805380273\n",
      "train loss:0.006890579039757245\n",
      "train loss:0.0013960591266550318\n",
      "train loss:0.010085582460158586\n",
      "train loss:0.009190981402107204\n",
      "train loss:0.02742922621523339\n",
      "train loss:0.006350144729218405\n",
      "train loss:0.004187826719486426\n",
      "train loss:0.0012996818408457138\n",
      "train loss:0.005648423272243273\n",
      "train loss:0.005837280402824492\n",
      "train loss:0.008958510927970464\n",
      "train loss:0.0024853529924876165\n",
      "train loss:0.003618040931526521\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     19\u001b[39m network = SimpleConvNet(input_dim=(\u001b[32m1\u001b[39m,\u001b[32m28\u001b[39m,\u001b[32m28\u001b[39m), \n\u001b[32m     20\u001b[39m                         conv_param = {\u001b[33m'\u001b[39m\u001b[33mfilter_num\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m30\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfilter_size\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m5\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpad\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstride\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m},\n\u001b[32m     21\u001b[39m                         hidden_size=\u001b[32m100\u001b[39m, output_size=\u001b[32m10\u001b[39m, weight_init_std=\u001b[32m0.01\u001b[39m)\n\u001b[32m     23\u001b[39m trainer = Trainer(network, x_train, t_train, x_test, t_test,\n\u001b[32m     24\u001b[39m                   epochs=max_epochs, mini_batch_size=\u001b[32m100\u001b[39m,\n\u001b[32m     25\u001b[39m                   optimizer=\u001b[33m'\u001b[39m\u001b[33mAdam\u001b[39m\u001b[33m'\u001b[39m, optimizer_param={\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.001\u001b[39m},\n\u001b[32m     26\u001b[39m                   evaluate_sample_num_per_epoch=\u001b[32m1000\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# パラメータの保存\u001b[39;00m\n\u001b[32m     30\u001b[39m network.save_params(\u001b[33m\"\u001b[39m\u001b[33mparams.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Deep-learning/deep-learning-from-scratch/src/../common/trainer.py:71\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_iter):\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     test_acc = \u001b[38;5;28mself\u001b[39m.network.accuracy(\u001b[38;5;28mself\u001b[39m.x_test, \u001b[38;5;28mself\u001b[39m.t_test)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Deep-learning/deep-learning-from-scratch/src/../common/trainer.py:44\u001b[39m, in \u001b[36mTrainer.train_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     41\u001b[39m x_batch = \u001b[38;5;28mself\u001b[39m.x_train[batch_mask]\n\u001b[32m     42\u001b[39m t_batch = \u001b[38;5;28mself\u001b[39m.t_train[batch_mask]\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m grads = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.update(\u001b[38;5;28mself\u001b[39m.network.params, grads)\n\u001b[32m     47\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.network.loss(x_batch, t_batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Deep-learning/deep-learning-from-scratch/src/simple_convnet.py:135\u001b[39m, in \u001b[36mSimpleConvNet.gradient\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m    133\u001b[39m layers.reverse()\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     dout = \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# 設定\u001b[39;00m\n\u001b[32m    138\u001b[39m grads = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Deep-learning/deep-learning-from-scratch/src/../common/layers.py:236\u001b[39m, in \u001b[36mConvolution.backward\u001b[39m\u001b[34m(self, dout)\u001b[39m\n\u001b[32m    233\u001b[39m FN, C, FH, FW = \u001b[38;5;28mself\u001b[39m.W.shape\n\u001b[32m    234\u001b[39m dout = dout.transpose(\u001b[32m0\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m1\u001b[39m).reshape(-\u001b[32m1\u001b[39m, FN)\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m \u001b[38;5;28mself\u001b[39m.db = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;28mself\u001b[39m.dW = np.dot(\u001b[38;5;28mself\u001b[39m.col.T, dout)\n\u001b[32m    238\u001b[39m \u001b[38;5;28mself\u001b[39m.dW = \u001b[38;5;28mself\u001b[39m.dW.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m).reshape(FN, C, FH, FW)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Deep-learning/venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2466\u001b[39m, in \u001b[36msum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2463\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m   2464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[32m-> \u001b[39m\u001b[32m2466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Deep-learning/venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78214bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
